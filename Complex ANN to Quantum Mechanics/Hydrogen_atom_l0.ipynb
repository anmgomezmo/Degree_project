{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries required for Neural Network\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import time \n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "dtype = torch.float\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# For plots\n",
    "plt.rcParams.update({'font.size':16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sin() activation function\n",
    "class mySin(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(input):\n",
    "        return torch.sin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivative with auto-differention\n",
    "def df_dx(x,f):\n",
    "    pts = torch.ones(x.shape, dtype=dtype, device=torch.device(device))\n",
    "    return grad([f],[x],grad_outputs=pts,create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic perturbation of the evaluation points\n",
    "# force t[0]=t0 & force points to be in the t-interval\n",
    "def perturbation(grid,t0,tf,sig=0.5):\n",
    "    delta_t = grid[1] - grid[0]\n",
    "    noise = delta_t * torch.randn_like(grid) * sig\n",
    "    t = grid + noise\n",
    "    t.data[2] = torch.ones(1,1)*(-1)\n",
    "    t.data[t<t0] = t0 - t.data[t<t0]\n",
    "    t.data[t>tf] = 2*tf - t.data[t>tf]\n",
    "    t.data[0] = torch.ones(1,1)*t0\n",
    "    t.data[-1] = torch.ones(1,1)*tf\n",
    "    t.requires_grad = False\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial or parametric solution for boundary contitions \n",
    "def parametric_solution(t,nn,t0,x1,tf):\n",
    "    N1,N2 = nn(t)\n",
    "    # There are two parametric solutions, we take some that satisfy dirichlet conditions of radial part of hydrogen atom\n",
    "    g= 1-torch.exp(-(t-tf))\n",
    "    R_hat = g*N1\n",
    "    return R_hat, N2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radial differential equation or eigenvalue problem of hamiltonian operator\n",
    "def hamiltonian_equation(r,R,E,V):\n",
    "    dR_dr = df_dx(r,R)\n",
    "    dR_drr = df_dx(r,dR_dr)\n",
    "    f = dR_drr + dR_dr*2/r + (2*E+V)*R\n",
    "    L = (f.pow(2)).mean()\n",
    "    var_loss = (f.pow(2)).var()    # variance of f\n",
    "    H_R = -1*dR_drr/2 + V*R\n",
    "    return L, f, H_R, var_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the potential at each point r\n",
    "def potential(r):\n",
    "    l = 0\n",
    "    rnp = r.data.numpy()     # points from tensor to numpy\n",
    "    Vnp = 2/rnp - l*(l+1)/rnp**2     # potential over numpy arrays\n",
    "    Vtorch = torch.from_numpy(Vnp) \n",
    "    return Vtorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qNN(torch.nn.Module):\n",
    "    def __init__(self, n_hl=10):      # neurons in hidden layer\n",
    "        super(qNN,self).__init__()\n",
    "\n",
    "        # Define the activation function\n",
    "        self.activ_func = mySin()\n",
    "        # Define layers\n",
    "        self.E_in = torch.nn.Linear(1,1)\n",
    "        self.Lin_1 = torch.nn.Linear(2,n_hl)\n",
    "        self.Lin_2 = torch.nn.Linear(n_hl,n_hl)\n",
    "        self.out = torch.nn.Linear(n_hl,1)\n",
    "\n",
    "    def forward(self,t):\n",
    "            \n",
    "        In_1 = self.E_in(torch.ones_like(t))\n",
    "        L1 = self.Lin_1(torch.cat((t,In_1),1))\n",
    "        h1 = self.activ_func(L1)\n",
    "        L2 = self.Lin_2(h1)\n",
    "        h2 = self.activ_func(L2)\n",
    "        out = self.out(h2)\n",
    "\n",
    "        return out, -1*torch.abs(In_1)   # Negative energy because eigenvalues are negatives for hydrogen atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(t0, tf, x1, neurons, epochs, n_train, lr, minibatch_number=1, orthtime=2e4):\n",
    "    '''        \n",
    "        Inputs\n",
    "        t0 -- left point on domain\n",
    "        tf -- right point on domain\n",
    "        x1 -- \n",
    "        neurons -- number of units at hidden layer\n",
    "        epochs -- number of iterations of training\n",
    "        n_train -- number of points between t0 & tf including them\n",
    "        lr -- learning rate \n",
    "        minibatch_number -- size of minibatch for points on domain\n",
    "        orthtime -- time for training one eigenstate\n",
    "\n",
    "        Outputs \n",
    "        model_1 -- best model for schrodinger equation (lowest loss)\n",
    "        loss_histories -- tuple of lists with histories of variables\n",
    "        runTime -- time of training\n",
    "        fc_ground -- model for ground state \n",
    "        fc_first_excited -- model for first excited state\n",
    "        model_0 -- last trained model \n",
    "    '''\n",
    "    \n",
    "    par2 = 0\n",
    "    model_0 = qNN(neurons)\n",
    "    model_0 = model_0.to(device)\n",
    "    model_1 = 0\n",
    "    betas = [0.999,0.9999]\n",
    "    optimizer = optim.Adam(model_0.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    # List's to save variables\n",
    "    Loss_history = []         ;         Llim = 1e+20    # Upper limit of loss\n",
    "    nontriv_loss_history = []\n",
    "    SE_loss_history = []\n",
    "    En_history = []\n",
    "    orth_losses = []\n",
    "    fc_ground = 0\n",
    "    fc_first_excited = 0\n",
    "\n",
    "    di = (None, 1e+20)\n",
    "    dic = {}\n",
    "    for i in range(1000):\n",
    "        dic[i] = di\n",
    "\n",
    "    grid = torch.linspace(t0,tf,n_train).reshape(-1,1)\n",
    "\n",
    "    # Training iterations\n",
    "    TeP0 = time.time()\n",
    "    walle = 0\n",
    "\n",
    "    for tt in range(epochs):\n",
    "        \n",
    "        t = torch.abs(perturbation(grid,t0,tf,sig=0.03*tf)) + 1e-1\n",
    "\n",
    "        # Batching\n",
    "        batch_size = int(n_train/minibatch_number)\n",
    "        batch_start, batch_end = 0, batch_size\n",
    "\n",
    "        idx = np.random.permutation(n_train)\n",
    "        t_b = t[idx]\n",
    "        t_b.requires_grad = True\n",
    "        t_f = t[-1]\n",
    "        t_f = t_f.reshape(-1,1)\n",
    "        t_f.requires_grad = True\n",
    "        loss = 0\n",
    "        \n",
    "        # Reinitialize weights at orthtime\n",
    "        if tt == orthtime:\n",
    "            model_0.apply(weights_init)\n",
    "\n",
    "        for nbatch in range(minibatch_number):\n",
    "            # Batch time set\n",
    "            t_mb = t_b[batch_start:batch_end].to(device)\n",
    "            norm_loss_reg = 1.0   # regularization coefficient for normalization loss\n",
    "            orth_loss_reg = 0.01    # regularization coefficient for orthogonality loss\n",
    "\n",
    "            # Neural Network solutions\n",
    "            nn, En = model_0(t_mb)\n",
    "            En_history.append(En[0].data.tolist()[0])  # Append energy values from tensor to scalar (not list)\n",
    "\n",
    "            psi = parametric_solution(t_mb, model_0, t0, x1, tf)[0].to(device)\n",
    "            Pot = potential(t_mb.to(device))\n",
    "            Ltot, f_red, H_psi, var_loss = hamiltonian_equation(t_mb, psi, En.to(device), Pot.to(device))\n",
    "            Ltot *= 1\n",
    "            SE_loss_history.append(Ltot)\n",
    "            criteria_loss = Ltot\n",
    "\n",
    "            # Normalization loss\n",
    "            Ltot += norm_loss_reg*(n_train/(tf-t0)*1.0 - torch.sqrt(torch.dot(psi[:,0],psi[:,0]))).pow(2) \n",
    "\n",
    "            # Ortholoss after orthtime\n",
    "            if tt > orthtime and tt < 2*orthtime:\n",
    "                par2 = parametric_solution(t_mb, fc_ground, t0, x1, tf)[0]\n",
    "                ortho_loss = orth_loss_reg*torch.sqrt(torch.dot(par2[:,0]*t_mb[:,0], psi[:,0]*t_mb[:,0]).pow(2))#/100\n",
    "                orth_losses.append(ortho_loss)\n",
    "                Ltot += ortho_loss\n",
    "            elif tt >= 2*orthtime:\n",
    "                par2 = parametric_solution[0](t_mb, fc_ground, t0, x1, tf)\n",
    "                par3 = parametric_solution[0](t_mb, fc_first_excited, t0, x1, tf)\n",
    "                ortho_loss = orth_loss_reg*torch.sqrt(torch.dot((par2[:,0] + par3[:,0])*t_mb[:,0], psi[:,0]*t_mb[:,0]).pow(2))#/10\n",
    "                orth_losses.append(ortho_loss)\n",
    "                Ltot += ortho_loss\n",
    "\n",
    "            # Keep the log\n",
    "            nontriv_loss_history.append(norm_loss_reg*(n_train/(tf-t0)*1.0 - torch.sqrt(torch.dot(psi[:,0],psi[:,0]))).pow(2))\n",
    "\n",
    "            # Optimizer\n",
    "            Ltot.backward(retain_graph=False)\n",
    "            optimizer.step()\n",
    "            loss += Ltot.to(device).data.numpy()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "\n",
    "        # Keep the loss function history\n",
    "        Loss_history.append(loss)\n",
    "\n",
    "        # Keep the best model (lowest loss) by using a deep copy\n",
    "        if criteria_loss < Llim:\n",
    "            model_1 = copy.deepcopy(model_0)\n",
    "            Llim = criteria_loss\n",
    "            if tt < orthtime:\n",
    "                fc_ground = copy.deepcopy(model_0)\n",
    "        \n",
    "        E_bin = abs(En[0].data.tolist()[0]//0.01)\n",
    "        if criteria_loss < dic[E_bin][1]:\n",
    "            dic[E_bin] = (copy.deepcopy(model_0), criteria_loss, (t_mb, f_red, H_psi, psi))\n",
    "        \n",
    "        if tt > orthtime and tt < 2*orthtime:\n",
    "            fc_first_excited = copy.deepcopy(model_0)\n",
    "\n",
    "    TePf = time.time()\n",
    "    runTime = TePf - TeP0\n",
    "    loss_histories = (Loss_history, nontriv_loss_history, SE_loss_history, model_0, En_history, dic, orth_losses)\n",
    "    return model_1, loss_histories, runTime, fc_ground, fc_first_excited, model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "t0 = 0.1\n",
    "tf = 15\n",
    "xBC1 = 0\n",
    "\n",
    "n_train, neurons, epochs, lr, mb, orthtime = 300, 100, int(1e4), 8e-3, 1, 2e4\n",
    "model1, loss_history1, runTime1, fc_ground, first_exc, last_exc  = training(t0,tf,xBC1,neurons,\n",
    "                                                                                    epochs,n_train,lr,mb,orthtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fc_ground,'fc_ground')\n",
    "torch.save(first_exc,'fc_first_exc')\n",
    "torch.save(last_exc,'fc_last_exc')\n",
    "torch.save(loss_history1,'loss_history1')\n",
    "#np.savetxt('loss_histories',loss_history1,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training time (minutes): ',runTime1/60)\n",
    "\n",
    "# Loss funtion\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(loss_history1[0], '-b', alpha=0.8)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Total loss')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the predicted solution\n",
    "nTest = n_train\n",
    "tTest = torch.linspace(t0-0.1,tf+0.1,nTest).reshape(-1,1)\n",
    "tTest.requires_grad = True\n",
    "t_net = tTest.detach().numpy()\n",
    "psi = parametric_solution(tTest, fc_ground.to(device), t0, xBC1, tf)[0]\n",
    "psi_net = psi.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(t_net, psi_net, '-b', linewidth=1, alpha=0.8, label='ANN')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\psi(x)$')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "012450d03bb01005539d256c75e43f99a367f585e0636c001d74ca2aa2e634ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
