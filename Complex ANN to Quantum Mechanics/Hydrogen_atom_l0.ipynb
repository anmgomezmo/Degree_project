{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries required for Neural Network\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import time \n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "dtype = torch.float\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<torch.cuda.device object at 0x7fecc955ce20>\n",
      "1\n",
      "NVIDIA GeForce MX230\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# For plots\n",
    "plt.rcParams.update({'font.size':16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sin() activation function\n",
    "class mySin(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def activation_function(input):\n",
    "        return torch.sin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivative with auto-differention\n",
    "def df_dx(x,f):\n",
    "    pts = torch.ones(x.shape, dtype=dtype, device=torch.device(device))\n",
    "    return grad([f],[x],grad_outputs=pts,create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic perturbation of the evaluation points\n",
    "# force t[0]=t0 & force points to be in the t-interval\n",
    "def perturbation(grid,t0,tf,sig=0.5):\n",
    "    delta_t = grid[1] - grid[0]\n",
    "    noise = delta_t * torch.randn_like(grid) * sig\n",
    "    t = grid + noise\n",
    "    t.data[2] = torch.ones(1,1)*(-1)\n",
    "    t.data[t<t0] = t0 - t.data[t<t0]\n",
    "    t.data[t>tf] = 2*tf - t.data[t>tf]\n",
    "    t.data[0] = torch.ones(1,1)*t0\n",
    "    t.data[-1] = torch.ones(1,1)*tf\n",
    "    t.requires_grad = False\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial or parametric solution for boundary contitions \n",
    "def parametric_solution(t,nn,t0,x1,endpoint):\n",
    "    N1,N2 = nn(t)\n",
    "    # There are two parametric solutions, we take some that satisfy dirichlet conditions of radial part of hydrogen atom\n",
    "    f = 1-torch.exp(t-endpoint)\n",
    "    psi_hat = f*N1\n",
    "    return psi_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radial differential equation or eigenvalue problem of hamiltonian operator\n",
    "def hamiltonian_equation(t,psi,E,V):\n",
    "    dpsi_dx = df_dx(t,psi)\n",
    "    dpsi_dxx = df_dx(t,dpsi_dx)\n",
    "    f = dpsi_dxx + dpsi_dx*2/t + (2*E+V)*psi\n",
    "    L = (f.pow(2)).mean()\n",
    "    var_loss = (f.pow(2)).var()    # variance of f\n",
    "    H_psi = -1*dpsi_dxx/2 + V*psi\n",
    "    return L, f, H_psi, var_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the potential at each point r\n",
    "def potential(Xs):\n",
    "    l = 0\n",
    "    Xsnp = Xs.data.numpy()     # points from tensor to numpy\n",
    "    Vnp = 2/Xsnp - l*(l+1)/Xsnp**2     # potential over numpy arrays\n",
    "    Vtorch = torch.from_numpy(Vnp) \n",
    "    return Vtorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qNN(torch.nn.Module):\n",
    "    def __init__(self, n_hl=10):      # neurons in hidden layer\n",
    "        super(qNN,self).__init__()\n",
    "\n",
    "        # Define the activation function\n",
    "        self.activ_func = mySin()\n",
    "        # Define layers\n",
    "        self.E_in = torch.nn.Linear(1,1)\n",
    "        self.Lin_1 = torch.nn.Linear(2,n_hl)\n",
    "        self.Lin_2 = torch.nn.Linear(n_hl,n_hl)\n",
    "        self.out = torch.nn.Linear(n_hl,1)\n",
    "\n",
    "    def forward(self,t):\n",
    "            \n",
    "        In_1 = self.E_in(torch.ones_like(t))\n",
    "        L1 = self.Lin_1(torch.cat((t,In_1),1))\n",
    "        h1 = self.activ_func(L1)\n",
    "        L2 = self.Lin_2(h1)\n",
    "        h2 = self.activ_func(L2)\n",
    "        out = self.out(h2)\n",
    "\n",
    "        return out, -1*torch.abs(In_1)   # Negative energy because eigenvalues are negatives for hydrogen atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(t0, tf, x1, neurons, epochs, n_train, lr, minibatch_number=1):\n",
    "    '''        \n",
    "        Inputs\n",
    "        t0 -- left point on domain\n",
    "        tf -- right point on domain\n",
    "        x1 --\n",
    "        neurons -- number of units at hidden layer\n",
    "        epochs -- number of iterations of training\n",
    "        n_train -- number of points between t0 & tf including them\n",
    "        lr -- learning rate \n",
    "        minibatch_number -- size of minibatch for points on domain\n",
    "\n",
    "        Outputs \n",
    "\n",
    "    '''\n",
    "    \n",
    "    par2 = 0\n",
    "    model_0 = qNN(neurons)\n",
    "    model_0 = model_0.to(device)\n",
    "    model_1 = 0\n",
    "    betas = [0.999,0.9999]\n",
    "    optimizer = optim.Adam(model_0.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    # List's to save variables\n",
    "    Loss_history = []         ;         Llim = 1e+20    # Upper limit of loss\n",
    "    En_loss_history = []\n",
    "    boundary_loss_history = []\n",
    "    nontriv_loss_history = []\n",
    "    SE_loss_history = []\n",
    "    Ennontriv_loss_history = []\n",
    "    criteria_loss_history = []\n",
    "    En_history = []\n",
    "    prob_loss = []\n",
    "    EWall_history = []\n",
    "    orth_losses = []\n",
    "    var_loss_history = []\n",
    "    fc_ground = 0\n",
    "    fc_first_excited = 0\n",
    "    reinit = 0\n",
    "    orthtime = 2e+4\n",
    "\n",
    "    di = (None, 1e+20)\n",
    "    dic = {}\n",
    "    for i in range(1000):\n",
    "        dic[i] = di\n",
    "\n",
    "    grid = torch.linspace(t0,tf,n_train).reshape(-1,1)\n",
    "\n",
    "    # Training iterations\n",
    "    TeP0 = time.time()\n",
    "    walle = 0\n",
    "\n",
    "    for tt in range(epochs):\n",
    "        \n",
    "        t = torch.abs(perturbation(grid,t0,tf,sig=0.03*tf)) + 1e-1\n",
    "\n",
    "        # Batching\n",
    "        batch_size = int(n_train/minibatch_number)\n",
    "        batch_start, batch_end = 0, batch_size\n",
    "\n",
    "        idx = np.random.permutation(n_train)\n",
    "        t_b = t[idx]\n",
    "        t_b.requires_grad = True\n",
    "        t_f = t[-1]\n",
    "        t_f = t_f.reshape(-1,1)\n",
    "        t_f.requires_grad = True\n",
    "        loss = 0\n",
    "        \n",
    "        # Reinitialize weights at orthtime\n",
    "        if tt == orthtime:\n",
    "            model_0.apply(weights_init)\n",
    "\n",
    "        for nbatch in range(minibatch_number):\n",
    "            # Batch time set\n",
    "            t_mb = t_b[batch_start:batch_end].to(device)\n",
    "            norm_loss_reg = 1.0\n",
    "\n",
    "            # Neural Network solutions\n",
    "            nn, En = model_0(t_mb)\n",
    "            En_history.append(En[0].data.tolist()[0])  # Append energy values from tensor to scalar (not list)\n",
    "\n",
    "            psi = parametric_solution(t_mb, model_0, t0, x1, tf).to(device)\n",
    "            Pot = potential(t_mb.to(device))\n",
    "            Ltot, f_red, H_psi, var_loss = hamiltonian_equation(t_mb, psi, En.to(device), Pot.to(device))\n",
    "            Ltot *= 1\n",
    "            SE_loss_history.append(Ltot)\n",
    "            criteria_loss = Ltot\n",
    "\n",
    "            # Normalization loss\n",
    "            Ltot += norm_loss_reg*(n_train/(tf-t0)*1.0 - torch.sqrt(torch.dot(psi[:,0],psi[:,0]))).pow(2) \n",
    "\n",
    "            # Ortholoss after orthtime\n",
    "            if tt > 2e+4 and tt < 4e+4:\n",
    "                par2 = parametric_solution(t_mb, fc_ground, t0, x1, tf)\n",
    "                ortho_loss = 0.01*torch.sqrt(torch.dot(par2[:,0]*t_mb[:,0], psi[:,0]*t_mb[:,0]).pow(2))/100\n",
    "                orth_losses.append(ortho_loss)\n",
    "                Ltot += ortho_loss\n",
    "            elif tt >= 4e+4:\n",
    "                par2 = parametric_solution(t_mb, fc_ground, t0, x1, tf)\n",
    "                par3 = parametric_solution(t_mb, fc_first_excited, t0, x1, tf)\n",
    "                ortho_loss = 0.01*torch.sqrt(torch.dot((par2[:,0] + par3[:,0])*t_mb[:,0], psi[:,0]*t_mb[:,0]).pow(2))/10\n",
    "                orth_losses.append(ortho_loss)\n",
    "                Ltot += ortho_loss\n",
    "\n",
    "            # Keep the log\n",
    "            nontriv_loss_history.append(norm_loss_reg*(n_train/(tf-t0)*1.0 - torch.sqrt(torch.dot(psi[:,0],psi[:,0]))).pow(2))\n",
    "\n",
    "            # Optimizer\n",
    "            Ltot.backward(retain_graph=False)\n",
    "            optimizer.step()\n",
    "            loss += Ltot.to(device).data.numpy()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "\n",
    "        # Keep the loss function history\n",
    "        Loss_history.append(loss)\n",
    "\n",
    "        # Keep the best model (lowest loss) by using a deep copy\n",
    "        if criteria_loss < Llim:\n",
    "            model_1 = copy.deepcopy(model_0)\n",
    "            Llim = criteria_loss\n",
    "            if tt < orthtime:\n",
    "                fc_ground = copy.deepcopy(model_0)\n",
    "        \n",
    "        E_bin = abs(En[0].data.tolist()[0]//0.01)\n",
    "        if criteria_loss < dic[E_bin][1]:\n",
    "            dic[E_bin] = (copy.deepcopy(model_0), criteria_loss, (t_mb, f_red, H_psi, psi))\n",
    "        \n",
    "        if tt > 3.9e+4:\n",
    "            fc_first_excited = copy.deepcopy(model_0)\n",
    "\n",
    "    TePf = time.time()\n",
    "    runTime = TePf - TeP0\n",
    "    loss_histories = (Loss_history, boundary_loss_history, nontriv_loss_history, SE_loss_history, Ennontriv_loss_history, En_loss_history)\n",
    "    return model_1, loss_histories, runTime, fc_first_excited, fc_ground "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [mySin] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/andresgo/Documents/Degree project/Degree_project/Complex ANN to Quantum Mechanics/Hydrogen_atom_l0.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=3'>4</a>\u001b[0m xBC1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=5'>6</a>\u001b[0m n_train, neurons, epochs, lr, mb \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39mint\u001b[39m(\u001b[39m2e4\u001b[39m), \u001b[39m8e-3\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=6'>7</a>\u001b[0m model1, loss_history1, runTime1, first_exc, fc_ground \u001b[39m=\u001b[39m training(t0,tf,xBC1,neurons,epochs,n_train,lr,mb)\n",
      "\u001b[1;32m/home/andresgo/Documents/Degree project/Degree_project/Complex ANN to Quantum Mechanics/Hydrogen_atom_l0.ipynb Cell 16\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(t0, tf, x1, neurons, epochs, n_train, lr, minibatch_number)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=75'>76</a>\u001b[0m norm_loss_reg \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=77'>78</a>\u001b[0m \u001b[39m# Neural Network solutions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=78'>79</a>\u001b[0m nn, En \u001b[39m=\u001b[39m model_0(t_mb)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=79'>80</a>\u001b[0m En_history\u001b[39m.\u001b[39mappend(En[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m])  \u001b[39m# Append energy values from tensor to scalar (not list)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=81'>82</a>\u001b[0m psi \u001b[39m=\u001b[39m parametric_solution(t_mb, model_0, t0, x1, tf)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/NN/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/andresgo/Documents/Degree project/Degree_project/Complex ANN to Quantum Mechanics/Hydrogen_atom_l0.ipynb Cell 16\u001b[0m in \u001b[0;36mqNN.forward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=14'>15</a>\u001b[0m In_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mE_in(torch\u001b[39m.\u001b[39mones_like(t))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=15'>16</a>\u001b[0m L1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLin_1(torch\u001b[39m.\u001b[39mcat((t,In_1),\u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=16'>17</a>\u001b[0m h1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactiv_func(L1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=17'>18</a>\u001b[0m L2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLin_2(h1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andresgo/Documents/Degree%20project/Degree_project/Complex%20ANN%20to%20Quantum%20Mechanics/Hydrogen_atom_l0.ipynb#ch0000015?line=18'>19</a>\u001b[0m h2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactiv_func(L2)\n",
      "File \u001b[0;32m~/anaconda3/envs/NN/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/NN/lib/python3.9/site-packages/torch/nn/modules/module.py:201\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [mySin] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "t0 = 0.1\n",
    "tf = 15\n",
    "xBC1 = 0\n",
    "\n",
    "n_train, neurons, epochs, lr, mb = 300, 10, int(2e4), 8e-3, 1\n",
    "model1, loss_history1, runTime1, first_exc, fc_ground = training(t0,tf,xBC1,neurons,epochs,n_train,lr,mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "012450d03bb01005539d256c75e43f99a367f585e0636c001d74ca2aa2e634ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
