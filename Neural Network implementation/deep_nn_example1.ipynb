{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c625fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 21:54:22.746297: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-15 21:54:22.746320: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72898e94",
   "metadata": {},
   "source": [
    "## Differential equation\n",
    "$$\\frac{d\\psi}{dx} + \\left(x+\\frac{1+3x^2}{1+x+x^3} \\right)\\psi = x^3+2x+x^2\\left(\\frac{1+3x^2}{1+x+x^3} \\right)$$ \n",
    "Dataset are vectors of domain of differential equation, like the vectors are one-dimentional, the shape of dataset is one by m samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04419852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_sol(x):\n",
    "    psi = tf.pow(x,2) + tf.exp(-0.5*tf.pow(x,2))/(1+x+tf.pow(x,3))\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a66c5f",
   "metadata": {},
   "source": [
    "## Implementation Neural Network model\n",
    "\n",
    "Neural Network with one hidden layer with 10 hidden units and sigmoid activation, and one linear output unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aff363",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see API model</summary>\n",
    "  def deep_nn_model(input_shape):\n",
    "    '''\n",
    "        Model definition of newral network for LINEAR -> SIGMOID -> LINEAR -> LINEAR\n",
    "        \n",
    "        Inputs:\n",
    "        input_shape -- shape or dimension of vectors (in this case onedimentional vectors)\n",
    "        \n",
    "        Return:\n",
    "        model -- TF Keras model (object containing the information for the entire training process)  \n",
    "    '''\n",
    "    \n",
    "    X = Input(shape=input_shape, name=\"X\")\n",
    "    Y = Input(shape=input_shape, name=\"Y\")\n",
    "    \n",
    "    A1 = Dense(units=10, activation=\"sigmoid\", name=\"A1\")(X)\n",
    "    N = Dense(units=1, activation=\"linear\", use_bias=False, name=\"N\")(A1)  \n",
    "    \n",
    "    model = tf.keras.Model(inputs=[X,Y], outputs=N)\n",
    "\n",
    "    return model\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f30161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y,N):\n",
    "    '''\n",
    "        Compute the cost function of differential equation\n",
    "        \n",
    "        Inputs:\n",
    "        X -- dataset of domain vectors\n",
    "        N -- output of forward propagation\n",
    "        \n",
    "        Return:\n",
    "        cost -- tensor of the cost function\n",
    "    '''\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(Y)   \n",
    "    dN_dx = g.gradient(N, Y)\n",
    "\n",
    "    psi = 1 + tf.multiply(Y, N)\n",
    "    dpsi_dx = N + tf.multiply(Y, dN_dx)\n",
    "    coeff_1 = tf.multiply(Y + (1+3*tf.square(Y))/(1+Y+tf.pow(Y,3)), psi)\n",
    "    coeff_2 = tf.pow(Y,3) + 2*Y + tf.multiply(tf.square(Y), (1+3*tf.square(Y))/(1+Y+tf.pow(Y,3)))\n",
    "    \n",
    "    cost = tf.reduce_sum(tf.square(dpsi_dx + coeff_1 - coeff_2))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af001a78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data x:\n",
      " tf.Tensor(\n",
      "[0.         0.11111111 0.22222222 0.33333334 0.44444445 0.5555556\n",
      " 0.6666667  0.7777778  0.8888889  1.        ], shape=(10,), dtype=float32)\n",
      "Train data y:\n",
      " tf.Tensor(\n",
      "[1.         0.90570426 0.8405067  0.80140585 0.7887944  0.8048698\n",
      " 0.8523673  0.93362933 1.0500929  1.2021769 ], shape=(10,), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 21:54:25.220789: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-15 21:54:25.220838: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-15 21:54:25.220872: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (littlerocket): /proc/driver/nvidia/version does not exist\n",
      "2022-03-15 21:54:25.221350: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "n_h = 10\n",
    "X_train = tf.constant(np.linspace(0,1,10), dtype=tf.float32)\n",
    "Y_train = analytic_sol(X_train)\n",
    "print(\"Train data x:\\n\",X_train)\n",
    "print(\"Train data y:\\n\",Y_train)\n",
    "print(type(X_train))\n",
    "print(type(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ad04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Input(shape=1, name=\"X\")\n",
    "Y = Input(shape=1, name=\"Y\")    \n",
    "A1 = Dense(units=10, activation=\"sigmoid\", name=\"A1\")(X)\n",
    "N = Dense(units=1, activation=\"linear\", use_bias=False, name=\"N\")(A1)  \n",
    "model = tf.keras.Model(inputs=[X,Y], outputs=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345fcb18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Passed in object of type <class 'keras.engine.keras_tensor.KerasTensor'>, not tf.Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39madd_loss(\u001b[43mcompute_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#model.optimizer(\"adam\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mcompute_cost\u001b[0;34m(Y, N)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Compute the cost function of differential equation\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    cost -- tensor of the cost function\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m g:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     15\u001b[0m dN_dx \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mgradient(N, Y)\n\u001b[1;32m     17\u001b[0m psi \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mmultiply(Y, N)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:906\u001b[0m, in \u001b[0;36mGradientTape.watch\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(tensor, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (_pywrap_utils\u001b[38;5;241m.\u001b[39mIsTensor(t) \u001b[38;5;129;01mor\u001b[39;00m _pywrap_utils\u001b[38;5;241m.\u001b[39mIsVariable(t)):\n\u001b[0;32m--> 906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed in object of type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, not tf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;28mtype\u001b[39m(t)))\n\u001b[1;32m    908\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backprop_util\u001b[38;5;241m.\u001b[39mIsTrainable(t):\n\u001b[1;32m    909\u001b[0m     logging\u001b[38;5;241m.\u001b[39mlog_first_n(\n\u001b[1;32m    910\u001b[0m         logging\u001b[38;5;241m.\u001b[39mWARN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dtype of the watched tensor must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating (e.g. tf.float32), got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m, t\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Passed in object of type <class 'keras.engine.keras_tensor.KerasTensor'>, not tf.Tensor"
     ]
    }
   ],
   "source": [
    "model.add_loss(compute_cost(Y,N))\n",
    "#model.optimizer(\"adam\")\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023be144",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()\n",
    "model.fit([X_train, X_train], epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab27c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
