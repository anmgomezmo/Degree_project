{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb94f89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020576f",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d970ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(1,10),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(10,1, bias=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = self.layers(x)\n",
    "        return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a56de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(x, loss_fn, optimizer):\n",
    "    x = x.to(device)\n",
    "    def closure():\n",
    "        loss = loss_fn(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d63b2",
   "metadata": {},
   "source": [
    "### Differential equation\n",
    "$$\\frac{d^2\\phi(r)}{dr^2} + \\frac{2m}{\\hbar^2}\\left(E-\\frac{l(l+1)}{2mr^2}\\hbar^2-V(r)\\right)\\phi(r) = 0$$ \n",
    "Dataset are vectors of domain of differential equation, like the vectors are one-dimentional, the shape of dataset is one by m samples. Trial solution $\\phi_t(r) = e^{-\\beta r^2}N(r,\\vec{p})$, with $\\phi(r=0) = 0$ and $\\phi(r\\rightarrow\\infty) = 0$ as boundary conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03384e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=10, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=10, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19eb90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=0.0001)\n",
    "beta = 2\n",
    "global Z\n",
    "Z = 1\n",
    "global e\n",
    "#e = -1.602e-19\n",
    "e = -1\n",
    "global hbar\n",
    "#hbar = 1.054e-34\n",
    "hbar = 1\n",
    "global m\n",
    "#m = 9.109e-31\n",
    "m = 1\n",
    "global l\n",
    "l = 0\n",
    "V = lambda r: -(Z*e**2)/r\n",
    "Phi_t = lambda r: torch.exp(-beta*r**2) * model.forward(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e21bb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(r):\n",
    "    r.requires_grad = True\n",
    "    \n",
    "    outputs = Phi_t(r)\n",
    "    Phi_t_r = torch.autograd.grad(outputs, r, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]\n",
    "    Phi_t_r_r = torch.autograd.grad(Phi_t_r, r, grad_outputs=torch.ones_like(Phi_t_r), create_graph=True)[0]\n",
    "    H_Phi_t = -(hbar**2/(2*m))*Phi_t_r_r + (l*(l+1)*hbar**2/(2*m*r**2) + V(r))*outputs\n",
    "    \n",
    "    prom = outputs.size()[0]\n",
    "    \n",
    "    delta = r[1]-r[0]\n",
    "    norm = torch.sum(outputs**2)*delta\n",
    "    \n",
    "    global E\n",
    "    E = (torch.sum(outputs*H_Phi_t)*delta)/norm\n",
    "    \n",
    "    return (torch.mean((H_Phi_t - E*outputs)**2)*prom)/norm #multiply by m to avoit division by m in the mean function of torh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959e5101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " ---------------------- loss: tensor([14181.8389], grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      " ---------------------- loss: tensor([17897.5410], grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      " ---------------------- loss: tensor([17496.9414], grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      " ---------------------- loss: tensor([17240.5879], grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      " ---------------------- loss: tensor([17020.6035], grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      " ---------------------- loss: tensor([16805.1113], grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      " ---------------------- loss: tensor([16578.7441], grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      " ---------------------- loss: tensor([16332.2588], grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      " ---------------------- loss: tensor([16061.3604], grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      " ---------------------- loss: tensor([15777.2041], grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      " ---------------------- loss: tensor([15478.1504], grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      " ---------------------- loss: tensor([15137.9980], grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      " ---------------------- loss: tensor([14713.4033], grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      " ---------------------- loss: tensor([14238.9385], grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      " ---------------------- loss: tensor([13732.4619], grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      " ---------------------- loss: tensor([13141.3506], grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      " ---------------------- loss: tensor([10591.3193], grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      " ---------------------- loss: tensor([1859.7192], grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      " ---------------------- loss: tensor([1683.2389], grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      " ---------------------- loss: tensor([1577.2643], grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      " ---------------------- loss: tensor([1500.2408], grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      " ---------------------- loss: tensor([1436.9774], grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      " ---------------------- loss: tensor([1380.5272], grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      " ---------------------- loss: tensor([1319.4954], grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      " ---------------------- loss: tensor([1243.5540], grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      " ---------------------- loss: tensor([1175.1608], grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      " ---------------------- loss: tensor([1118.5118], grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      " ---------------------- loss: tensor([1069.0223], grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      " ---------------------- loss: tensor([1015.0699], grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      " ---------------------- loss: tensor([951.7311], grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      " ---------------------- loss: tensor([897.0548], grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      " ---------------------- loss: tensor([846.9471], grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      " ---------------------- loss: tensor([792.4639], grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      " ---------------------- loss: tensor([748.0601], grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      " ---------------------- loss: tensor([714.8066], grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      " ---------------------- loss: tensor([678.4927], grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      " ---------------------- loss: tensor([636.9766], grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      " ---------------------- loss: tensor([601.7516], grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      " ---------------------- loss: tensor([570.0591], grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      " ---------------------- loss: tensor([543.6771], grad_fn=<DivBackward0>)\n",
      "Epoch 41\n",
      " ---------------------- loss: tensor([520.5307], grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      " ---------------------- loss: tensor([496.9481], grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      " ---------------------- loss: tensor([471.9410], grad_fn=<DivBackward0>)\n",
      "Epoch 44\n",
      " ---------------------- loss: tensor([452.8301], grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      " ---------------------- loss: tensor([436.3644], grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      " ---------------------- loss: tensor([418.6366], grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      " ---------------------- loss: tensor([401.0834], grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      " ---------------------- loss: tensor([387.0548], grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      " ---------------------- loss: tensor([373.9307], grad_fn=<DivBackward0>)\n",
      "Epoch 50\n",
      " ---------------------- loss: tensor([360.9285], grad_fn=<DivBackward0>)\n",
      "Epoch 51\n",
      " ---------------------- loss: tensor([347.1599], grad_fn=<DivBackward0>)\n",
      "Epoch 52\n",
      " ---------------------- loss: tensor([334.6930], grad_fn=<DivBackward0>)\n",
      "Epoch 53\n",
      " ---------------------- loss: tensor([323.5372], grad_fn=<DivBackward0>)\n",
      "Epoch 54\n",
      " ---------------------- loss: tensor([312.8322], grad_fn=<DivBackward0>)\n",
      "Epoch 55\n",
      " ---------------------- loss: tensor([300.7515], grad_fn=<DivBackward0>)\n",
      "Epoch 56\n",
      " ---------------------- loss: tensor([288.8650], grad_fn=<DivBackward0>)\n",
      "Epoch 57\n",
      " ---------------------- loss: tensor([280.2415], grad_fn=<DivBackward0>)\n",
      "Epoch 58\n",
      " ---------------------- loss: tensor([271.5879], grad_fn=<DivBackward0>)\n",
      "Epoch 59\n",
      " ---------------------- loss: tensor([261.7061], grad_fn=<DivBackward0>)\n",
      "Epoch 60\n",
      " ---------------------- loss: tensor([250.4788], grad_fn=<DivBackward0>)\n",
      "Epoch 61\n",
      " ---------------------- loss: tensor([243.6709], grad_fn=<DivBackward0>)\n",
      "Epoch 62\n",
      " ---------------------- loss: tensor([236.3656], grad_fn=<DivBackward0>)\n",
      "Epoch 63\n",
      " ---------------------- loss: tensor([226.4174], grad_fn=<DivBackward0>)\n",
      "Epoch 64\n",
      " ---------------------- loss: tensor([218.0627], grad_fn=<DivBackward0>)\n",
      "Epoch 65\n",
      " ---------------------- loss: tensor([212.5618], grad_fn=<DivBackward0>)\n",
      "Epoch 66\n",
      " ---------------------- loss: tensor([207.9379], grad_fn=<DivBackward0>)\n",
      "Epoch 67\n",
      " ---------------------- loss: tensor([203.9776], grad_fn=<DivBackward0>)\n",
      "Epoch 68\n",
      " ---------------------- loss: tensor([200.8392], grad_fn=<DivBackward0>)\n",
      "Epoch 69\n",
      " ---------------------- loss: tensor([197.6776], grad_fn=<DivBackward0>)\n",
      "Epoch 70\n",
      " ---------------------- loss: tensor([194.4339], grad_fn=<DivBackward0>)\n",
      "Epoch 71\n",
      " ---------------------- loss: tensor([190.3329], grad_fn=<DivBackward0>)\n",
      "Epoch 72\n",
      " ---------------------- loss: tensor([186.1002], grad_fn=<DivBackward0>)\n",
      "Epoch 73\n",
      " ---------------------- loss: tensor([182.1474], grad_fn=<DivBackward0>)\n",
      "Epoch 74\n",
      " ---------------------- loss: tensor([179.1616], grad_fn=<DivBackward0>)\n",
      "Epoch 75\n",
      " ---------------------- loss: tensor([176.7903], grad_fn=<DivBackward0>)\n",
      "Epoch 76\n",
      " ---------------------- loss: tensor([174.9677], grad_fn=<DivBackward0>)\n",
      "Epoch 77\n",
      " ---------------------- loss: tensor([173.4931], grad_fn=<DivBackward0>)\n",
      "Epoch 78\n",
      " ---------------------- loss: tensor([172.1716], grad_fn=<DivBackward0>)\n",
      "Epoch 79\n",
      " ---------------------- loss: tensor([170.8419], grad_fn=<DivBackward0>)\n",
      "Epoch 80\n",
      " ---------------------- loss: tensor([169.2217], grad_fn=<DivBackward0>)\n",
      "Epoch 81\n",
      " ---------------------- loss: tensor([167.3011], grad_fn=<DivBackward0>)\n",
      "Epoch 82\n",
      " ---------------------- loss: tensor([165.2980], grad_fn=<DivBackward0>)\n",
      "Epoch 83\n",
      " ---------------------- loss: tensor([163.5326], grad_fn=<DivBackward0>)\n",
      "Epoch 84\n",
      " ---------------------- loss: tensor([162.1107], grad_fn=<DivBackward0>)\n",
      "Epoch 85\n",
      " ---------------------- loss: tensor([161.0469], grad_fn=<DivBackward0>)\n",
      "Epoch 86\n",
      " ---------------------- loss: tensor([160.2188], grad_fn=<DivBackward0>)\n",
      "Epoch 87\n",
      " ---------------------- loss: tensor([159.5541], grad_fn=<DivBackward0>)\n",
      "Epoch 88\n",
      " ---------------------- loss: tensor([158.9521], grad_fn=<DivBackward0>)\n",
      "Epoch 89\n",
      " ---------------------- loss: tensor([158.2869], grad_fn=<DivBackward0>)\n",
      "Epoch 90\n",
      " ---------------------- loss: tensor([157.3563], grad_fn=<DivBackward0>)\n",
      "Epoch 91\n",
      " ---------------------- loss: tensor([156.2274], grad_fn=<DivBackward0>)\n",
      "Epoch 92\n",
      " ---------------------- loss: tensor([155.0815], grad_fn=<DivBackward0>)\n",
      "Epoch 93\n",
      " ---------------------- loss: tensor([154.0995], grad_fn=<DivBackward0>)\n",
      "Epoch 94\n",
      " ---------------------- loss: tensor([153.3325], grad_fn=<DivBackward0>)\n",
      "Epoch 95\n",
      " ---------------------- loss: tensor([152.7457], grad_fn=<DivBackward0>)\n",
      "Epoch 96\n",
      " ---------------------- loss: tensor([152.2859], grad_fn=<DivBackward0>)\n",
      "Epoch 97\n",
      " ---------------------- loss: tensor([151.9236], grad_fn=<DivBackward0>)\n",
      "Epoch 98\n",
      " ---------------------- loss: tensor([151.5888], grad_fn=<DivBackward0>)\n",
      "Epoch 99\n",
      " ---------------------- loss: tensor([151.1199], grad_fn=<DivBackward0>)\n",
      "Epoch 100\n",
      " ---------------------- loss: tensor([150.1868], grad_fn=<DivBackward0>)\n",
      "Epoch 101\n",
      " ---------------------- loss: tensor([149.1512], grad_fn=<DivBackward0>)\n",
      "Epoch 102\n",
      " ---------------------- loss: tensor([148.3650], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103\n",
      " ---------------------- loss: tensor([147.7812], grad_fn=<DivBackward0>)\n",
      "Epoch 104\n",
      " ---------------------- loss: tensor([147.3180], grad_fn=<DivBackward0>)\n",
      "Epoch 105\n",
      " ---------------------- loss: tensor([146.9541], grad_fn=<DivBackward0>)\n",
      "Epoch 106\n",
      " ---------------------- loss: tensor([146.6681], grad_fn=<DivBackward0>)\n",
      "Epoch 107\n",
      " ---------------------- loss: tensor([146.4410], grad_fn=<DivBackward0>)\n",
      "Epoch 108\n",
      " ---------------------- loss: tensor([146.2432], grad_fn=<DivBackward0>)\n",
      "Epoch 109\n",
      " ---------------------- loss: tensor([146.0556], grad_fn=<DivBackward0>)\n",
      "Epoch 110\n",
      " ---------------------- loss: tensor([145.8667], grad_fn=<DivBackward0>)\n",
      "Epoch 111\n",
      " ---------------------- loss: tensor([145.6589], grad_fn=<DivBackward0>)\n",
      "Epoch 112\n",
      " ---------------------- loss: tensor([145.4408], grad_fn=<DivBackward0>)\n",
      "Epoch 113\n",
      " ---------------------- loss: tensor([145.0729], grad_fn=<DivBackward0>)\n",
      "Epoch 114\n",
      " ---------------------- loss: tensor([144.6505], grad_fn=<DivBackward0>)\n",
      "Epoch 115\n",
      " ---------------------- loss: tensor([144.3614], grad_fn=<DivBackward0>)\n",
      "Epoch 116\n",
      " ---------------------- loss: tensor([144.1689], grad_fn=<DivBackward0>)\n",
      "Epoch 117\n",
      " ---------------------- loss: tensor([144.0134], grad_fn=<DivBackward0>)\n",
      "Epoch 118\n",
      " ---------------------- loss: tensor([143.8798], grad_fn=<DivBackward0>)\n",
      "Epoch 119\n",
      " ---------------------- loss: tensor([143.7544], grad_fn=<DivBackward0>)\n",
      "Epoch 120\n",
      " ---------------------- loss: tensor([143.5315], grad_fn=<DivBackward0>)\n",
      "Epoch 121\n",
      " ---------------------- loss: tensor([143.2375], grad_fn=<DivBackward0>)\n",
      "Epoch 122\n",
      " ---------------------- loss: tensor([143.0239], grad_fn=<DivBackward0>)\n",
      "Epoch 123\n",
      " ---------------------- loss: tensor([142.8334], grad_fn=<DivBackward0>)\n",
      "Epoch 124\n",
      " ---------------------- loss: tensor([142.6484], grad_fn=<DivBackward0>)\n",
      "Epoch 125\n",
      " ---------------------- loss: tensor([142.4788], grad_fn=<DivBackward0>)\n",
      "Epoch 126\n",
      " ---------------------- loss: tensor([142.3127], grad_fn=<DivBackward0>)\n",
      "Epoch 127\n",
      " ---------------------- loss: tensor([142.1460], grad_fn=<DivBackward0>)\n",
      "Epoch 128\n",
      " ---------------------- loss: tensor([141.9783], grad_fn=<DivBackward0>)\n",
      "Epoch 129\n",
      " ---------------------- loss: tensor([141.7507], grad_fn=<DivBackward0>)\n",
      "Epoch 130\n",
      " ---------------------- loss: tensor([141.4673], grad_fn=<DivBackward0>)\n",
      "Epoch 131\n",
      " ---------------------- loss: tensor([141.1592], grad_fn=<DivBackward0>)\n",
      "Epoch 132\n",
      " ---------------------- loss: tensor([140.5062], grad_fn=<DivBackward0>)\n",
      "Epoch 133\n",
      " ---------------------- loss: tensor([139.0827], grad_fn=<DivBackward0>)\n",
      "Epoch 134\n",
      " ---------------------- loss: tensor([137.4975], grad_fn=<DivBackward0>)\n",
      "Epoch 135\n",
      " ---------------------- loss: tensor([136.3637], grad_fn=<DivBackward0>)\n",
      "Epoch 136\n",
      " ---------------------- loss: tensor([135.2786], grad_fn=<DivBackward0>)\n",
      "Epoch 137\n",
      " ---------------------- loss: tensor([134.3720], grad_fn=<DivBackward0>)\n",
      "Epoch 138\n",
      " ---------------------- loss: tensor([133.6154], grad_fn=<DivBackward0>)\n",
      "Epoch 139\n",
      " ---------------------- loss: tensor([132.7543], grad_fn=<DivBackward0>)\n",
      "Epoch 140\n",
      " ---------------------- loss: tensor([131.7532], grad_fn=<DivBackward0>)\n",
      "Epoch 141\n",
      " ---------------------- loss: tensor([130.5321], grad_fn=<DivBackward0>)\n",
      "Epoch 142\n",
      " ---------------------- loss: tensor([128.3711], grad_fn=<DivBackward0>)\n",
      "Epoch 143\n",
      " ---------------------- loss: tensor([119.8480], grad_fn=<DivBackward0>)\n",
      "Epoch 144\n",
      " ---------------------- loss: tensor([116.1335], grad_fn=<DivBackward0>)\n",
      "Epoch 145\n",
      " ---------------------- loss: tensor([113.6454], grad_fn=<DivBackward0>)\n",
      "Epoch 146\n",
      " ---------------------- loss: tensor([111.3934], grad_fn=<DivBackward0>)\n",
      "Epoch 147\n",
      " ---------------------- loss: tensor([109.0937], grad_fn=<DivBackward0>)\n",
      "Epoch 148\n",
      " ---------------------- loss: tensor([106.9105], grad_fn=<DivBackward0>)\n",
      "Epoch 149\n",
      " ---------------------- loss: tensor([105.0862], grad_fn=<DivBackward0>)\n",
      "Epoch 150\n",
      " ---------------------- loss: tensor([103.4114], grad_fn=<DivBackward0>)\n",
      "Epoch 151\n",
      " ---------------------- loss: tensor([101.6373], grad_fn=<DivBackward0>)\n",
      "Epoch 152\n",
      " ---------------------- loss: tensor([99.6429], grad_fn=<DivBackward0>)\n",
      "Epoch 153\n",
      " ---------------------- loss: tensor([97.1477], grad_fn=<DivBackward0>)\n",
      "Epoch 154\n",
      " ---------------------- loss: tensor([89.4050], grad_fn=<DivBackward0>)\n",
      "Epoch 155\n",
      " ---------------------- loss: tensor([85.7897], grad_fn=<DivBackward0>)\n",
      "Epoch 156\n",
      " ---------------------- loss: tensor([83.1332], grad_fn=<DivBackward0>)\n",
      "Epoch 157\n",
      " ---------------------- loss: tensor([80.6109], grad_fn=<DivBackward0>)\n",
      "Epoch 158\n",
      " ---------------------- loss: tensor([78.1393], grad_fn=<DivBackward0>)\n",
      "Epoch 159\n",
      " ---------------------- loss: tensor([75.5647], grad_fn=<DivBackward0>)\n",
      "Epoch 160\n",
      " ---------------------- loss: tensor([72.2747], grad_fn=<DivBackward0>)\n",
      "Epoch 161\n",
      " ---------------------- loss: tensor([68.8843], grad_fn=<DivBackward0>)\n",
      "Epoch 162\n",
      " ---------------------- loss: tensor([67.2465], grad_fn=<DivBackward0>)\n",
      "Epoch 163\n",
      " ---------------------- loss: tensor([65.9300], grad_fn=<DivBackward0>)\n",
      "Epoch 164\n",
      " ---------------------- loss: tensor([64.4956], grad_fn=<DivBackward0>)\n",
      "Epoch 165\n",
      " ---------------------- loss: tensor([63.9764], grad_fn=<DivBackward0>)\n",
      "Epoch 166\n",
      " ---------------------- loss: tensor([63.6205], grad_fn=<DivBackward0>)\n",
      "Epoch 167\n",
      " ---------------------- loss: tensor([63.2886], grad_fn=<DivBackward0>)\n",
      "Epoch 168\n",
      " ---------------------- loss: tensor([62.7724], grad_fn=<DivBackward0>)\n",
      "Epoch 169\n",
      " ---------------------- loss: tensor([62.1144], grad_fn=<DivBackward0>)\n",
      "Epoch 170\n",
      " ---------------------- loss: tensor([61.3245], grad_fn=<DivBackward0>)\n",
      "Epoch 171\n",
      " ---------------------- loss: tensor([60.5169], grad_fn=<DivBackward0>)\n",
      "Epoch 172\n",
      " ---------------------- loss: tensor([59.8041], grad_fn=<DivBackward0>)\n",
      "Epoch 173\n",
      " ---------------------- loss: tensor([59.2453], grad_fn=<DivBackward0>)\n",
      "Epoch 174\n",
      " ---------------------- loss: tensor([58.7850], grad_fn=<DivBackward0>)\n",
      "Epoch 175\n",
      " ---------------------- loss: tensor([58.3845], grad_fn=<DivBackward0>)\n",
      "Epoch 176\n",
      " ---------------------- loss: tensor([57.9853], grad_fn=<DivBackward0>)\n",
      "Epoch 177\n",
      " ---------------------- loss: tensor([57.3310], grad_fn=<DivBackward0>)\n",
      "Epoch 178\n",
      " ---------------------- loss: tensor([56.7800], grad_fn=<DivBackward0>)\n",
      "Epoch 179\n",
      " ---------------------- loss: tensor([56.3390], grad_fn=<DivBackward0>)\n",
      "Epoch 180\n",
      " ---------------------- loss: tensor([55.9514], grad_fn=<DivBackward0>)\n",
      "Epoch 181\n",
      " ---------------------- loss: tensor([55.5714], grad_fn=<DivBackward0>)\n",
      "Epoch 182\n",
      " ---------------------- loss: tensor([55.2652], grad_fn=<DivBackward0>)\n",
      "Epoch 183\n",
      " ---------------------- loss: tensor([54.9936], grad_fn=<DivBackward0>)\n",
      "Epoch 184\n",
      " ---------------------- loss: tensor([54.5493], grad_fn=<DivBackward0>)\n",
      "Epoch 185\n",
      " ---------------------- loss: tensor([54.2112], grad_fn=<DivBackward0>)\n",
      "Epoch 186\n",
      " ---------------------- loss: tensor([53.9711], grad_fn=<DivBackward0>)\n",
      "Epoch 187\n",
      " ---------------------- loss: tensor([53.6656], grad_fn=<DivBackward0>)\n",
      "Epoch 188\n",
      " ---------------------- loss: tensor([53.3434], grad_fn=<DivBackward0>)\n",
      "Epoch 189\n",
      " ---------------------- loss: tensor([53.1310], grad_fn=<DivBackward0>)\n",
      "Epoch 190\n",
      " ---------------------- loss: tensor([52.9749], grad_fn=<DivBackward0>)\n",
      "Epoch 191\n",
      " ---------------------- loss: tensor([52.7317], grad_fn=<DivBackward0>)\n",
      "Epoch 192\n",
      " ---------------------- loss: tensor([52.5105], grad_fn=<DivBackward0>)\n",
      "Epoch 193\n",
      " ---------------------- loss: tensor([52.3541], grad_fn=<DivBackward0>)\n",
      "Epoch 194\n",
      " ---------------------- loss: tensor([52.1705], grad_fn=<DivBackward0>)\n",
      "Epoch 195\n",
      " ---------------------- loss: tensor([51.9697], grad_fn=<DivBackward0>)\n",
      "Epoch 196\n",
      " ---------------------- loss: tensor([51.7394], grad_fn=<DivBackward0>)\n",
      "Epoch 197\n",
      " ---------------------- loss: tensor([51.6698], grad_fn=<DivBackward0>)\n",
      "Epoch 198\n",
      " ---------------------- loss: tensor([51.5510], grad_fn=<DivBackward0>)\n",
      "Epoch 199\n",
      " ---------------------- loss: tensor([51.4103], grad_fn=<DivBackward0>)\n",
      "Epoch 200\n",
      " ---------------------- loss: tensor([51.3312], grad_fn=<DivBackward0>)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "upper_r = 6\n",
    "lower_r = 1e-2\n",
    "steps = 100\n",
    "R_train = torch.Tensor(np.linspace(lower_r, upper_r, steps)[:,None])\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n ---------------------- loss: {loss_fn(R_train.to(device))}\")\n",
    "    training(R_train, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3b74c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6250], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e8fe7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f10b9b00c70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rr = np.linspace(lower_r, upper_r, steps)[:,None]\n",
    "\n",
    "with torch.no_grad():\n",
    "    yy = Phi_t(torch.Tensor(rr).to(device)).cpu().numpy()\n",
    "#yt = xx**2 + np.exp(-xx**2 / 2)/(1+xx+xx**3)\n",
    "\n",
    "fig, axs = plt.subplots(dpi=100)\n",
    "#axs.plot(xx, yt, label=\"True\")\n",
    "axs.plot(rr, yy, \".\", label=\"Neural Network Approximation\")\n",
    "axs.set_xlabel(\"$x$\")\n",
    "axs.set_ylabel(\"$\\phi(x)$\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b29354",
   "metadata": {},
   "source": [
    "## With Xavier initialization of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6857e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Linear)):\n",
    "            nn.init.xavier_normal_(m.weight.data)\n",
    "        # if you also want for linear layers ,add one more elif condition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b8dcddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " ---------------------- loss: tensor([1391.9270], grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      " ---------------------- loss: tensor([1379.7018], grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      " ---------------------- loss: tensor([1375.4695], grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      " ---------------------- loss: tensor([1371.2252], grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      " ---------------------- loss: tensor([1366.9746], grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      " ---------------------- loss: tensor([1362.6860], grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      " ---------------------- loss: tensor([1358.3827], grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      " ---------------------- loss: tensor([1354.0688], grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      " ---------------------- loss: tensor([1349.7057], grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      " ---------------------- loss: tensor([1345.2908], grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      " ---------------------- loss: tensor([1340.8215], grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      " ---------------------- loss: tensor([1336.2494], grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      " ---------------------- loss: tensor([1331.5723], grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      " ---------------------- loss: tensor([1326.7046], grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      " ---------------------- loss: tensor([1321.5620], grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      " ---------------------- loss: tensor([1316.0359], grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      " ---------------------- loss: tensor([1309.7960], grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      " ---------------------- loss: tensor([1302.2798], grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      " ---------------------- loss: tensor([1292.2866], grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      " ---------------------- loss: tensor([1276.7582], grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      " ---------------------- loss: tensor([1248.2257], grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      " ---------------------- loss: tensor([1194.3221], grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      " ---------------------- loss: tensor([1123.7354], grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      " ---------------------- loss: tensor([1058.5149], grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      " ---------------------- loss: tensor([1001.0622], grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      " ---------------------- loss: tensor([947.3428], grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      " ---------------------- loss: tensor([893.9233], grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      " ---------------------- loss: tensor([839.3153], grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      " ---------------------- loss: tensor([787.6444], grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      " ---------------------- loss: tensor([741.3868], grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      " ---------------------- loss: tensor([700.6767], grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      " ---------------------- loss: tensor([663.0262], grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      " ---------------------- loss: tensor([626.7699], grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      " ---------------------- loss: tensor([590.5523], grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      " ---------------------- loss: tensor([556.2198], grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      " ---------------------- loss: tensor([524.6737], grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      " ---------------------- loss: tensor([496.3285], grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      " ---------------------- loss: tensor([470.5362], grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      " ---------------------- loss: tensor([445.9427], grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      " ---------------------- loss: tensor([421.8463], grad_fn=<DivBackward0>)\n",
      "Epoch 41\n",
      " ---------------------- loss: tensor([398.7360], grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      " ---------------------- loss: tensor([377.4408], grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      " ---------------------- loss: tensor([358.2254], grad_fn=<DivBackward0>)\n",
      "Epoch 44\n",
      " ---------------------- loss: tensor([340.7074], grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      " ---------------------- loss: tensor([324.4655], grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      " ---------------------- loss: tensor([308.8105], grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      " ---------------------- loss: tensor([293.2420], grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      " ---------------------- loss: tensor([278.6238], grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      " ---------------------- loss: tensor([265.6071], grad_fn=<DivBackward0>)\n",
      "Epoch 50\n",
      " ---------------------- loss: tensor([253.9387], grad_fn=<DivBackward0>)\n",
      "Epoch 51\n",
      " ---------------------- loss: tensor([243.1983], grad_fn=<DivBackward0>)\n",
      "Epoch 52\n",
      " ---------------------- loss: tensor([233.7627], grad_fn=<DivBackward0>)\n",
      "Epoch 53\n",
      " ---------------------- loss: tensor([224.5983], grad_fn=<DivBackward0>)\n",
      "Epoch 54\n",
      " ---------------------- loss: tensor([213.9502], grad_fn=<DivBackward0>)\n",
      "Epoch 55\n",
      " ---------------------- loss: tensor([204.7577], grad_fn=<DivBackward0>)\n",
      "Epoch 56\n",
      " ---------------------- loss: tensor([197.3563], grad_fn=<DivBackward0>)\n",
      "Epoch 57\n",
      " ---------------------- loss: tensor([190.3388], grad_fn=<DivBackward0>)\n",
      "Epoch 58\n",
      " ---------------------- loss: tensor([184.1044], grad_fn=<DivBackward0>)\n",
      "Epoch 59\n",
      " ---------------------- loss: tensor([179.2343], grad_fn=<DivBackward0>)\n",
      "Epoch 60\n",
      " ---------------------- loss: tensor([173.7249], grad_fn=<DivBackward0>)\n",
      "Epoch 61\n",
      " ---------------------- loss: tensor([165.8042], grad_fn=<DivBackward0>)\n",
      "Epoch 62\n",
      " ---------------------- loss: tensor([160.3555], grad_fn=<DivBackward0>)\n",
      "Epoch 63\n",
      " ---------------------- loss: tensor([155.8069], grad_fn=<DivBackward0>)\n",
      "Epoch 64\n",
      " ---------------------- loss: tensor([151.5846], grad_fn=<DivBackward0>)\n",
      "Epoch 65\n",
      " ---------------------- loss: tensor([148.1662], grad_fn=<DivBackward0>)\n",
      "Epoch 66\n",
      " ---------------------- loss: tensor([145.3066], grad_fn=<DivBackward0>)\n",
      "Epoch 67\n",
      " ---------------------- loss: tensor([140.5880], grad_fn=<DivBackward0>)\n",
      "Epoch 68\n",
      " ---------------------- loss: tensor([136.1345], grad_fn=<DivBackward0>)\n",
      "Epoch 69\n",
      " ---------------------- loss: tensor([132.8350], grad_fn=<DivBackward0>)\n",
      "Epoch 70\n",
      " ---------------------- loss: tensor([130.1423], grad_fn=<DivBackward0>)\n",
      "Epoch 71\n",
      " ---------------------- loss: tensor([127.8227], grad_fn=<DivBackward0>)\n",
      "Epoch 72\n",
      " ---------------------- loss: tensor([125.9364], grad_fn=<DivBackward0>)\n",
      "Epoch 73\n",
      " ---------------------- loss: tensor([124.1463], grad_fn=<DivBackward0>)\n",
      "Epoch 74\n",
      " ---------------------- loss: tensor([120.9435], grad_fn=<DivBackward0>)\n",
      "Epoch 75\n",
      " ---------------------- loss: tensor([118.0093], grad_fn=<DivBackward0>)\n",
      "Epoch 76\n",
      " ---------------------- loss: tensor([116.3476], grad_fn=<DivBackward0>)\n",
      "Epoch 77\n",
      " ---------------------- loss: tensor([114.8203], grad_fn=<DivBackward0>)\n",
      "Epoch 78\n",
      " ---------------------- loss: tensor([113.6810], grad_fn=<DivBackward0>)\n",
      "Epoch 79\n",
      " ---------------------- loss: tensor([112.7288], grad_fn=<DivBackward0>)\n",
      "Epoch 80\n",
      " ---------------------- loss: tensor([111.7217], grad_fn=<DivBackward0>)\n",
      "Epoch 81\n",
      " ---------------------- loss: tensor([109.8934], grad_fn=<DivBackward0>)\n",
      "Epoch 82\n",
      " ---------------------- loss: tensor([108.4958], grad_fn=<DivBackward0>)\n",
      "Epoch 83\n",
      " ---------------------- loss: tensor([107.5200], grad_fn=<DivBackward0>)\n",
      "Epoch 84\n",
      " ---------------------- loss: tensor([106.7234], grad_fn=<DivBackward0>)\n",
      "Epoch 85\n",
      " ---------------------- loss: tensor([106.1591], grad_fn=<DivBackward0>)\n",
      "Epoch 86\n",
      " ---------------------- loss: tensor([105.6371], grad_fn=<DivBackward0>)\n",
      "Epoch 87\n",
      " ---------------------- loss: tensor([105.1585], grad_fn=<DivBackward0>)\n",
      "Epoch 88\n",
      " ---------------------- loss: tensor([104.5468], grad_fn=<DivBackward0>)\n",
      "Epoch 89\n",
      " ---------------------- loss: tensor([103.8841], grad_fn=<DivBackward0>)\n",
      "Epoch 90\n",
      " ---------------------- loss: tensor([103.2886], grad_fn=<DivBackward0>)\n",
      "Epoch 91\n",
      " ---------------------- loss: tensor([102.8424], grad_fn=<DivBackward0>)\n",
      "Epoch 92\n",
      " ---------------------- loss: tensor([102.5570], grad_fn=<DivBackward0>)\n",
      "Epoch 93\n",
      " ---------------------- loss: tensor([102.3193], grad_fn=<DivBackward0>)\n",
      "Epoch 94\n",
      " ---------------------- loss: tensor([101.7682], grad_fn=<DivBackward0>)\n",
      "Epoch 95\n",
      " ---------------------- loss: tensor([101.5803], grad_fn=<DivBackward0>)\n",
      "Epoch 96\n",
      " ---------------------- loss: tensor([100.7347], grad_fn=<DivBackward0>)\n",
      "Epoch 97\n",
      " ---------------------- loss: tensor([100.6202], grad_fn=<DivBackward0>)\n",
      "Epoch 98\n",
      " ---------------------- loss: tensor([100.4661], grad_fn=<DivBackward0>)\n",
      "Epoch 99\n",
      " ---------------------- loss: tensor([100.4273], grad_fn=<DivBackward0>)\n",
      "Epoch 100\n",
      " ---------------------- loss: tensor([100.3078], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101\n",
      " ---------------------- loss: tensor([100.1924], grad_fn=<DivBackward0>)\n",
      "Epoch 102\n",
      " ---------------------- loss: tensor([100.0533], grad_fn=<DivBackward0>)\n",
      "Epoch 103\n",
      " ---------------------- loss: tensor([99.9406], grad_fn=<DivBackward0>)\n",
      "Epoch 104\n",
      " ---------------------- loss: tensor([99.7555], grad_fn=<DivBackward0>)\n",
      "Epoch 105\n",
      " ---------------------- loss: tensor([99.5067], grad_fn=<DivBackward0>)\n",
      "Epoch 106\n",
      " ---------------------- loss: tensor([99.4437], grad_fn=<DivBackward0>)\n",
      "Epoch 107\n",
      " ---------------------- loss: tensor([99.3369], grad_fn=<DivBackward0>)\n",
      "Epoch 108\n",
      " ---------------------- loss: tensor([99.2598], grad_fn=<DivBackward0>)\n",
      "Epoch 109\n",
      " ---------------------- loss: tensor([99.1546], grad_fn=<DivBackward0>)\n",
      "Epoch 110\n",
      " ---------------------- loss: tensor([98.9568], grad_fn=<DivBackward0>)\n",
      "Epoch 111\n",
      " ---------------------- loss: tensor([98.8679], grad_fn=<DivBackward0>)\n",
      "Epoch 112\n",
      " ---------------------- loss: tensor([98.7991], grad_fn=<DivBackward0>)\n",
      "Epoch 113\n",
      " ---------------------- loss: tensor([98.7335], grad_fn=<DivBackward0>)\n",
      "Epoch 114\n",
      " ---------------------- loss: tensor([98.6218], grad_fn=<DivBackward0>)\n",
      "Epoch 115\n",
      " ---------------------- loss: tensor([98.5224], grad_fn=<DivBackward0>)\n",
      "Epoch 116\n",
      " ---------------------- loss: tensor([98.4917], grad_fn=<DivBackward0>)\n",
      "Epoch 117\n",
      " ---------------------- loss: tensor([98.4571], grad_fn=<DivBackward0>)\n",
      "Epoch 118\n",
      " ---------------------- loss: tensor([98.2579], grad_fn=<DivBackward0>)\n",
      "Epoch 119\n",
      " ---------------------- loss: tensor([98.1668], grad_fn=<DivBackward0>)\n",
      "Epoch 120\n",
      " ---------------------- loss: tensor([98.1043], grad_fn=<DivBackward0>)\n",
      "Epoch 121\n",
      " ---------------------- loss: tensor([98.0560], grad_fn=<DivBackward0>)\n",
      "Epoch 122\n",
      " ---------------------- loss: tensor([97.9913], grad_fn=<DivBackward0>)\n",
      "Epoch 123\n",
      " ---------------------- loss: tensor([97.9442], grad_fn=<DivBackward0>)\n",
      "Epoch 124\n",
      " ---------------------- loss: tensor([97.2573], grad_fn=<DivBackward0>)\n",
      "Epoch 125\n",
      " ---------------------- loss: tensor([97.2287], grad_fn=<DivBackward0>)\n",
      "Epoch 126\n",
      " ---------------------- loss: tensor([97.2062], grad_fn=<DivBackward0>)\n",
      "Epoch 127\n",
      " ---------------------- loss: tensor([96.9979], grad_fn=<DivBackward0>)\n",
      "Epoch 128\n",
      " ---------------------- loss: tensor([96.9231], grad_fn=<DivBackward0>)\n",
      "Epoch 129\n",
      " ---------------------- loss: tensor([96.8936], grad_fn=<DivBackward0>)\n",
      "Epoch 130\n",
      " ---------------------- loss: tensor([96.8461], grad_fn=<DivBackward0>)\n",
      "Epoch 131\n",
      " ---------------------- loss: tensor([92.3301], grad_fn=<DivBackward0>)\n",
      "Epoch 132\n",
      " ---------------------- loss: tensor([92.3060], grad_fn=<DivBackward0>)\n",
      "Epoch 133\n",
      " ---------------------- loss: tensor([92.2355], grad_fn=<DivBackward0>)\n",
      "Epoch 134\n",
      " ---------------------- loss: tensor([90.6318], grad_fn=<DivBackward0>)\n",
      "Epoch 135\n",
      " ---------------------- loss: tensor([90.5498], grad_fn=<DivBackward0>)\n",
      "Epoch 136\n",
      " ---------------------- loss: tensor([90.4430], grad_fn=<DivBackward0>)\n",
      "Epoch 137\n",
      " ---------------------- loss: tensor([90.2706], grad_fn=<DivBackward0>)\n",
      "Epoch 138\n",
      " ---------------------- loss: tensor([90.0871], grad_fn=<DivBackward0>)\n",
      "Epoch 139\n",
      " ---------------------- loss: tensor([89.9528], grad_fn=<DivBackward0>)\n",
      "Epoch 140\n",
      " ---------------------- loss: tensor([89.5531], grad_fn=<DivBackward0>)\n",
      "Epoch 141\n",
      " ---------------------- loss: tensor([89.0574], grad_fn=<DivBackward0>)\n",
      "Epoch 142\n",
      " ---------------------- loss: tensor([88.8111], grad_fn=<DivBackward0>)\n",
      "Epoch 143\n",
      " ---------------------- loss: tensor([88.5834], grad_fn=<DivBackward0>)\n",
      "Epoch 144\n",
      " ---------------------- loss: tensor([88.3880], grad_fn=<DivBackward0>)\n",
      "Epoch 145\n",
      " ---------------------- loss: tensor([87.1442], grad_fn=<DivBackward0>)\n",
      "Epoch 146\n",
      " ---------------------- loss: tensor([86.7924], grad_fn=<DivBackward0>)\n",
      "Epoch 147\n",
      " ---------------------- loss: tensor([86.6665], grad_fn=<DivBackward0>)\n",
      "Epoch 148\n",
      " ---------------------- loss: tensor([86.6502], grad_fn=<DivBackward0>)\n",
      "Epoch 149\n",
      " ---------------------- loss: tensor([86.5539], grad_fn=<DivBackward0>)\n",
      "Epoch 150\n",
      " ---------------------- loss: tensor([86.4923], grad_fn=<DivBackward0>)\n",
      "Epoch 151\n",
      " ---------------------- loss: tensor([86.3908], grad_fn=<DivBackward0>)\n",
      "Epoch 152\n",
      " ---------------------- loss: tensor([84.6246], grad_fn=<DivBackward0>)\n",
      "Epoch 153\n",
      " ---------------------- loss: tensor([84.6200], grad_fn=<DivBackward0>)\n",
      "Epoch 154\n",
      " ---------------------- loss: tensor([84.6037], grad_fn=<DivBackward0>)\n",
      "Epoch 155\n",
      " ---------------------- loss: tensor([83.5238], grad_fn=<DivBackward0>)\n",
      "Epoch 156\n",
      " ---------------------- loss: tensor([83.2061], grad_fn=<DivBackward0>)\n",
      "Epoch 157\n",
      " ---------------------- loss: tensor([83.1130], grad_fn=<DivBackward0>)\n",
      "Epoch 158\n",
      " ---------------------- loss: tensor([83.0052], grad_fn=<DivBackward0>)\n",
      "Epoch 159\n",
      " ---------------------- loss: tensor([82.8579], grad_fn=<DivBackward0>)\n",
      "Epoch 160\n",
      " ---------------------- loss: tensor([82.7273], grad_fn=<DivBackward0>)\n",
      "Epoch 161\n",
      " ---------------------- loss: tensor([82.4511], grad_fn=<DivBackward0>)\n",
      "Epoch 162\n",
      " ---------------------- loss: tensor([146.9030], grad_fn=<DivBackward0>)\n",
      "Epoch 163\n",
      " ---------------------- loss: tensor([146.1699], grad_fn=<DivBackward0>)\n",
      "Epoch 164\n",
      " ---------------------- loss: tensor([145.4225], grad_fn=<DivBackward0>)\n",
      "Epoch 165\n",
      " ---------------------- loss: tensor([144.6727], grad_fn=<DivBackward0>)\n",
      "Epoch 166\n",
      " ---------------------- loss: tensor([143.9198], grad_fn=<DivBackward0>)\n",
      "Epoch 167\n",
      " ---------------------- loss: tensor([143.1456], grad_fn=<DivBackward0>)\n",
      "Epoch 168\n",
      " ---------------------- loss: tensor([142.3425], grad_fn=<DivBackward0>)\n",
      "Epoch 169\n",
      " ---------------------- loss: tensor([141.5518], grad_fn=<DivBackward0>)\n",
      "Epoch 170\n",
      " ---------------------- loss: tensor([140.7446], grad_fn=<DivBackward0>)\n",
      "Epoch 171\n",
      " ---------------------- loss: tensor([139.9044], grad_fn=<DivBackward0>)\n",
      "Epoch 172\n",
      " ---------------------- loss: tensor([139.0448], grad_fn=<DivBackward0>)\n",
      "Epoch 173\n",
      " ---------------------- loss: tensor([138.1751], grad_fn=<DivBackward0>)\n",
      "Epoch 174\n",
      " ---------------------- loss: tensor([137.3012], grad_fn=<DivBackward0>)\n",
      "Epoch 175\n",
      " ---------------------- loss: tensor([136.4219], grad_fn=<DivBackward0>)\n",
      "Epoch 176\n",
      " ---------------------- loss: tensor([135.5388], grad_fn=<DivBackward0>)\n",
      "Epoch 177\n",
      " ---------------------- loss: tensor([134.6239], grad_fn=<DivBackward0>)\n",
      "Epoch 178\n",
      " ---------------------- loss: tensor([133.7029], grad_fn=<DivBackward0>)\n",
      "Epoch 179\n",
      " ---------------------- loss: tensor([132.7716], grad_fn=<DivBackward0>)\n",
      "Epoch 180\n",
      " ---------------------- loss: tensor([131.8232], grad_fn=<DivBackward0>)\n",
      "Epoch 181\n",
      " ---------------------- loss: tensor([130.8569], grad_fn=<DivBackward0>)\n",
      "Epoch 182\n",
      " ---------------------- loss: tensor([129.8821], grad_fn=<DivBackward0>)\n",
      "Epoch 183\n",
      " ---------------------- loss: tensor([128.8913], grad_fn=<DivBackward0>)\n",
      "Epoch 184\n",
      " ---------------------- loss: tensor([127.8849], grad_fn=<DivBackward0>)\n",
      "Epoch 185\n",
      " ---------------------- loss: tensor([126.8677], grad_fn=<DivBackward0>)\n",
      "Epoch 186\n",
      " ---------------------- loss: tensor([125.8392], grad_fn=<DivBackward0>)\n",
      "Epoch 187\n",
      " ---------------------- loss: tensor([124.7964], grad_fn=<DivBackward0>)\n",
      "Epoch 188\n",
      " ---------------------- loss: tensor([123.7470], grad_fn=<DivBackward0>)\n",
      "Epoch 189\n",
      " ---------------------- loss: tensor([122.6845], grad_fn=<DivBackward0>)\n",
      "Epoch 190\n",
      " ---------------------- loss: tensor([121.6151], grad_fn=<DivBackward0>)\n",
      "Epoch 191\n",
      " ---------------------- loss: tensor([120.5285], grad_fn=<DivBackward0>)\n",
      "Epoch 192\n",
      " ---------------------- loss: tensor([119.4370], grad_fn=<DivBackward0>)\n",
      "Epoch 193\n",
      " ---------------------- loss: tensor([118.3424], grad_fn=<DivBackward0>)\n",
      "Epoch 194\n",
      " ---------------------- loss: tensor([117.2372], grad_fn=<DivBackward0>)\n",
      "Epoch 195\n",
      " ---------------------- loss: tensor([116.1253], grad_fn=<DivBackward0>)\n",
      "Epoch 196\n",
      " ---------------------- loss: tensor([115.0049], grad_fn=<DivBackward0>)\n",
      "Epoch 197\n",
      " ---------------------- loss: tensor([113.8795], grad_fn=<DivBackward0>)\n",
      "Epoch 198\n",
      " ---------------------- loss: tensor([112.7547], grad_fn=<DivBackward0>)\n",
      "Epoch 199\n",
      " ---------------------- loss: tensor([111.6231], grad_fn=<DivBackward0>)\n",
      "Epoch 200\n",
      " ---------------------- loss: tensor([86.4598], grad_fn=<DivBackward0>)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "initialize_weights(model)\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=0.0001)\n",
    "upper_r = 6\n",
    "lower_r = 1e-2\n",
    "steps = 100\n",
    "R_train = torch.Tensor(np.linspace(lower_r, upper_r, steps)[:,None])\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n ---------------------- loss: {loss_fn(R_train.to(device))}\")\n",
    "    training(R_train, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e38ff0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1649], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f1216e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f10b2922940>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rr = np.linspace(lower_r, upper_r, steps)[:,None]\n",
    "\n",
    "with torch.no_grad():\n",
    "    yy = Phi_t(torch.Tensor(rr).to(device)).cpu().numpy()\n",
    "#yt = xx**2 + np.exp(-xx**2 / 2)/(1+xx+xx**3)\n",
    "\n",
    "fig, axs = plt.subplots(dpi=100)\n",
    "#axs.plot(xx, yt, label=\"True\")\n",
    "axs.plot(rr, yy, \".\", label=\"Neural Network Approximation\")\n",
    "axs.set_xlabel(\"$x$\")\n",
    "axs.set_ylabel(\"$\\phi(x)$\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e750e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b41d6f38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " ---------------------- loss: tensor([16123.1299], grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      " ---------------------- loss: tensor([14351.2256], grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      " ---------------------- loss: tensor([9189.5664], grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      " ---------------------- loss: tensor([17826.1875], grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      " ---------------------- loss: tensor([757.8359], grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      " ---------------------- loss: tensor([534.0950], grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      " ---------------------- loss: tensor([515.5474], grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      " ---------------------- loss: tensor([366.2880], grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      " ---------------------- loss: tensor([330.3906], grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      " ---------------------- loss: tensor([318.8073], grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      " ---------------------- loss: tensor([302.4662], grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      " ---------------------- loss: tensor([293.5590], grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      " ---------------------- loss: tensor([193.8883], grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      " ---------------------- loss: tensor([189.6454], grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      " ---------------------- loss: tensor([187.9212], grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      " ---------------------- loss: tensor([180.8159], grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      " ---------------------- loss: tensor([167.3514], grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      " ---------------------- loss: tensor([156.7144], grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      " ---------------------- loss: tensor([151.5005], grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      " ---------------------- loss: tensor([146.2237], grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      " ---------------------- loss: tensor([134.1278], grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      " ---------------------- loss: tensor([98.9530], grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      " ---------------------- loss: tensor([81.5560], grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      " ---------------------- loss: tensor([77.0327], grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      " ---------------------- loss: tensor([71.7115], grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      " ---------------------- loss: tensor([17.1944], grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      " ---------------------- loss: tensor([15.7470], grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      " ---------------------- loss: tensor([12.1457], grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      " ---------------------- loss: tensor([10.4168], grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      " ---------------------- loss: tensor([8.9032], grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      " ---------------------- loss: tensor([8.8598], grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      " ---------------------- loss: tensor([8.8504], grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      " ---------------------- loss: tensor([8.8447], grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      " ---------------------- loss: tensor([8.8293], grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      " ---------------------- loss: tensor([8.1035], grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      " ---------------------- loss: tensor([7.6066], grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      " ---------------------- loss: tensor([7.5273], grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      " ---------------------- loss: tensor([7.2847], grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      " ---------------------- loss: tensor([6.8804], grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      " ---------------------- loss: tensor([6.1154], grad_fn=<DivBackward0>)\n",
      "Epoch 41\n",
      " ---------------------- loss: tensor([5.0176], grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      " ---------------------- loss: tensor([4.2605], grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      " ---------------------- loss: tensor([3.9528], grad_fn=<DivBackward0>)\n",
      "Epoch 44\n",
      " ---------------------- loss: tensor([3.8840], grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      " ---------------------- loss: tensor([3.8641], grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      " ---------------------- loss: tensor([3.8146], grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      " ---------------------- loss: tensor([3.7948], grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      " ---------------------- loss: tensor([3.7781], grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      " ---------------------- loss: tensor([3.7687], grad_fn=<DivBackward0>)\n",
      "Epoch 50\n",
      " ---------------------- loss: tensor([3.7634], grad_fn=<DivBackward0>)\n",
      "Epoch 51\n",
      " ---------------------- loss: tensor([3.7572], grad_fn=<DivBackward0>)\n",
      "Epoch 52\n",
      " ---------------------- loss: tensor([3.7488], grad_fn=<DivBackward0>)\n",
      "Epoch 53\n",
      " ---------------------- loss: tensor([3.7198], grad_fn=<DivBackward0>)\n",
      "Epoch 54\n",
      " ---------------------- loss: tensor([3.7021], grad_fn=<DivBackward0>)\n",
      "Epoch 55\n",
      " ---------------------- loss: tensor([3.6834], grad_fn=<DivBackward0>)\n",
      "Epoch 56\n",
      " ---------------------- loss: tensor([3.6777], grad_fn=<DivBackward0>)\n",
      "Epoch 57\n",
      " ---------------------- loss: tensor([3.6755], grad_fn=<DivBackward0>)\n",
      "Epoch 58\n",
      " ---------------------- loss: tensor([3.6745], grad_fn=<DivBackward0>)\n",
      "Epoch 59\n",
      " ---------------------- loss: tensor([3.6728], grad_fn=<DivBackward0>)\n",
      "Epoch 60\n",
      " ---------------------- loss: tensor([3.6696], grad_fn=<DivBackward0>)\n",
      "Epoch 61\n",
      " ---------------------- loss: tensor([3.6646], grad_fn=<DivBackward0>)\n",
      "Epoch 62\n",
      " ---------------------- loss: tensor([3.6636], grad_fn=<DivBackward0>)\n",
      "Epoch 63\n",
      " ---------------------- loss: tensor([3.6620], grad_fn=<DivBackward0>)\n",
      "Epoch 64\n",
      " ---------------------- loss: tensor([3.6590], grad_fn=<DivBackward0>)\n",
      "Epoch 65\n",
      " ---------------------- loss: tensor([3.6527], grad_fn=<DivBackward0>)\n",
      "Epoch 66\n",
      " ---------------------- loss: tensor([3.6433], grad_fn=<DivBackward0>)\n",
      "Epoch 67\n",
      " ---------------------- loss: tensor([3.6312], grad_fn=<DivBackward0>)\n",
      "Epoch 68\n",
      " ---------------------- loss: tensor([3.6151], grad_fn=<DivBackward0>)\n",
      "Epoch 69\n",
      " ---------------------- loss: tensor([3.5909], grad_fn=<DivBackward0>)\n",
      "Epoch 70\n",
      " ---------------------- loss: tensor([3.5338], grad_fn=<DivBackward0>)\n",
      "Epoch 71\n",
      " ---------------------- loss: tensor([3.4103], grad_fn=<DivBackward0>)\n",
      "Epoch 72\n",
      " ---------------------- loss: tensor([3.1967], grad_fn=<DivBackward0>)\n",
      "Epoch 73\n",
      " ---------------------- loss: tensor([2.9135], grad_fn=<DivBackward0>)\n",
      "Epoch 74\n",
      " ---------------------- loss: tensor([2.6691], grad_fn=<DivBackward0>)\n",
      "Epoch 75\n",
      " ---------------------- loss: tensor([2.3914], grad_fn=<DivBackward0>)\n",
      "Epoch 76\n",
      " ---------------------- loss: tensor([1.9553], grad_fn=<DivBackward0>)\n",
      "Epoch 77\n",
      " ---------------------- loss: tensor([1.6310], grad_fn=<DivBackward0>)\n",
      "Epoch 78\n",
      " ---------------------- loss: tensor([1.3345], grad_fn=<DivBackward0>)\n",
      "Epoch 79\n",
      " ---------------------- loss: tensor([0.7516], grad_fn=<DivBackward0>)\n",
      "Epoch 80\n",
      " ---------------------- loss: tensor([0.4663], grad_fn=<DivBackward0>)\n",
      "Epoch 81\n",
      " ---------------------- loss: tensor([0.3719], grad_fn=<DivBackward0>)\n",
      "Epoch 82\n",
      " ---------------------- loss: tensor([0.2989], grad_fn=<DivBackward0>)\n",
      "Epoch 83\n",
      " ---------------------- loss: tensor([0.2904], grad_fn=<DivBackward0>)\n",
      "Epoch 84\n",
      " ---------------------- loss: tensor([0.2842], grad_fn=<DivBackward0>)\n",
      "Epoch 85\n",
      " ---------------------- loss: tensor([0.2638], grad_fn=<DivBackward0>)\n",
      "Epoch 86\n",
      " ---------------------- loss: tensor([0.2391], grad_fn=<DivBackward0>)\n",
      "Epoch 87\n",
      " ---------------------- loss: tensor([0.1991], grad_fn=<DivBackward0>)\n",
      "Epoch 88\n",
      " ---------------------- loss: tensor([0.1645], grad_fn=<DivBackward0>)\n",
      "Epoch 89\n",
      " ---------------------- loss: tensor([0.1383], grad_fn=<DivBackward0>)\n",
      "Epoch 90\n",
      " ---------------------- loss: tensor([0.1158], grad_fn=<DivBackward0>)\n",
      "Epoch 91\n",
      " ---------------------- loss: tensor([0.1017], grad_fn=<DivBackward0>)\n",
      "Epoch 92\n",
      " ---------------------- loss: tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "Epoch 93\n",
      " ---------------------- loss: tensor([0.0824], grad_fn=<DivBackward0>)\n",
      "Epoch 94\n",
      " ---------------------- loss: tensor([0.0770], grad_fn=<DivBackward0>)\n",
      "Epoch 95\n",
      " ---------------------- loss: tensor([0.0737], grad_fn=<DivBackward0>)\n",
      "Epoch 96\n",
      " ---------------------- loss: tensor([0.0701], grad_fn=<DivBackward0>)\n",
      "Epoch 97\n",
      " ---------------------- loss: tensor([0.0680], grad_fn=<DivBackward0>)\n",
      "Epoch 98\n",
      " ---------------------- loss: tensor([0.0677], grad_fn=<DivBackward0>)\n",
      "Epoch 99\n",
      " ---------------------- loss: tensor([0.0630], grad_fn=<DivBackward0>)\n",
      "Epoch 100\n",
      " ---------------------- loss: tensor([0.0612], grad_fn=<DivBackward0>)\n",
      "Epoch 101\n",
      " ---------------------- loss: tensor([0.0706], grad_fn=<DivBackward0>)\n",
      "Epoch 102\n",
      " ---------------------- loss: tensor([0.0555], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103\n",
      " ---------------------- loss: tensor([0.0550], grad_fn=<DivBackward0>)\n",
      "Epoch 104\n",
      " ---------------------- loss: tensor([0.0520], grad_fn=<DivBackward0>)\n",
      "Epoch 105\n",
      " ---------------------- loss: tensor([0.0489], grad_fn=<DivBackward0>)\n",
      "Epoch 106\n",
      " ---------------------- loss: tensor([0.0486], grad_fn=<DivBackward0>)\n",
      "Epoch 107\n",
      " ---------------------- loss: tensor([0.0459], grad_fn=<DivBackward0>)\n",
      "Epoch 108\n",
      " ---------------------- loss: tensor([0.0430], grad_fn=<DivBackward0>)\n",
      "Epoch 109\n",
      " ---------------------- loss: tensor([0.3362], grad_fn=<DivBackward0>)\n",
      "Epoch 110\n",
      " ---------------------- loss: tensor([0.0513], grad_fn=<DivBackward0>)\n",
      "Epoch 111\n",
      " ---------------------- loss: tensor([0.0364], grad_fn=<DivBackward0>)\n",
      "Epoch 112\n",
      " ---------------------- loss: tensor([0.0352], grad_fn=<DivBackward0>)\n",
      "Epoch 113\n",
      " ---------------------- loss: tensor([0.0336], grad_fn=<DivBackward0>)\n",
      "Epoch 114\n",
      " ---------------------- loss: tensor([0.0348], grad_fn=<DivBackward0>)\n",
      "Epoch 115\n",
      " ---------------------- loss: tensor([0.0336], grad_fn=<DivBackward0>)\n",
      "Epoch 116\n",
      " ---------------------- loss: tensor([0.0318], grad_fn=<DivBackward0>)\n",
      "Epoch 117\n",
      " ---------------------- loss: tensor([0.0287], grad_fn=<DivBackward0>)\n",
      "Epoch 118\n",
      " ---------------------- loss: tensor([0.0286], grad_fn=<DivBackward0>)\n",
      "Epoch 119\n",
      " ---------------------- loss: tensor([0.0283], grad_fn=<DivBackward0>)\n",
      "Epoch 120\n",
      " ---------------------- loss: tensor([0.0277], grad_fn=<DivBackward0>)\n",
      "Epoch 121\n",
      " ---------------------- loss: tensor([0.0277], grad_fn=<DivBackward0>)\n",
      "Epoch 122\n",
      " ---------------------- loss: tensor([0.0286], grad_fn=<DivBackward0>)\n",
      "Epoch 123\n",
      " ---------------------- loss: tensor([0.0305], grad_fn=<DivBackward0>)\n",
      "Epoch 124\n",
      " ---------------------- loss: tensor([0.0390], grad_fn=<DivBackward0>)\n",
      "Epoch 125\n",
      " ---------------------- loss: tensor([0.1063], grad_fn=<DivBackward0>)\n",
      "Epoch 126\n",
      " ---------------------- loss: tensor([0.0233], grad_fn=<DivBackward0>)\n",
      "Epoch 127\n",
      " ---------------------- loss: tensor([0.0224], grad_fn=<DivBackward0>)\n",
      "Epoch 128\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 129\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 130\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 131\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 132\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 133\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 134\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 135\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 136\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 137\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 138\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 139\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 140\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 141\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 142\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 143\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 144\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 145\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 146\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 147\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 148\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 149\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 150\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 151\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 152\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 153\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 154\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 155\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 156\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 157\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 158\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 159\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 160\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 161\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 162\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 163\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 164\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 165\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 166\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 167\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 168\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 169\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 170\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 171\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 172\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 173\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 174\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 175\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 176\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 177\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 178\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 179\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 180\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 181\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 182\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 183\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 184\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 185\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 186\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 187\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 188\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 189\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 190\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 191\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 192\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 193\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 194\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 195\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 196\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 197\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 198\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 199\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 200\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 201\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 202\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 203\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 204\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 205\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 206\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 207\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 208\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 209\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 210\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 211\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 212\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 213\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 214\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 215\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 216\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 217\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 218\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 219\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 220\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 221\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 222\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 223\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 224\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 225\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 226\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 227\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 228\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 229\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 230\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 231\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 232\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 233\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 234\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 235\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 236\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 237\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 238\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 239\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 240\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 241\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 242\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 243\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 245\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 246\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 247\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 248\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 249\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 250\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 251\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 252\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 253\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 254\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 255\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 256\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 257\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 258\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 259\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 260\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 261\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 262\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 263\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 264\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 265\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 266\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 267\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 268\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 269\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 270\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 271\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 272\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 273\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 274\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 275\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 276\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 277\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 278\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 279\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 280\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 281\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 282\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 283\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 284\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 285\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 286\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 287\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 288\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 289\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 290\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 291\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 292\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 293\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 294\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 295\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 296\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 297\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 298\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 299\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 300\n",
      " ---------------------- loss: tensor([19809.6270], grad_fn=<DivBackward0>)\n",
      "Done!\n",
      "\n",
      "\n",
      "Epoch 1\n",
      " ---------------------- loss: tensor([17374.6797], grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      " ---------------------- loss: tensor([995.4298], grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      " ---------------------- loss: tensor([174.3798], grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      " ---------------------- loss: tensor([81.3231], grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      " ---------------------- loss: tensor([70.4370], grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      " ---------------------- loss: tensor([69.0866], grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      " ---------------------- loss: tensor([68.9029], grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      " ---------------------- loss: tensor([68.8246], grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      " ---------------------- loss: tensor([68.5472], grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      " ---------------------- loss: tensor([139.8785], grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      " ---------------------- loss: tensor([2902.6626], grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      " ---------------------- loss: tensor([350.5857], grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      " ---------------------- loss: tensor([88.2951], grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      " ---------------------- loss: tensor([69.0849], grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      " ---------------------- loss: tensor([65.1826], grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      " ---------------------- loss: tensor([63.5854], grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      " ---------------------- loss: tensor([62.4994], grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      " ---------------------- loss: tensor([62.0735], grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      " ---------------------- loss: tensor([61.8147], grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      " ---------------------- loss: tensor([61.0698], grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      " ---------------------- loss: tensor([60.9907], grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      " ---------------------- loss: tensor([61.0060], grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      " ---------------------- loss: tensor([60.9835], grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      " ---------------------- loss: tensor([60.9787], grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      " ---------------------- loss: tensor([60.9318], grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      " ---------------------- loss: tensor([60.9208], grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      " ---------------------- loss: tensor([60.9149], grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      " ---------------------- loss: tensor([28222.4902], grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      " ---------------------- loss: tensor([28222.2676], grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      " ---------------------- loss: tensor([28222.0469], grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      " ---------------------- loss: tensor([28222.0469], grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      " ---------------------- loss: tensor([28221.8828], grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      " ---------------------- loss: tensor([28221.6621], grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      " ---------------------- loss: tensor([28221.4258], grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      " ---------------------- loss: tensor([28221.3438], grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      " ---------------------- loss: tensor([28221.2207], grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      " ---------------------- loss: tensor([28221.0039], grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      " ---------------------- loss: tensor([28220.7715], grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      " ---------------------- loss: tensor([28220.5488], grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      " ---------------------- loss: tensor([28220.3320], grad_fn=<DivBackward0>)\n",
      "Epoch 41\n",
      " ---------------------- loss: tensor([28220.1738], grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      " ---------------------- loss: tensor([28219.9473], grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      " ---------------------- loss: tensor([28219.8711], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44\n",
      " ---------------------- loss: tensor([28219.6406], grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      " ---------------------- loss: tensor([28219.5020], grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      " ---------------------- loss: tensor([28219.2695], grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      " ---------------------- loss: tensor([28219.2090], grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      " ---------------------- loss: tensor([28218.9863], grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      " ---------------------- loss: tensor([28218.7598], grad_fn=<DivBackward0>)\n",
      "Epoch 50\n",
      " ---------------------- loss: tensor([28218.5312], grad_fn=<DivBackward0>)\n",
      "Epoch 51\n",
      " ---------------------- loss: tensor([28218.5176], grad_fn=<DivBackward0>)\n",
      "Epoch 52\n",
      " ---------------------- loss: tensor([28218.4824], grad_fn=<DivBackward0>)\n",
      "Epoch 53\n",
      " ---------------------- loss: tensor([28218.2539], grad_fn=<DivBackward0>)\n",
      "Epoch 54\n",
      " ---------------------- loss: tensor([28218.0996], grad_fn=<DivBackward0>)\n",
      "Epoch 55\n",
      " ---------------------- loss: tensor([28217.8770], grad_fn=<DivBackward0>)\n",
      "Epoch 56\n",
      " ---------------------- loss: tensor([28217.8594], grad_fn=<DivBackward0>)\n",
      "Epoch 57\n",
      " ---------------------- loss: tensor([28217.6289], grad_fn=<DivBackward0>)\n",
      "Epoch 58\n",
      " ---------------------- loss: tensor([28217.5977], grad_fn=<DivBackward0>)\n",
      "Epoch 59\n",
      " ---------------------- loss: tensor([28217.4004], grad_fn=<DivBackward0>)\n",
      "Epoch 60\n",
      " ---------------------- loss: tensor([28217.1738], grad_fn=<DivBackward0>)\n",
      "Epoch 61\n",
      " ---------------------- loss: tensor([28216.9941], grad_fn=<DivBackward0>)\n",
      "Epoch 62\n",
      " ---------------------- loss: tensor([28216.7578], grad_fn=<DivBackward0>)\n",
      "Epoch 63\n",
      " ---------------------- loss: tensor([28216.7578], grad_fn=<DivBackward0>)\n",
      "Epoch 64\n",
      " ---------------------- loss: tensor([28216.5352], grad_fn=<DivBackward0>)\n",
      "Epoch 65\n",
      " ---------------------- loss: tensor([28216.3438], grad_fn=<DivBackward0>)\n",
      "Epoch 66\n",
      " ---------------------- loss: tensor([28216.1133], grad_fn=<DivBackward0>)\n",
      "Epoch 67\n",
      " ---------------------- loss: tensor([28215.8867], grad_fn=<DivBackward0>)\n",
      "Epoch 68\n",
      " ---------------------- loss: tensor([28215.6602], grad_fn=<DivBackward0>)\n",
      "Epoch 69\n",
      " ---------------------- loss: tensor([28215.5371], grad_fn=<DivBackward0>)\n",
      "Epoch 70\n",
      " ---------------------- loss: tensor([28215.3066], grad_fn=<DivBackward0>)\n",
      "Epoch 71\n",
      " ---------------------- loss: tensor([28215.0820], grad_fn=<DivBackward0>)\n",
      "Epoch 72\n",
      " ---------------------- loss: tensor([28214.8477], grad_fn=<DivBackward0>)\n",
      "Epoch 73\n",
      " ---------------------- loss: tensor([28214.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 74\n",
      " ---------------------- loss: tensor([28214.4316], grad_fn=<DivBackward0>)\n",
      "Epoch 75\n",
      " ---------------------- loss: tensor([28214.3848], grad_fn=<DivBackward0>)\n",
      "Epoch 76\n",
      " ---------------------- loss: tensor([28214.3418], grad_fn=<DivBackward0>)\n",
      "Epoch 77\n",
      " ---------------------- loss: tensor([28214.1035], grad_fn=<DivBackward0>)\n",
      "Epoch 78\n",
      " ---------------------- loss: tensor([28213.8887], grad_fn=<DivBackward0>)\n",
      "Epoch 79\n",
      " ---------------------- loss: tensor([28213.6602], grad_fn=<DivBackward0>)\n",
      "Epoch 80\n",
      " ---------------------- loss: tensor([28213.4316], grad_fn=<DivBackward0>)\n",
      "Epoch 81\n",
      " ---------------------- loss: tensor([28213.2070], grad_fn=<DivBackward0>)\n",
      "Epoch 82\n",
      " ---------------------- loss: tensor([28213.0566], grad_fn=<DivBackward0>)\n",
      "Epoch 83\n",
      " ---------------------- loss: tensor([28212.8340], grad_fn=<DivBackward0>)\n",
      "Epoch 84\n",
      " ---------------------- loss: tensor([28212.6973], grad_fn=<DivBackward0>)\n",
      "Epoch 85\n",
      " ---------------------- loss: tensor([28212.5625], grad_fn=<DivBackward0>)\n",
      "Epoch 86\n",
      " ---------------------- loss: tensor([28212.4863], grad_fn=<DivBackward0>)\n",
      "Epoch 87\n",
      " ---------------------- loss: tensor([28212.2520], grad_fn=<DivBackward0>)\n",
      "Epoch 88\n",
      " ---------------------- loss: tensor([28212.0195], grad_fn=<DivBackward0>)\n",
      "Epoch 89\n",
      " ---------------------- loss: tensor([28212.0195], grad_fn=<DivBackward0>)\n",
      "Epoch 90\n",
      " ---------------------- loss: tensor([28211.7930], grad_fn=<DivBackward0>)\n",
      "Epoch 91\n",
      " ---------------------- loss: tensor([28211.5605], grad_fn=<DivBackward0>)\n",
      "Epoch 92\n",
      " ---------------------- loss: tensor([28211.3359], grad_fn=<DivBackward0>)\n",
      "Epoch 93\n",
      " ---------------------- loss: tensor([28211.1152], grad_fn=<DivBackward0>)\n",
      "Epoch 94\n",
      " ---------------------- loss: tensor([28210.9531], grad_fn=<DivBackward0>)\n",
      "Epoch 95\n",
      " ---------------------- loss: tensor([28210.7227], grad_fn=<DivBackward0>)\n",
      "Epoch 96\n",
      " ---------------------- loss: tensor([28210.7051], grad_fn=<DivBackward0>)\n",
      "Epoch 97\n",
      " ---------------------- loss: tensor([28210.4688], grad_fn=<DivBackward0>)\n",
      "Epoch 98\n",
      " ---------------------- loss: tensor([28210.2402], grad_fn=<DivBackward0>)\n",
      "Epoch 99\n",
      " ---------------------- loss: tensor([28210.0176], grad_fn=<DivBackward0>)\n",
      "Epoch 100\n",
      " ---------------------- loss: tensor([28210.0176], grad_fn=<DivBackward0>)\n",
      "Epoch 101\n",
      " ---------------------- loss: tensor([28209.7793], grad_fn=<DivBackward0>)\n",
      "Epoch 102\n",
      " ---------------------- loss: tensor([28209.7090], grad_fn=<DivBackward0>)\n",
      "Epoch 103\n",
      " ---------------------- loss: tensor([28209.6270], grad_fn=<DivBackward0>)\n",
      "Epoch 104\n",
      " ---------------------- loss: tensor([28209.4023], grad_fn=<DivBackward0>)\n",
      "Epoch 105\n",
      " ---------------------- loss: tensor([28209.1758], grad_fn=<DivBackward0>)\n",
      "Epoch 106\n",
      " ---------------------- loss: tensor([28208.9453], grad_fn=<DivBackward0>)\n",
      "Epoch 107\n",
      " ---------------------- loss: tensor([28208.7188], grad_fn=<DivBackward0>)\n",
      "Epoch 108\n",
      " ---------------------- loss: tensor([28208.6484], grad_fn=<DivBackward0>)\n",
      "Epoch 109\n",
      " ---------------------- loss: tensor([28208.4121], grad_fn=<DivBackward0>)\n",
      "Epoch 110\n",
      " ---------------------- loss: tensor([28208.1895], grad_fn=<DivBackward0>)\n",
      "Epoch 111\n",
      " ---------------------- loss: tensor([28207.9668], grad_fn=<DivBackward0>)\n",
      "Epoch 112\n",
      " ---------------------- loss: tensor([28207.9473], grad_fn=<DivBackward0>)\n",
      "Epoch 113\n",
      " ---------------------- loss: tensor([28207.8770], grad_fn=<DivBackward0>)\n",
      "Epoch 114\n",
      " ---------------------- loss: tensor([28207.6504], grad_fn=<DivBackward0>)\n",
      "Epoch 115\n",
      " ---------------------- loss: tensor([28207.4180], grad_fn=<DivBackward0>)\n",
      "Epoch 116\n",
      " ---------------------- loss: tensor([28207.1973], grad_fn=<DivBackward0>)\n",
      "Epoch 117\n",
      " ---------------------- loss: tensor([28206.9648], grad_fn=<DivBackward0>)\n",
      "Epoch 118\n",
      " ---------------------- loss: tensor([28206.8613], grad_fn=<DivBackward0>)\n",
      "Epoch 119\n",
      " ---------------------- loss: tensor([28206.6309], grad_fn=<DivBackward0>)\n",
      "Epoch 120\n",
      " ---------------------- loss: tensor([28206.3945], grad_fn=<DivBackward0>)\n",
      "Epoch 121\n",
      " ---------------------- loss: tensor([28206.3516], grad_fn=<DivBackward0>)\n",
      "Epoch 122\n",
      " ---------------------- loss: tensor([28206.2129], grad_fn=<DivBackward0>)\n",
      "Epoch 123\n",
      " ---------------------- loss: tensor([28205.9785], grad_fn=<DivBackward0>)\n",
      "Epoch 124\n",
      " ---------------------- loss: tensor([28205.7559], grad_fn=<DivBackward0>)\n",
      "Epoch 125\n",
      " ---------------------- loss: tensor([28205.5293], grad_fn=<DivBackward0>)\n",
      "Epoch 126\n",
      " ---------------------- loss: tensor([28205.3008], grad_fn=<DivBackward0>)\n",
      "Epoch 127\n",
      " ---------------------- loss: tensor([28205.1484], grad_fn=<DivBackward0>)\n",
      "Epoch 128\n",
      " ---------------------- loss: tensor([28204.9297], grad_fn=<DivBackward0>)\n",
      "Epoch 129\n",
      " ---------------------- loss: tensor([28204.6973], grad_fn=<DivBackward0>)\n",
      "Epoch 130\n",
      " ---------------------- loss: tensor([28204.4688], grad_fn=<DivBackward0>)\n",
      "Epoch 131\n",
      " ---------------------- loss: tensor([28204.3652], grad_fn=<DivBackward0>)\n",
      "Epoch 132\n",
      " ---------------------- loss: tensor([28204.2852], grad_fn=<DivBackward0>)\n",
      "Epoch 133\n",
      " ---------------------- loss: tensor([28204.0684], grad_fn=<DivBackward0>)\n",
      "Epoch 134\n",
      " ---------------------- loss: tensor([28204.0215], grad_fn=<DivBackward0>)\n",
      "Epoch 135\n",
      " ---------------------- loss: tensor([28203.7949], grad_fn=<DivBackward0>)\n",
      "Epoch 136\n",
      " ---------------------- loss: tensor([28203.5586], grad_fn=<DivBackward0>)\n",
      "Epoch 137\n",
      " ---------------------- loss: tensor([28203.3340], grad_fn=<DivBackward0>)\n",
      "Epoch 138\n",
      " ---------------------- loss: tensor([28203.1016], grad_fn=<DivBackward0>)\n",
      "Epoch 139\n",
      " ---------------------- loss: tensor([28203.0762], grad_fn=<DivBackward0>)\n",
      "Epoch 140\n",
      " ---------------------- loss: tensor([28203.0547], grad_fn=<DivBackward0>)\n",
      "Epoch 141\n",
      " ---------------------- loss: tensor([28202.8184], grad_fn=<DivBackward0>)\n",
      "Epoch 142\n",
      " ---------------------- loss: tensor([28202.5996], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143\n",
      " ---------------------- loss: tensor([28202.3633], grad_fn=<DivBackward0>)\n",
      "Epoch 144\n",
      " ---------------------- loss: tensor([28202.1348], grad_fn=<DivBackward0>)\n",
      "Epoch 145\n",
      " ---------------------- loss: tensor([28201.9102], grad_fn=<DivBackward0>)\n",
      "Epoch 146\n",
      " ---------------------- loss: tensor([28201.6777], grad_fn=<DivBackward0>)\n",
      "Epoch 147\n",
      " ---------------------- loss: tensor([28201.4492], grad_fn=<DivBackward0>)\n",
      "Epoch 148\n",
      " ---------------------- loss: tensor([28201.2227], grad_fn=<DivBackward0>)\n",
      "Epoch 149\n",
      " ---------------------- loss: tensor([28200.9980], grad_fn=<DivBackward0>)\n",
      "Epoch 150\n",
      " ---------------------- loss: tensor([28200.8594], grad_fn=<DivBackward0>)\n",
      "Epoch 151\n",
      " ---------------------- loss: tensor([28200.6309], grad_fn=<DivBackward0>)\n",
      "Epoch 152\n",
      " ---------------------- loss: tensor([28200.4766], grad_fn=<DivBackward0>)\n",
      "Epoch 153\n",
      " ---------------------- loss: tensor([28200.2461], grad_fn=<DivBackward0>)\n",
      "Epoch 154\n",
      " ---------------------- loss: tensor([28200.2188], grad_fn=<DivBackward0>)\n",
      "Epoch 155\n",
      " ---------------------- loss: tensor([28199.9863], grad_fn=<DivBackward0>)\n",
      "Epoch 156\n",
      " ---------------------- loss: tensor([28199.7441], grad_fn=<DivBackward0>)\n",
      "Epoch 157\n",
      " ---------------------- loss: tensor([28199.5195], grad_fn=<DivBackward0>)\n",
      "Epoch 158\n",
      " ---------------------- loss: tensor([28199.2930], grad_fn=<DivBackward0>)\n",
      "Epoch 159\n",
      " ---------------------- loss: tensor([28199.0605], grad_fn=<DivBackward0>)\n",
      "Epoch 160\n",
      " ---------------------- loss: tensor([28198.8867], grad_fn=<DivBackward0>)\n",
      "Epoch 161\n",
      " ---------------------- loss: tensor([28198.6602], grad_fn=<DivBackward0>)\n",
      "Epoch 162\n",
      " ---------------------- loss: tensor([28198.4258], grad_fn=<DivBackward0>)\n",
      "Epoch 163\n",
      " ---------------------- loss: tensor([28198.2012], grad_fn=<DivBackward0>)\n",
      "Epoch 164\n",
      " ---------------------- loss: tensor([28197.9707], grad_fn=<DivBackward0>)\n",
      "Epoch 165\n",
      " ---------------------- loss: tensor([28197.7324], grad_fn=<DivBackward0>)\n",
      "Epoch 166\n",
      " ---------------------- loss: tensor([28197.5020], grad_fn=<DivBackward0>)\n",
      "Epoch 167\n",
      " ---------------------- loss: tensor([28197.4883], grad_fn=<DivBackward0>)\n",
      "Epoch 168\n",
      " ---------------------- loss: tensor([28197.3770], grad_fn=<DivBackward0>)\n",
      "Epoch 169\n",
      " ---------------------- loss: tensor([28197.3086], grad_fn=<DivBackward0>)\n",
      "Epoch 170\n",
      " ---------------------- loss: tensor([28197.2656], grad_fn=<DivBackward0>)\n",
      "Epoch 171\n",
      " ---------------------- loss: tensor([28197.1484], grad_fn=<DivBackward0>)\n",
      "Epoch 172\n",
      " ---------------------- loss: tensor([28196.9180], grad_fn=<DivBackward0>)\n",
      "Epoch 173\n",
      " ---------------------- loss: tensor([28196.7461], grad_fn=<DivBackward0>)\n",
      "Epoch 174\n",
      " ---------------------- loss: tensor([28196.5098], grad_fn=<DivBackward0>)\n",
      "Epoch 175\n",
      " ---------------------- loss: tensor([28196.2773], grad_fn=<DivBackward0>)\n",
      "Epoch 176\n",
      " ---------------------- loss: tensor([28196.0527], grad_fn=<DivBackward0>)\n",
      "Epoch 177\n",
      " ---------------------- loss: tensor([28195.8203], grad_fn=<DivBackward0>)\n",
      "Epoch 178\n",
      " ---------------------- loss: tensor([28195.5957], grad_fn=<DivBackward0>)\n",
      "Epoch 179\n",
      " ---------------------- loss: tensor([28195.4863], grad_fn=<DivBackward0>)\n",
      "Epoch 180\n",
      " ---------------------- loss: tensor([28195.2402], grad_fn=<DivBackward0>)\n",
      "Epoch 181\n",
      " ---------------------- loss: tensor([28195.1934], grad_fn=<DivBackward0>)\n",
      "Epoch 182\n",
      " ---------------------- loss: tensor([28194.9531], grad_fn=<DivBackward0>)\n",
      "Epoch 183\n",
      " ---------------------- loss: tensor([28194.7344], grad_fn=<DivBackward0>)\n",
      "Epoch 184\n",
      " ---------------------- loss: tensor([28194.6738], grad_fn=<DivBackward0>)\n",
      "Epoch 185\n",
      " ---------------------- loss: tensor([28194.4355], grad_fn=<DivBackward0>)\n",
      "Epoch 186\n",
      " ---------------------- loss: tensor([28194.3711], grad_fn=<DivBackward0>)\n",
      "Epoch 187\n",
      " ---------------------- loss: tensor([28194.1367], grad_fn=<DivBackward0>)\n",
      "Epoch 188\n",
      " ---------------------- loss: tensor([28193.9043], grad_fn=<DivBackward0>)\n",
      "Epoch 189\n",
      " ---------------------- loss: tensor([28193.7480], grad_fn=<DivBackward0>)\n",
      "Epoch 190\n",
      " ---------------------- loss: tensor([28193.5078], grad_fn=<DivBackward0>)\n",
      "Epoch 191\n",
      " ---------------------- loss: tensor([28193.3770], grad_fn=<DivBackward0>)\n",
      "Epoch 192\n",
      " ---------------------- loss: tensor([28193.1465], grad_fn=<DivBackward0>)\n",
      "Epoch 193\n",
      " ---------------------- loss: tensor([28192.9102], grad_fn=<DivBackward0>)\n",
      "Epoch 194\n",
      " ---------------------- loss: tensor([28192.6797], grad_fn=<DivBackward0>)\n",
      "Epoch 195\n",
      " ---------------------- loss: tensor([28192.6562], grad_fn=<DivBackward0>)\n",
      "Epoch 196\n",
      " ---------------------- loss: tensor([28192.4258], grad_fn=<DivBackward0>)\n",
      "Epoch 197\n",
      " ---------------------- loss: tensor([28192.1895], grad_fn=<DivBackward0>)\n",
      "Epoch 198\n",
      " ---------------------- loss: tensor([28191.9570], grad_fn=<DivBackward0>)\n",
      "Epoch 199\n",
      " ---------------------- loss: tensor([28191.7266], grad_fn=<DivBackward0>)\n",
      "Epoch 200\n",
      " ---------------------- loss: tensor([28191.4961], grad_fn=<DivBackward0>)\n",
      "Epoch 201\n",
      " ---------------------- loss: tensor([28191.2969], grad_fn=<DivBackward0>)\n",
      "Epoch 202\n",
      " ---------------------- loss: tensor([28191.0625], grad_fn=<DivBackward0>)\n",
      "Epoch 203\n",
      " ---------------------- loss: tensor([28190.8281], grad_fn=<DivBackward0>)\n",
      "Epoch 204\n",
      " ---------------------- loss: tensor([28190.5918], grad_fn=<DivBackward0>)\n",
      "Epoch 205\n",
      " ---------------------- loss: tensor([28190.3652], grad_fn=<DivBackward0>)\n",
      "Epoch 206\n",
      " ---------------------- loss: tensor([28190.1230], grad_fn=<DivBackward0>)\n",
      "Epoch 207\n",
      " ---------------------- loss: tensor([28189.9004], grad_fn=<DivBackward0>)\n",
      "Epoch 208\n",
      " ---------------------- loss: tensor([28189.6641], grad_fn=<DivBackward0>)\n",
      "Epoch 209\n",
      " ---------------------- loss: tensor([28189.4258], grad_fn=<DivBackward0>)\n",
      "Epoch 210\n",
      " ---------------------- loss: tensor([28189.1953], grad_fn=<DivBackward0>)\n",
      "Epoch 211\n",
      " ---------------------- loss: tensor([28188.9688], grad_fn=<DivBackward0>)\n",
      "Epoch 212\n",
      " ---------------------- loss: tensor([28188.7344], grad_fn=<DivBackward0>)\n",
      "Epoch 213\n",
      " ---------------------- loss: tensor([28188.4961], grad_fn=<DivBackward0>)\n",
      "Epoch 214\n",
      " ---------------------- loss: tensor([28188.2676], grad_fn=<DivBackward0>)\n",
      "Epoch 215\n",
      " ---------------------- loss: tensor([28188.0312], grad_fn=<DivBackward0>)\n",
      "Epoch 216\n",
      " ---------------------- loss: tensor([28187.8867], grad_fn=<DivBackward0>)\n",
      "Epoch 217\n",
      " ---------------------- loss: tensor([28187.6504], grad_fn=<DivBackward0>)\n",
      "Epoch 218\n",
      " ---------------------- loss: tensor([28187.6191], grad_fn=<DivBackward0>)\n",
      "Epoch 219\n",
      " ---------------------- loss: tensor([28187.3945], grad_fn=<DivBackward0>)\n",
      "Epoch 220\n",
      " ---------------------- loss: tensor([28187.1602], grad_fn=<DivBackward0>)\n",
      "Epoch 221\n",
      " ---------------------- loss: tensor([28186.9277], grad_fn=<DivBackward0>)\n",
      "Epoch 222\n",
      " ---------------------- loss: tensor([28186.6914], grad_fn=<DivBackward0>)\n",
      "Epoch 223\n",
      " ---------------------- loss: tensor([28186.4590], grad_fn=<DivBackward0>)\n",
      "Epoch 224\n",
      " ---------------------- loss: tensor([28186.2344], grad_fn=<DivBackward0>)\n",
      "Epoch 225\n",
      " ---------------------- loss: tensor([28185.9805], grad_fn=<DivBackward0>)\n",
      "Epoch 226\n",
      " ---------------------- loss: tensor([28185.7480], grad_fn=<DivBackward0>)\n",
      "Epoch 227\n",
      " ---------------------- loss: tensor([28185.5098], grad_fn=<DivBackward0>)\n",
      "Epoch 228\n",
      " ---------------------- loss: tensor([28185.2812], grad_fn=<DivBackward0>)\n",
      "Epoch 229\n",
      " ---------------------- loss: tensor([28185.0430], grad_fn=<DivBackward0>)\n",
      "Epoch 230\n",
      " ---------------------- loss: tensor([28184.7988], grad_fn=<DivBackward0>)\n",
      "Epoch 231\n",
      " ---------------------- loss: tensor([28184.7324], grad_fn=<DivBackward0>)\n",
      "Epoch 232\n",
      " ---------------------- loss: tensor([28184.4902], grad_fn=<DivBackward0>)\n",
      "Epoch 233\n",
      " ---------------------- loss: tensor([28184.2500], grad_fn=<DivBackward0>)\n",
      "Epoch 234\n",
      " ---------------------- loss: tensor([28184.0098], grad_fn=<DivBackward0>)\n",
      "Epoch 235\n",
      " ---------------------- loss: tensor([28183.7734], grad_fn=<DivBackward0>)\n",
      "Epoch 236\n",
      " ---------------------- loss: tensor([28183.7480], grad_fn=<DivBackward0>)\n",
      "Epoch 237\n",
      " ---------------------- loss: tensor([28183.7129], grad_fn=<DivBackward0>)\n",
      "Epoch 238\n",
      " ---------------------- loss: tensor([28183.4629], grad_fn=<DivBackward0>)\n",
      "Epoch 239\n",
      " ---------------------- loss: tensor([28183.3574], grad_fn=<DivBackward0>)\n",
      "Epoch 240\n",
      " ---------------------- loss: tensor([28183.1094], grad_fn=<DivBackward0>)\n",
      "Epoch 241\n",
      " ---------------------- loss: tensor([28182.8770], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242\n",
      " ---------------------- loss: tensor([28182.6914], grad_fn=<DivBackward0>)\n",
      "Epoch 243\n",
      " ---------------------- loss: tensor([28182.4473], grad_fn=<DivBackward0>)\n",
      "Epoch 244\n",
      " ---------------------- loss: tensor([28182.1992], grad_fn=<DivBackward0>)\n",
      "Epoch 245\n",
      " ---------------------- loss: tensor([28181.9727], grad_fn=<DivBackward0>)\n",
      "Epoch 246\n",
      " ---------------------- loss: tensor([28181.7324], grad_fn=<DivBackward0>)\n",
      "Epoch 247\n",
      " ---------------------- loss: tensor([28181.4902], grad_fn=<DivBackward0>)\n",
      "Epoch 248\n",
      " ---------------------- loss: tensor([28181.2500], grad_fn=<DivBackward0>)\n",
      "Epoch 249\n",
      " ---------------------- loss: tensor([28181.0117], grad_fn=<DivBackward0>)\n",
      "Epoch 250\n",
      " ---------------------- loss: tensor([28180.7734], grad_fn=<DivBackward0>)\n",
      "Epoch 251\n",
      " ---------------------- loss: tensor([28180.5352], grad_fn=<DivBackward0>)\n",
      "Epoch 252\n",
      " ---------------------- loss: tensor([28180.3066], grad_fn=<DivBackward0>)\n",
      "Epoch 253\n",
      " ---------------------- loss: tensor([28180.0586], grad_fn=<DivBackward0>)\n",
      "Epoch 254\n",
      " ---------------------- loss: tensor([28179.8262], grad_fn=<DivBackward0>)\n",
      "Epoch 255\n",
      " ---------------------- loss: tensor([28179.5840], grad_fn=<DivBackward0>)\n",
      "Epoch 256\n",
      " ---------------------- loss: tensor([28179.5664], grad_fn=<DivBackward0>)\n",
      "Epoch 257\n",
      " ---------------------- loss: tensor([28179.3223], grad_fn=<DivBackward0>)\n",
      "Epoch 258\n",
      " ---------------------- loss: tensor([28179.0938], grad_fn=<DivBackward0>)\n",
      "Epoch 259\n",
      " ---------------------- loss: tensor([28178.8496], grad_fn=<DivBackward0>)\n",
      "Epoch 260\n",
      " ---------------------- loss: tensor([28178.6016], grad_fn=<DivBackward0>)\n",
      "Epoch 261\n",
      " ---------------------- loss: tensor([28178.3633], grad_fn=<DivBackward0>)\n",
      "Epoch 262\n",
      " ---------------------- loss: tensor([28178.2207], grad_fn=<DivBackward0>)\n",
      "Epoch 263\n",
      " ---------------------- loss: tensor([28178.], grad_fn=<DivBackward0>)\n",
      "Epoch 264\n",
      " ---------------------- loss: tensor([28177.7559], grad_fn=<DivBackward0>)\n",
      "Epoch 265\n",
      " ---------------------- loss: tensor([28177.5234], grad_fn=<DivBackward0>)\n",
      "Epoch 266\n",
      " ---------------------- loss: tensor([28177.2812], grad_fn=<DivBackward0>)\n",
      "Epoch 267\n",
      " ---------------------- loss: tensor([28177.0430], grad_fn=<DivBackward0>)\n",
      "Epoch 268\n",
      " ---------------------- loss: tensor([28176.8008], grad_fn=<DivBackward0>)\n",
      "Epoch 269\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 270\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 271\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 272\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 273\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 274\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 275\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 276\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 277\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 278\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 279\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 280\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 281\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 282\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 283\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 284\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 285\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 286\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 287\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 288\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 289\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 290\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 291\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 292\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 293\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 294\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 295\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 296\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 297\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 298\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 299\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Epoch 300\n",
      " ---------------------- loss: tensor([19391.7871], grad_fn=<DivBackward0>)\n",
      "Done!\n",
      "\n",
      "\n",
      "Epoch 1\n",
      " ---------------------- loss: tensor([31266.1562], grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      " ---------------------- loss: tensor([16192.1094], grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      " ---------------------- loss: tensor([14133.0020], grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      " ---------------------- loss: tensor([13533.6006], grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      " ---------------------- loss: tensor([13097.3438], grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      " ---------------------- loss: tensor([12730.2529], grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      " ---------------------- loss: tensor([12395.2393], grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      " ---------------------- loss: tensor([12052.0283], grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      " ---------------------- loss: tensor([11696.1025], grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      " ---------------------- loss: tensor([11285.2012], grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      " ---------------------- loss: tensor([10752.4844], grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      " ---------------------- loss: tensor([9036.4951], grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      " ---------------------- loss: tensor([6552.3857], grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      " ---------------------- loss: tensor([4912.7793], grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      " ---------------------- loss: tensor([3991.0344], grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      " ---------------------- loss: tensor([2741.2637], grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      " ---------------------- loss: tensor([2047.9957], grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      " ---------------------- loss: tensor([1681.9463], grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      " ---------------------- loss: tensor([1173.7850], grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      " ---------------------- loss: tensor([507.3086], grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      " ---------------------- loss: tensor([393.1262], grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      " ---------------------- loss: tensor([320.1592], grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      " ---------------------- loss: tensor([267.1109], grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      " ---------------------- loss: tensor([225.1183], grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      " ---------------------- loss: tensor([190.6073], grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      " ---------------------- loss: tensor([161.4208], grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      " ---------------------- loss: tensor([134.3799], grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      " ---------------------- loss: tensor([108.1304], grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      " ---------------------- loss: tensor([84.6158], grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      " ---------------------- loss: tensor([67.4689], grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      " ---------------------- loss: tensor([57.1018], grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      " ---------------------- loss: tensor([49.6420], grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      " ---------------------- loss: tensor([43.3721], grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      " ---------------------- loss: tensor([38.8787], grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      " ---------------------- loss: tensor([35.6370], grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      " ---------------------- loss: tensor([33.2231], grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      " ---------------------- loss: tensor([31.3703], grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      " ---------------------- loss: tensor([29.9558], grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      " ---------------------- loss: tensor([28.7804], grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      " ---------------------- loss: tensor([27.8432], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41\n",
      " ---------------------- loss: tensor([27.0630], grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      " ---------------------- loss: tensor([26.1447], grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      " ---------------------- loss: tensor([25.7046], grad_fn=<DivBackward0>)\n",
      "Epoch 44\n",
      " ---------------------- loss: tensor([25.3070], grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      " ---------------------- loss: tensor([24.9462], grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      " ---------------------- loss: tensor([29.2168], grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      " ---------------------- loss: tensor([27.8525], grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      " ---------------------- loss: tensor([26.7458], grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      " ---------------------- loss: tensor([25.9129], grad_fn=<DivBackward0>)\n",
      "Epoch 50\n",
      " ---------------------- loss: tensor([25.2804], grad_fn=<DivBackward0>)\n",
      "Epoch 51\n",
      " ---------------------- loss: tensor([24.8003], grad_fn=<DivBackward0>)\n",
      "Epoch 52\n",
      " ---------------------- loss: tensor([24.5110], grad_fn=<DivBackward0>)\n",
      "Epoch 53\n",
      " ---------------------- loss: tensor([24.3225], grad_fn=<DivBackward0>)\n",
      "Epoch 54\n",
      " ---------------------- loss: tensor([24.2601], grad_fn=<DivBackward0>)\n",
      "Epoch 55\n",
      " ---------------------- loss: tensor([24.1427], grad_fn=<DivBackward0>)\n",
      "Epoch 56\n",
      " ---------------------- loss: tensor([24.0461], grad_fn=<DivBackward0>)\n",
      "Epoch 57\n",
      " ---------------------- loss: tensor([23.9176], grad_fn=<DivBackward0>)\n",
      "Epoch 58\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 59\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 60\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 61\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 62\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 63\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 64\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 65\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 66\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 67\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 68\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 69\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 70\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 71\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 72\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 73\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 74\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 75\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 76\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 77\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 78\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 79\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 80\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 81\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 82\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 83\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 84\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 85\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 86\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 87\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 88\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 89\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 90\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 91\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 92\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 93\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 94\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 95\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 96\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 97\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 98\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 99\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 100\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 101\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 102\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 103\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 104\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 105\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 106\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 107\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 108\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 109\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 110\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 111\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 112\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 113\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 114\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 115\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 116\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 117\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 118\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 119\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 120\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 121\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 122\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 123\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 124\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 125\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 126\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 127\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 128\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 129\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 130\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 131\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 132\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 133\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 134\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 135\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 136\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 137\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 138\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 139\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 140\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 141\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 142\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 143\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 144\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 145\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 146\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 147\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 149\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 150\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 151\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 152\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 153\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 154\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 155\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 156\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 157\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 158\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 159\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 160\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 161\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 162\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 163\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 164\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 165\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 166\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 167\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 168\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 169\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 170\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 171\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 172\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 173\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 174\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 175\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 176\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 177\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 178\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 179\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 180\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 181\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 182\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 183\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 184\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 185\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 186\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 187\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 188\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 189\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 190\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 191\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 192\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 193\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 194\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 195\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 196\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 197\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 198\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 199\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 200\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 201\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 202\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 203\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 204\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 205\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 206\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 207\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 208\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 209\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 210\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 211\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 212\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 213\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 214\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 215\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 216\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 217\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 218\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 219\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 220\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 221\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 222\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 223\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 224\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 225\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 226\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 227\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 228\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 229\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 230\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 231\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 232\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 233\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 234\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 235\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 236\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 237\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 238\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 239\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 240\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 241\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 242\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 243\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 244\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 245\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 246\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 247\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 248\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 249\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 250\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 251\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 252\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 253\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 254\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 255\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 256\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 257\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 258\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 259\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 260\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 261\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 263\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 264\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 265\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 266\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 267\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 268\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 269\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 270\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 271\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 272\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 273\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 274\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 275\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 276\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 277\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 278\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 279\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 280\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 281\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 282\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 283\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 284\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 285\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 286\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 287\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 288\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 289\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 290\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 291\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 292\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 293\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 294\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 295\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 296\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 297\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 298\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 299\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Epoch 300\n",
      " ---------------------- loss: tensor([24.1547], grad_fn=<DivBackward0>)\n",
      "Done!\n",
      "\n",
      "\n",
      "Epoch 1\n",
      " ---------------------- loss: tensor([12542.1895], grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      " ---------------------- loss: tensor([13818.7070], grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      " ---------------------- loss: tensor([10422.9678], grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      " ---------------------- loss: tensor([3328.2673], grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      " ---------------------- loss: tensor([2215.0398], grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      " ---------------------- loss: tensor([1317.8069], grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      " ---------------------- loss: tensor([850.5248], grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      " ---------------------- loss: tensor([601.4097], grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      " ---------------------- loss: tensor([471.6224], grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      " ---------------------- loss: tensor([382.4249], grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      " ---------------------- loss: tensor([311.8963], grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      " ---------------------- loss: tensor([247.3914], grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      " ---------------------- loss: tensor([185.3256], grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      " ---------------------- loss: tensor([150.4071], grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      " ---------------------- loss: tensor([111.7217], grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      " ---------------------- loss: tensor([58.4225], grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      " ---------------------- loss: tensor([54.4930], grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      " ---------------------- loss: tensor([51.9931], grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      " ---------------------- loss: tensor([50.2663], grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      " ---------------------- loss: tensor([49.0066], grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      " ---------------------- loss: tensor([48.0478], grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      " ---------------------- loss: tensor([47.2916], grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      " ---------------------- loss: tensor([46.6742], grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      " ---------------------- loss: tensor([46.1514], grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      " ---------------------- loss: tensor([45.6921], grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      " ---------------------- loss: tensor([45.2719], grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      " ---------------------- loss: tensor([44.8713], grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      " ---------------------- loss: tensor([44.4713], grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      " ---------------------- loss: tensor([44.0548], grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      " ---------------------- loss: tensor([43.5775], grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      " ---------------------- loss: tensor([42.9370], grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      " ---------------------- loss: tensor([41.8759], grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      " ---------------------- loss: tensor([36.6904], grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      " ---------------------- loss: tensor([31.0724], grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      " ---------------------- loss: tensor([27.6453], grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      " ---------------------- loss: tensor([24.8412], grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      " ---------------------- loss: tensor([22.6601], grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      " ---------------------- loss: tensor([20.8471], grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      " ---------------------- loss: tensor([19.6711], grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      " ---------------------- loss: tensor([18.9297], grad_fn=<DivBackward0>)\n",
      "Epoch 41\n",
      " ---------------------- loss: tensor([18.3378], grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      " ---------------------- loss: tensor([17.8193], grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      " ---------------------- loss: tensor([17.3625], grad_fn=<DivBackward0>)\n",
      "Epoch 44\n",
      " ---------------------- loss: tensor([16.9739], grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      " ---------------------- loss: tensor([16.6188], grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      " ---------------------- loss: tensor([16.2962], grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      " ---------------------- loss: tensor([16.0114], grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      " ---------------------- loss: tensor([15.7077], grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      " ---------------------- loss: tensor([15.4933], grad_fn=<DivBackward0>)\n",
      "Epoch 50\n",
      " ---------------------- loss: tensor([15.2984], grad_fn=<DivBackward0>)\n",
      "Epoch 51\n",
      " ---------------------- loss: tensor([15.0399], grad_fn=<DivBackward0>)\n",
      "Epoch 52\n",
      " ---------------------- loss: tensor([14.7762], grad_fn=<DivBackward0>)\n",
      "Epoch 53\n",
      " ---------------------- loss: tensor([14.5301], grad_fn=<DivBackward0>)\n",
      "Epoch 54\n",
      " ---------------------- loss: tensor([14.3149], grad_fn=<DivBackward0>)\n",
      "Epoch 55\n",
      " ---------------------- loss: tensor([14.1365], grad_fn=<DivBackward0>)\n",
      "Epoch 56\n",
      " ---------------------- loss: tensor([13.9958], grad_fn=<DivBackward0>)\n",
      "Epoch 57\n",
      " ---------------------- loss: tensor([13.8793], grad_fn=<DivBackward0>)\n",
      "Epoch 58\n",
      " ---------------------- loss: tensor([13.7821], grad_fn=<DivBackward0>)\n",
      "Epoch 59\n",
      " ---------------------- loss: tensor([13.7006], grad_fn=<DivBackward0>)\n",
      "Epoch 60\n",
      " ---------------------- loss: tensor([13.6179], grad_fn=<DivBackward0>)\n",
      "Epoch 61\n",
      " ---------------------- loss: tensor([13.5540], grad_fn=<DivBackward0>)\n",
      "Epoch 62\n",
      " ---------------------- loss: tensor([13.4893], grad_fn=<DivBackward0>)\n",
      "Epoch 63\n",
      " ---------------------- loss: tensor([13.4388], grad_fn=<DivBackward0>)\n",
      "Epoch 64\n",
      " ---------------------- loss: tensor([13.3901], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65\n",
      " ---------------------- loss: tensor([13.3420], grad_fn=<DivBackward0>)\n",
      "Epoch 66\n",
      " ---------------------- loss: tensor([13.2977], grad_fn=<DivBackward0>)\n",
      "Epoch 67\n",
      " ---------------------- loss: tensor([13.2243], grad_fn=<DivBackward0>)\n",
      "Epoch 68\n",
      " ---------------------- loss: tensor([13.1919], grad_fn=<DivBackward0>)\n",
      "Epoch 69\n",
      " ---------------------- loss: tensor([13.1657], grad_fn=<DivBackward0>)\n",
      "Epoch 70\n",
      " ---------------------- loss: tensor([13.1420], grad_fn=<DivBackward0>)\n",
      "Epoch 71\n",
      " ---------------------- loss: tensor([13.1245], grad_fn=<DivBackward0>)\n",
      "Epoch 72\n",
      " ---------------------- loss: tensor([13.1090], grad_fn=<DivBackward0>)\n",
      "Epoch 73\n",
      " ---------------------- loss: tensor([13.0937], grad_fn=<DivBackward0>)\n",
      "Epoch 74\n",
      " ---------------------- loss: tensor([13.0737], grad_fn=<DivBackward0>)\n",
      "Epoch 75\n",
      " ---------------------- loss: tensor([13.0530], grad_fn=<DivBackward0>)\n",
      "Epoch 76\n",
      " ---------------------- loss: tensor([13.0463], grad_fn=<DivBackward0>)\n",
      "Epoch 77\n",
      " ---------------------- loss: tensor([13.0451], grad_fn=<DivBackward0>)\n",
      "Epoch 78\n",
      " ---------------------- loss: tensor([13.0437], grad_fn=<DivBackward0>)\n",
      "Epoch 79\n",
      " ---------------------- loss: tensor([13.0414], grad_fn=<DivBackward0>)\n",
      "Epoch 80\n",
      " ---------------------- loss: tensor([13.0382], grad_fn=<DivBackward0>)\n",
      "Epoch 81\n",
      " ---------------------- loss: tensor([13.0293], grad_fn=<DivBackward0>)\n",
      "Epoch 82\n",
      " ---------------------- loss: tensor([13.0234], grad_fn=<DivBackward0>)\n",
      "Epoch 83\n",
      " ---------------------- loss: tensor([13.0156], grad_fn=<DivBackward0>)\n",
      "Epoch 84\n",
      " ---------------------- loss: tensor([12.9948], grad_fn=<DivBackward0>)\n",
      "Epoch 85\n",
      " ---------------------- loss: tensor([12.9831], grad_fn=<DivBackward0>)\n",
      "Epoch 86\n",
      " ---------------------- loss: tensor([12.9618], grad_fn=<DivBackward0>)\n",
      "Epoch 87\n",
      " ---------------------- loss: tensor([12.9177], grad_fn=<DivBackward0>)\n",
      "Epoch 88\n",
      " ---------------------- loss: tensor([12.8705], grad_fn=<DivBackward0>)\n",
      "Epoch 89\n",
      " ---------------------- loss: tensor([12.8094], grad_fn=<DivBackward0>)\n",
      "Epoch 90\n",
      " ---------------------- loss: tensor([12.7634], grad_fn=<DivBackward0>)\n",
      "Epoch 91\n",
      " ---------------------- loss: tensor([12.7222], grad_fn=<DivBackward0>)\n",
      "Epoch 92\n",
      " ---------------------- loss: tensor([12.6852], grad_fn=<DivBackward0>)\n",
      "Epoch 93\n",
      " ---------------------- loss: tensor([12.6490], grad_fn=<DivBackward0>)\n",
      "Epoch 94\n",
      " ---------------------- loss: tensor([12.6158], grad_fn=<DivBackward0>)\n",
      "Epoch 95\n",
      " ---------------------- loss: tensor([12.5899], grad_fn=<DivBackward0>)\n",
      "Epoch 96\n",
      " ---------------------- loss: tensor([12.5643], grad_fn=<DivBackward0>)\n",
      "Epoch 97\n",
      " ---------------------- loss: tensor([12.5430], grad_fn=<DivBackward0>)\n",
      "Epoch 98\n",
      " ---------------------- loss: tensor([12.5245], grad_fn=<DivBackward0>)\n",
      "Epoch 99\n",
      " ---------------------- loss: tensor([12.5092], grad_fn=<DivBackward0>)\n",
      "Epoch 100\n",
      " ---------------------- loss: tensor([12.4864], grad_fn=<DivBackward0>)\n",
      "Epoch 101\n",
      " ---------------------- loss: tensor([12.4725], grad_fn=<DivBackward0>)\n",
      "Epoch 102\n",
      " ---------------------- loss: tensor([12.4559], grad_fn=<DivBackward0>)\n",
      "Epoch 103\n",
      " ---------------------- loss: tensor([12.4493], grad_fn=<DivBackward0>)\n",
      "Epoch 104\n",
      " ---------------------- loss: tensor([12.4417], grad_fn=<DivBackward0>)\n",
      "Epoch 105\n",
      " ---------------------- loss: tensor([12.4291], grad_fn=<DivBackward0>)\n",
      "Epoch 106\n",
      " ---------------------- loss: tensor([12.4282], grad_fn=<DivBackward0>)\n",
      "Epoch 107\n",
      " ---------------------- loss: tensor([12.4284], grad_fn=<DivBackward0>)\n",
      "Epoch 108\n",
      " ---------------------- loss: tensor([12.4242], grad_fn=<DivBackward0>)\n",
      "Epoch 109\n",
      " ---------------------- loss: tensor([12.4174], grad_fn=<DivBackward0>)\n",
      "Epoch 110\n",
      " ---------------------- loss: tensor([12.4146], grad_fn=<DivBackward0>)\n",
      "Epoch 111\n",
      " ---------------------- loss: tensor([12.3966], grad_fn=<DivBackward0>)\n",
      "Epoch 112\n",
      " ---------------------- loss: tensor([12.3403], grad_fn=<DivBackward0>)\n",
      "Epoch 113\n",
      " ---------------------- loss: tensor([12.2303], grad_fn=<DivBackward0>)\n",
      "Epoch 114\n",
      " ---------------------- loss: tensor([12.1777], grad_fn=<DivBackward0>)\n",
      "Epoch 115\n",
      " ---------------------- loss: tensor([13.2949], grad_fn=<DivBackward0>)\n",
      "Epoch 116\n",
      " ---------------------- loss: tensor([1359.3859], grad_fn=<DivBackward0>)\n",
      "Epoch 117\n",
      " ---------------------- loss: tensor([865.4152], grad_fn=<DivBackward0>)\n",
      "Epoch 118\n",
      " ---------------------- loss: tensor([248.5896], grad_fn=<DivBackward0>)\n",
      "Epoch 119\n",
      " ---------------------- loss: tensor([166.2943], grad_fn=<DivBackward0>)\n",
      "Epoch 120\n",
      " ---------------------- loss: tensor([120.9282], grad_fn=<DivBackward0>)\n",
      "Epoch 121\n",
      " ---------------------- loss: tensor([88.4332], grad_fn=<DivBackward0>)\n",
      "Epoch 122\n",
      " ---------------------- loss: tensor([63.5690], grad_fn=<DivBackward0>)\n",
      "Epoch 123\n",
      " ---------------------- loss: tensor([50.4211], grad_fn=<DivBackward0>)\n",
      "Epoch 124\n",
      " ---------------------- loss: tensor([41.6283], grad_fn=<DivBackward0>)\n",
      "Epoch 125\n",
      " ---------------------- loss: tensor([35.5402], grad_fn=<DivBackward0>)\n",
      "Epoch 126\n",
      " ---------------------- loss: tensor([30.9997], grad_fn=<DivBackward0>)\n",
      "Epoch 127\n",
      " ---------------------- loss: tensor([26.4492], grad_fn=<DivBackward0>)\n",
      "Epoch 128\n",
      " ---------------------- loss: tensor([23.6944], grad_fn=<DivBackward0>)\n",
      "Epoch 129\n",
      " ---------------------- loss: tensor([21.7013], grad_fn=<DivBackward0>)\n",
      "Epoch 130\n",
      " ---------------------- loss: tensor([19.3648], grad_fn=<DivBackward0>)\n",
      "Epoch 131\n",
      " ---------------------- loss: tensor([18.2143], grad_fn=<DivBackward0>)\n",
      "Epoch 132\n",
      " ---------------------- loss: tensor([16.7921], grad_fn=<DivBackward0>)\n",
      "Epoch 133\n",
      " ---------------------- loss: tensor([15.7362], grad_fn=<DivBackward0>)\n",
      "Epoch 134\n",
      " ---------------------- loss: tensor([15.0317], grad_fn=<DivBackward0>)\n",
      "Epoch 135\n",
      " ---------------------- loss: tensor([13.9030], grad_fn=<DivBackward0>)\n",
      "Epoch 136\n",
      " ---------------------- loss: tensor([13.6730], grad_fn=<DivBackward0>)\n",
      "Epoch 137\n",
      " ---------------------- loss: tensor([13.4687], grad_fn=<DivBackward0>)\n",
      "Epoch 138\n",
      " ---------------------- loss: tensor([13.3148], grad_fn=<DivBackward0>)\n",
      "Epoch 139\n",
      " ---------------------- loss: tensor([13.1927], grad_fn=<DivBackward0>)\n",
      "Epoch 140\n",
      " ---------------------- loss: tensor([13.0425], grad_fn=<DivBackward0>)\n",
      "Epoch 141\n",
      " ---------------------- loss: tensor([13.0367], grad_fn=<DivBackward0>)\n",
      "Epoch 142\n",
      " ---------------------- loss: tensor([12.5829], grad_fn=<DivBackward0>)\n",
      "Epoch 143\n",
      " ---------------------- loss: tensor([12.4936], grad_fn=<DivBackward0>)\n",
      "Epoch 144\n",
      " ---------------------- loss: tensor([12.4289], grad_fn=<DivBackward0>)\n",
      "Epoch 145\n",
      " ---------------------- loss: tensor([12.4338], grad_fn=<DivBackward0>)\n",
      "Epoch 146\n",
      " ---------------------- loss: tensor([12.4132], grad_fn=<DivBackward0>)\n",
      "Epoch 147\n",
      " ---------------------- loss: tensor([12.3828], grad_fn=<DivBackward0>)\n",
      "Epoch 148\n",
      " ---------------------- loss: tensor([12.3546], grad_fn=<DivBackward0>)\n",
      "Epoch 149\n",
      " ---------------------- loss: tensor([12.3255], grad_fn=<DivBackward0>)\n",
      "Epoch 150\n",
      " ---------------------- loss: tensor([12.3061], grad_fn=<DivBackward0>)\n",
      "Epoch 151\n",
      " ---------------------- loss: tensor([12.2844], grad_fn=<DivBackward0>)\n",
      "Epoch 152\n",
      " ---------------------- loss: tensor([12.1931], grad_fn=<DivBackward0>)\n",
      "Epoch 153\n",
      " ---------------------- loss: tensor([12.0513], grad_fn=<DivBackward0>)\n",
      "Epoch 154\n",
      " ---------------------- loss: tensor([11.9666], grad_fn=<DivBackward0>)\n",
      "Epoch 155\n",
      " ---------------------- loss: tensor([11.8174], grad_fn=<DivBackward0>)\n",
      "Epoch 156\n",
      " ---------------------- loss: tensor([12.1103], grad_fn=<DivBackward0>)\n",
      "Epoch 157\n",
      " ---------------------- loss: tensor([12.0137], grad_fn=<DivBackward0>)\n",
      "Epoch 158\n",
      " ---------------------- loss: tensor([26.9927], grad_fn=<DivBackward0>)\n",
      "Epoch 159\n",
      " ---------------------- loss: tensor([24.4956], grad_fn=<DivBackward0>)\n",
      "Epoch 160\n",
      " ---------------------- loss: tensor([22.4298], grad_fn=<DivBackward0>)\n",
      "Epoch 161\n",
      " ---------------------- loss: tensor([20.2702], grad_fn=<DivBackward0>)\n",
      "Epoch 162\n",
      " ---------------------- loss: tensor([18.5348], grad_fn=<DivBackward0>)\n",
      "Epoch 163\n",
      " ---------------------- loss: tensor([16.6021], grad_fn=<DivBackward0>)\n",
      "Epoch 164\n",
      " ---------------------- loss: tensor([15.3060], grad_fn=<DivBackward0>)\n",
      "Epoch 165\n",
      " ---------------------- loss: tensor([14.5857], grad_fn=<DivBackward0>)\n",
      "Epoch 166\n",
      " ---------------------- loss: tensor([10.8454], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167\n",
      " ---------------------- loss: tensor([10.6277], grad_fn=<DivBackward0>)\n",
      "Epoch 168\n",
      " ---------------------- loss: tensor([10.5222], grad_fn=<DivBackward0>)\n",
      "Epoch 169\n",
      " ---------------------- loss: tensor([10.4412], grad_fn=<DivBackward0>)\n",
      "Epoch 170\n",
      " ---------------------- loss: tensor([10.3592], grad_fn=<DivBackward0>)\n",
      "Epoch 171\n",
      " ---------------------- loss: tensor([10.3587], grad_fn=<DivBackward0>)\n",
      "Epoch 172\n",
      " ---------------------- loss: tensor([10.2503], grad_fn=<DivBackward0>)\n",
      "Epoch 173\n",
      " ---------------------- loss: tensor([10.1199], grad_fn=<DivBackward0>)\n",
      "Epoch 174\n",
      " ---------------------- loss: tensor([10.1141], grad_fn=<DivBackward0>)\n",
      "Epoch 175\n",
      " ---------------------- loss: tensor([10.1032], grad_fn=<DivBackward0>)\n",
      "Epoch 176\n",
      " ---------------------- loss: tensor([10.1039], grad_fn=<DivBackward0>)\n",
      "Epoch 177\n",
      " ---------------------- loss: tensor([10.1027], grad_fn=<DivBackward0>)\n",
      "Epoch 178\n",
      " ---------------------- loss: tensor([10.0737], grad_fn=<DivBackward0>)\n",
      "Epoch 179\n",
      " ---------------------- loss: tensor([10.0701], grad_fn=<DivBackward0>)\n",
      "Epoch 180\n",
      " ---------------------- loss: tensor([10.0552], grad_fn=<DivBackward0>)\n",
      "Epoch 181\n",
      " ---------------------- loss: tensor([10.0294], grad_fn=<DivBackward0>)\n",
      "Epoch 182\n",
      " ---------------------- loss: tensor([10.0089], grad_fn=<DivBackward0>)\n",
      "Epoch 183\n",
      " ---------------------- loss: tensor([11.4664], grad_fn=<DivBackward0>)\n",
      "Epoch 184\n",
      " ---------------------- loss: tensor([68.8728], grad_fn=<DivBackward0>)\n",
      "Epoch 185\n",
      " ---------------------- loss: tensor([13.6013], grad_fn=<DivBackward0>)\n",
      "Epoch 186\n",
      " ---------------------- loss: tensor([11.2873], grad_fn=<DivBackward0>)\n",
      "Epoch 187\n",
      " ---------------------- loss: tensor([11.0287], grad_fn=<DivBackward0>)\n",
      "Epoch 188\n",
      " ---------------------- loss: tensor([29.2421], grad_fn=<DivBackward0>)\n",
      "Epoch 189\n",
      " ---------------------- loss: tensor([24.8411], grad_fn=<DivBackward0>)\n",
      "Epoch 190\n",
      " ---------------------- loss: tensor([158.1650], grad_fn=<DivBackward0>)\n",
      "Epoch 191\n",
      " ---------------------- loss: tensor([44.3711], grad_fn=<DivBackward0>)\n",
      "Epoch 192\n",
      " ---------------------- loss: tensor([29.2281], grad_fn=<DivBackward0>)\n",
      "Epoch 193\n",
      " ---------------------- loss: tensor([25.8294], grad_fn=<DivBackward0>)\n",
      "Epoch 194\n",
      " ---------------------- loss: tensor([23.0239], grad_fn=<DivBackward0>)\n",
      "Epoch 195\n",
      " ---------------------- loss: tensor([20.7625], grad_fn=<DivBackward0>)\n",
      "Epoch 196\n",
      " ---------------------- loss: tensor([18.6391], grad_fn=<DivBackward0>)\n",
      "Epoch 197\n",
      " ---------------------- loss: tensor([17.4707], grad_fn=<DivBackward0>)\n",
      "Epoch 198\n",
      " ---------------------- loss: tensor([16.9660], grad_fn=<DivBackward0>)\n",
      "Epoch 199\n",
      " ---------------------- loss: tensor([451.8863], grad_fn=<DivBackward0>)\n",
      "Epoch 200\n",
      " ---------------------- loss: tensor([403.1808], grad_fn=<DivBackward0>)\n",
      "Epoch 201\n",
      " ---------------------- loss: tensor([290.8341], grad_fn=<DivBackward0>)\n",
      "Epoch 202\n",
      " ---------------------- loss: tensor([261.1030], grad_fn=<DivBackward0>)\n",
      "Epoch 203\n",
      " ---------------------- loss: tensor([163.6601], grad_fn=<DivBackward0>)\n",
      "Epoch 204\n",
      " ---------------------- loss: tensor([154.4620], grad_fn=<DivBackward0>)\n",
      "Epoch 205\n",
      " ---------------------- loss: tensor([143.2507], grad_fn=<DivBackward0>)\n",
      "Epoch 206\n",
      " ---------------------- loss: tensor([117.2984], grad_fn=<DivBackward0>)\n",
      "Epoch 207\n",
      " ---------------------- loss: tensor([99.4713], grad_fn=<DivBackward0>)\n",
      "Epoch 208\n",
      " ---------------------- loss: tensor([87.9124], grad_fn=<DivBackward0>)\n",
      "Epoch 209\n",
      " ---------------------- loss: tensor([78.4233], grad_fn=<DivBackward0>)\n",
      "Epoch 210\n",
      " ---------------------- loss: tensor([71.7246], grad_fn=<DivBackward0>)\n",
      "Epoch 211\n",
      " ---------------------- loss: tensor([66.8979], grad_fn=<DivBackward0>)\n",
      "Epoch 212\n",
      " ---------------------- loss: tensor([63.1342], grad_fn=<DivBackward0>)\n",
      "Epoch 213\n",
      " ---------------------- loss: tensor([59.2717], grad_fn=<DivBackward0>)\n",
      "Epoch 214\n",
      " ---------------------- loss: tensor([36.5413], grad_fn=<DivBackward0>)\n",
      "Epoch 215\n",
      " ---------------------- loss: tensor([34.0717], grad_fn=<DivBackward0>)\n",
      "Epoch 216\n",
      " ---------------------- loss: tensor([31.9227], grad_fn=<DivBackward0>)\n",
      "Epoch 217\n",
      " ---------------------- loss: tensor([30.0879], grad_fn=<DivBackward0>)\n",
      "Epoch 218\n",
      " ---------------------- loss: tensor([28.5865], grad_fn=<DivBackward0>)\n",
      "Epoch 219\n",
      " ---------------------- loss: tensor([27.3418], grad_fn=<DivBackward0>)\n",
      "Epoch 220\n",
      " ---------------------- loss: tensor([26.3173], grad_fn=<DivBackward0>)\n",
      "Epoch 221\n",
      " ---------------------- loss: tensor([25.4563], grad_fn=<DivBackward0>)\n",
      "Epoch 222\n",
      " ---------------------- loss: tensor([24.7302], grad_fn=<DivBackward0>)\n",
      "Epoch 223\n",
      " ---------------------- loss: tensor([24.1162], grad_fn=<DivBackward0>)\n",
      "Epoch 224\n",
      " ---------------------- loss: tensor([23.5971], grad_fn=<DivBackward0>)\n",
      "Epoch 225\n",
      " ---------------------- loss: tensor([23.1557], grad_fn=<DivBackward0>)\n",
      "Epoch 226\n",
      " ---------------------- loss: tensor([22.7838], grad_fn=<DivBackward0>)\n",
      "Epoch 227\n",
      " ---------------------- loss: tensor([22.4715], grad_fn=<DivBackward0>)\n",
      "Epoch 228\n",
      " ---------------------- loss: tensor([22.2079], grad_fn=<DivBackward0>)\n",
      "Epoch 229\n",
      " ---------------------- loss: tensor([21.9854], grad_fn=<DivBackward0>)\n",
      "Epoch 230\n",
      " ---------------------- loss: tensor([21.7944], grad_fn=<DivBackward0>)\n",
      "Epoch 231\n",
      " ---------------------- loss: tensor([21.6371], grad_fn=<DivBackward0>)\n",
      "Epoch 232\n",
      " ---------------------- loss: tensor([21.4990], grad_fn=<DivBackward0>)\n",
      "Epoch 233\n",
      " ---------------------- loss: tensor([21.3822], grad_fn=<DivBackward0>)\n",
      "Epoch 234\n",
      " ---------------------- loss: tensor([21.2817], grad_fn=<DivBackward0>)\n",
      "Epoch 235\n",
      " ---------------------- loss: tensor([21.1969], grad_fn=<DivBackward0>)\n",
      "Epoch 236\n",
      " ---------------------- loss: tensor([21.0959], grad_fn=<DivBackward0>)\n",
      "Epoch 237\n",
      " ---------------------- loss: tensor([21.0274], grad_fn=<DivBackward0>)\n",
      "Epoch 238\n",
      " ---------------------- loss: tensor([20.9090], grad_fn=<DivBackward0>)\n",
      "Epoch 239\n",
      " ---------------------- loss: tensor([20.6163], grad_fn=<DivBackward0>)\n",
      "Epoch 240\n",
      " ---------------------- loss: tensor([20.0263], grad_fn=<DivBackward0>)\n",
      "Epoch 241\n",
      " ---------------------- loss: tensor([18.9584], grad_fn=<DivBackward0>)\n",
      "Epoch 242\n",
      " ---------------------- loss: tensor([18.0531], grad_fn=<DivBackward0>)\n",
      "Epoch 243\n",
      " ---------------------- loss: tensor([17.6329], grad_fn=<DivBackward0>)\n",
      "Epoch 244\n",
      " ---------------------- loss: tensor([17.3446], grad_fn=<DivBackward0>)\n",
      "Epoch 245\n",
      " ---------------------- loss: tensor([17.1082], grad_fn=<DivBackward0>)\n",
      "Epoch 246\n",
      " ---------------------- loss: tensor([16.8988], grad_fn=<DivBackward0>)\n",
      "Epoch 247\n",
      " ---------------------- loss: tensor([16.7095], grad_fn=<DivBackward0>)\n",
      "Epoch 248\n",
      " ---------------------- loss: tensor([16.5278], grad_fn=<DivBackward0>)\n",
      "Epoch 249\n",
      " ---------------------- loss: tensor([16.2756], grad_fn=<DivBackward0>)\n",
      "Epoch 250\n",
      " ---------------------- loss: tensor([15.7911], grad_fn=<DivBackward0>)\n",
      "Epoch 251\n",
      " ---------------------- loss: tensor([15.2403], grad_fn=<DivBackward0>)\n",
      "Epoch 252\n",
      " ---------------------- loss: tensor([14.8232], grad_fn=<DivBackward0>)\n",
      "Epoch 253\n",
      " ---------------------- loss: tensor([14.5044], grad_fn=<DivBackward0>)\n",
      "Epoch 254\n",
      " ---------------------- loss: tensor([14.6312], grad_fn=<DivBackward0>)\n",
      "Epoch 255\n",
      " ---------------------- loss: tensor([14.3343], grad_fn=<DivBackward0>)\n",
      "Epoch 256\n",
      " ---------------------- loss: tensor([14.1170], grad_fn=<DivBackward0>)\n",
      "Epoch 257\n",
      " ---------------------- loss: tensor([13.9517], grad_fn=<DivBackward0>)\n",
      "Epoch 258\n",
      " ---------------------- loss: tensor([13.8263], grad_fn=<DivBackward0>)\n",
      "Epoch 259\n",
      " ---------------------- loss: tensor([13.6628], grad_fn=<DivBackward0>)\n",
      "Epoch 260\n",
      " ---------------------- loss: tensor([13.5998], grad_fn=<DivBackward0>)\n",
      "Epoch 261\n",
      " ---------------------- loss: tensor([13.5514], grad_fn=<DivBackward0>)\n",
      "Epoch 262\n",
      " ---------------------- loss: tensor([13.5100], grad_fn=<DivBackward0>)\n",
      "Epoch 263\n",
      " ---------------------- loss: tensor([13.4856], grad_fn=<DivBackward0>)\n",
      "Epoch 264\n",
      " ---------------------- loss: tensor([13.4565], grad_fn=<DivBackward0>)\n",
      "Epoch 265\n",
      " ---------------------- loss: tensor([13.4325], grad_fn=<DivBackward0>)\n",
      "Epoch 266\n",
      " ---------------------- loss: tensor([13.4103], grad_fn=<DivBackward0>)\n",
      "Epoch 267\n",
      " ---------------------- loss: tensor([13.3939], grad_fn=<DivBackward0>)\n",
      "Epoch 268\n",
      " ---------------------- loss: tensor([13.3805], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 269\n",
      " ---------------------- loss: tensor([13.3646], grad_fn=<DivBackward0>)\n",
      "Epoch 270\n",
      " ---------------------- loss: tensor([13.3567], grad_fn=<DivBackward0>)\n",
      "Epoch 271\n",
      " ---------------------- loss: tensor([13.3524], grad_fn=<DivBackward0>)\n",
      "Epoch 272\n",
      " ---------------------- loss: tensor([13.3483], grad_fn=<DivBackward0>)\n",
      "Epoch 273\n",
      " ---------------------- loss: tensor([29163.6191], grad_fn=<DivBackward0>)\n",
      "Epoch 274\n",
      " ---------------------- loss: tensor([18184.1250], grad_fn=<DivBackward0>)\n",
      "Epoch 275\n",
      " ---------------------- loss: tensor([18183.8105], grad_fn=<DivBackward0>)\n",
      "Epoch 276\n",
      " ---------------------- loss: tensor([18183.4961], grad_fn=<DivBackward0>)\n",
      "Epoch 277\n",
      " ---------------------- loss: tensor([18183.1816], grad_fn=<DivBackward0>)\n",
      "Epoch 278\n",
      " ---------------------- loss: tensor([18182.8633], grad_fn=<DivBackward0>)\n",
      "Epoch 279\n",
      " ---------------------- loss: tensor([18182.5469], grad_fn=<DivBackward0>)\n",
      "Epoch 280\n",
      " ---------------------- loss: tensor([18182.2305], grad_fn=<DivBackward0>)\n",
      "Epoch 281\n",
      " ---------------------- loss: tensor([18181.9160], grad_fn=<DivBackward0>)\n",
      "Epoch 282\n",
      " ---------------------- loss: tensor([18181.6035], grad_fn=<DivBackward0>)\n",
      "Epoch 283\n",
      " ---------------------- loss: tensor([18181.2891], grad_fn=<DivBackward0>)\n",
      "Epoch 284\n",
      " ---------------------- loss: tensor([18180.9707], grad_fn=<DivBackward0>)\n",
      "Epoch 285\n",
      " ---------------------- loss: tensor([18180.6562], grad_fn=<DivBackward0>)\n",
      "Epoch 286\n",
      " ---------------------- loss: tensor([18180.3477], grad_fn=<DivBackward0>)\n",
      "Epoch 287\n",
      " ---------------------- loss: tensor([18180.0312], grad_fn=<DivBackward0>)\n",
      "Epoch 288\n",
      " ---------------------- loss: tensor([18179.7109], grad_fn=<DivBackward0>)\n",
      "Epoch 289\n",
      " ---------------------- loss: tensor([18179.4004], grad_fn=<DivBackward0>)\n",
      "Epoch 290\n",
      " ---------------------- loss: tensor([18179.0859], grad_fn=<DivBackward0>)\n",
      "Epoch 291\n",
      " ---------------------- loss: tensor([18178.7695], grad_fn=<DivBackward0>)\n",
      "Epoch 292\n",
      " ---------------------- loss: tensor([18178.4551], grad_fn=<DivBackward0>)\n",
      "Epoch 293\n",
      " ---------------------- loss: tensor([18178.1406], grad_fn=<DivBackward0>)\n",
      "Epoch 294\n",
      " ---------------------- loss: tensor([18177.8223], grad_fn=<DivBackward0>)\n",
      "Epoch 295\n",
      " ---------------------- loss: tensor([18177.5117], grad_fn=<DivBackward0>)\n",
      "Epoch 296\n",
      " ---------------------- loss: tensor([18177.1934], grad_fn=<DivBackward0>)\n",
      "Epoch 297\n",
      " ---------------------- loss: tensor([18176.8770], grad_fn=<DivBackward0>)\n",
      "Epoch 298\n",
      " ---------------------- loss: tensor([18176.5664], grad_fn=<DivBackward0>)\n",
      "Epoch 299\n",
      " ---------------------- loss: tensor([18176.2500], grad_fn=<DivBackward0>)\n",
      "Epoch 300\n",
      " ---------------------- loss: tensor([18175.9336], grad_fn=<DivBackward0>)\n",
      "Done!\n",
      "\n",
      "\n",
      "Epoch 1\n",
      " ---------------------- loss: tensor([18947.0957], grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      " ---------------------- loss: tensor([16977.5742], grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      " ---------------------- loss: tensor([14627.3691], grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      " ---------------------- loss: tensor([12935.2676], grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      " ---------------------- loss: tensor([11389.2559], grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      " ---------------------- loss: tensor([9811.8438], grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      " ---------------------- loss: tensor([8127.7168], grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      " ---------------------- loss: tensor([6436.3643], grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      " ---------------------- loss: tensor([4505.2183], grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      " ---------------------- loss: tensor([2137.4604], grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      " ---------------------- loss: tensor([1571.7296], grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      " ---------------------- loss: tensor([1254.1854], grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      " ---------------------- loss: tensor([1033.0066], grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      " ---------------------- loss: tensor([863.2136], grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      " ---------------------- loss: tensor([728.7483], grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      " ---------------------- loss: tensor([619.9379], grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      " ---------------------- loss: tensor([529.4924], grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      " ---------------------- loss: tensor([455.0956], grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      " ---------------------- loss: tensor([392.9447], grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      " ---------------------- loss: tensor([339.5798], grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      " ---------------------- loss: tensor([294.2435], grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      " ---------------------- loss: tensor([256.9087], grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      " ---------------------- loss: tensor([226.8483], grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      " ---------------------- loss: tensor([202.8772], grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      " ---------------------- loss: tensor([183.2427], grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      " ---------------------- loss: tensor([165.8971], grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      " ---------------------- loss: tensor([149.9819], grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      " ---------------------- loss: tensor([136.4223], grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      " ---------------------- loss: tensor([125.4349], grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      " ---------------------- loss: tensor([116.6507], grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      " ---------------------- loss: tensor([109.7578], grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      " ---------------------- loss: tensor([104.1316], grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      " ---------------------- loss: tensor([98.6569], grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      " ---------------------- loss: tensor([93.4806], grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      " ---------------------- loss: tensor([89.4780], grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      " ---------------------- loss: tensor([86.1558], grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      " ---------------------- loss: tensor([83.3627], grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      " ---------------------- loss: tensor([81.3280], grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      " ---------------------- loss: tensor([79.7194], grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      " ---------------------- loss: tensor([77.6399], grad_fn=<DivBackward0>)\n",
      "Epoch 41\n",
      " ---------------------- loss: tensor([75.9148], grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      " ---------------------- loss: tensor([74.8263], grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      " ---------------------- loss: tensor([74.0219], grad_fn=<DivBackward0>)\n",
      "Epoch 44\n",
      " ---------------------- loss: tensor([73.0089], grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      " ---------------------- loss: tensor([72.1117], grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      " ---------------------- loss: tensor([71.5662], grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      " ---------------------- loss: tensor([71.0041], grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      " ---------------------- loss: tensor([70.1978], grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      " ---------------------- loss: tensor([69.7592], grad_fn=<DivBackward0>)\n",
      "Epoch 50\n",
      " ---------------------- loss: tensor([69.2998], grad_fn=<DivBackward0>)\n",
      "Epoch 51\n",
      " ---------------------- loss: tensor([68.8756], grad_fn=<DivBackward0>)\n",
      "Epoch 52\n",
      " ---------------------- loss: tensor([68.6092], grad_fn=<DivBackward0>)\n",
      "Epoch 53\n",
      " ---------------------- loss: tensor([68.3820], grad_fn=<DivBackward0>)\n",
      "Epoch 54\n",
      " ---------------------- loss: tensor([68.1361], grad_fn=<DivBackward0>)\n",
      "Epoch 55\n",
      " ---------------------- loss: tensor([67.9659], grad_fn=<DivBackward0>)\n",
      "Epoch 56\n",
      " ---------------------- loss: tensor([67.8464], grad_fn=<DivBackward0>)\n",
      "Epoch 57\n",
      " ---------------------- loss: tensor([67.7458], grad_fn=<DivBackward0>)\n",
      "Epoch 58\n",
      " ---------------------- loss: tensor([67.6665], grad_fn=<DivBackward0>)\n",
      "Epoch 59\n",
      " ---------------------- loss: tensor([67.6163], grad_fn=<DivBackward0>)\n",
      "Epoch 60\n",
      " ---------------------- loss: tensor([67.5828], grad_fn=<DivBackward0>)\n",
      "Epoch 61\n",
      " ---------------------- loss: tensor([67.4547], grad_fn=<DivBackward0>)\n",
      "Epoch 62\n",
      " ---------------------- loss: tensor([67.3754], grad_fn=<DivBackward0>)\n",
      "Epoch 63\n",
      " ---------------------- loss: tensor([67.3170], grad_fn=<DivBackward0>)\n",
      "Epoch 64\n",
      " ---------------------- loss: tensor([67.2663], grad_fn=<DivBackward0>)\n",
      "Epoch 65\n",
      " ---------------------- loss: tensor([67.2387], grad_fn=<DivBackward0>)\n",
      "Epoch 66\n",
      " ---------------------- loss: tensor([67.2041], grad_fn=<DivBackward0>)\n",
      "Epoch 67\n",
      " ---------------------- loss: tensor([67.1109], grad_fn=<DivBackward0>)\n",
      "Epoch 68\n",
      " ---------------------- loss: tensor([67.0676], grad_fn=<DivBackward0>)\n",
      "Epoch 69\n",
      " ---------------------- loss: tensor([67.0174], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70\n",
      " ---------------------- loss: tensor([66.9750], grad_fn=<DivBackward0>)\n",
      "Epoch 71\n",
      " ---------------------- loss: tensor([66.9403], grad_fn=<DivBackward0>)\n",
      "Epoch 72\n",
      " ---------------------- loss: tensor([66.8304], grad_fn=<DivBackward0>)\n",
      "Epoch 73\n",
      " ---------------------- loss: tensor([66.6648], grad_fn=<DivBackward0>)\n",
      "Epoch 74\n",
      " ---------------------- loss: tensor([66.3411], grad_fn=<DivBackward0>)\n",
      "Epoch 75\n",
      " ---------------------- loss: tensor([66.0598], grad_fn=<DivBackward0>)\n",
      "Epoch 76\n",
      " ---------------------- loss: tensor([65.8531], grad_fn=<DivBackward0>)\n",
      "Epoch 77\n",
      " ---------------------- loss: tensor([65.6548], grad_fn=<DivBackward0>)\n",
      "Epoch 78\n",
      " ---------------------- loss: tensor([65.4598], grad_fn=<DivBackward0>)\n",
      "Epoch 79\n",
      " ---------------------- loss: tensor([65.2643], grad_fn=<DivBackward0>)\n",
      "Epoch 80\n",
      " ---------------------- loss: tensor([65.0689], grad_fn=<DivBackward0>)\n",
      "Epoch 81\n",
      " ---------------------- loss: tensor([64.8799], grad_fn=<DivBackward0>)\n",
      "Epoch 82\n",
      " ---------------------- loss: tensor([64.6964], grad_fn=<DivBackward0>)\n",
      "Epoch 83\n",
      " ---------------------- loss: tensor([64.5191], grad_fn=<DivBackward0>)\n",
      "Epoch 84\n",
      " ---------------------- loss: tensor([64.3447], grad_fn=<DivBackward0>)\n",
      "Epoch 85\n",
      " ---------------------- loss: tensor([64.1665], grad_fn=<DivBackward0>)\n",
      "Epoch 86\n",
      " ---------------------- loss: tensor([63.9473], grad_fn=<DivBackward0>)\n",
      "Epoch 87\n",
      " ---------------------- loss: tensor([63.7038], grad_fn=<DivBackward0>)\n",
      "Epoch 88\n",
      " ---------------------- loss: tensor([63.5206], grad_fn=<DivBackward0>)\n",
      "Epoch 89\n",
      " ---------------------- loss: tensor([66.3029], grad_fn=<DivBackward0>)\n",
      "Epoch 90\n",
      " ---------------------- loss: tensor([66.2235], grad_fn=<DivBackward0>)\n",
      "Epoch 91\n",
      " ---------------------- loss: tensor([66.1312], grad_fn=<DivBackward0>)\n",
      "Epoch 92\n",
      " ---------------------- loss: tensor([65.8637], grad_fn=<DivBackward0>)\n",
      "Epoch 93\n",
      " ---------------------- loss: tensor([65.4956], grad_fn=<DivBackward0>)\n",
      "Epoch 94\n",
      " ---------------------- loss: tensor([65.2751], grad_fn=<DivBackward0>)\n",
      "Epoch 95\n",
      " ---------------------- loss: tensor([65.0974], grad_fn=<DivBackward0>)\n",
      "Epoch 96\n",
      " ---------------------- loss: tensor([64.9042], grad_fn=<DivBackward0>)\n",
      "Epoch 97\n",
      " ---------------------- loss: tensor([64.7760], grad_fn=<DivBackward0>)\n",
      "Epoch 98\n",
      " ---------------------- loss: tensor([64.5086], grad_fn=<DivBackward0>)\n",
      "Epoch 99\n",
      " ---------------------- loss: tensor([64.3315], grad_fn=<DivBackward0>)\n",
      "Epoch 100\n",
      " ---------------------- loss: tensor([64.1939], grad_fn=<DivBackward0>)\n",
      "Epoch 101\n",
      " ---------------------- loss: tensor([64.1099], grad_fn=<DivBackward0>)\n",
      "Epoch 102\n",
      " ---------------------- loss: tensor([64.0240], grad_fn=<DivBackward0>)\n",
      "Epoch 103\n",
      " ---------------------- loss: tensor([63.8968], grad_fn=<DivBackward0>)\n",
      "Epoch 104\n",
      " ---------------------- loss: tensor([63.7802], grad_fn=<DivBackward0>)\n",
      "Epoch 105\n",
      " ---------------------- loss: tensor([63.7171], grad_fn=<DivBackward0>)\n",
      "Epoch 106\n",
      " ---------------------- loss: tensor([63.6603], grad_fn=<DivBackward0>)\n",
      "Epoch 107\n",
      " ---------------------- loss: tensor([63.6096], grad_fn=<DivBackward0>)\n",
      "Epoch 108\n",
      " ---------------------- loss: tensor([63.5792], grad_fn=<DivBackward0>)\n",
      "Epoch 109\n",
      " ---------------------- loss: tensor([63.5162], grad_fn=<DivBackward0>)\n",
      "Epoch 110\n",
      " ---------------------- loss: tensor([63.4101], grad_fn=<DivBackward0>)\n",
      "Epoch 111\n",
      " ---------------------- loss: tensor([69.0356], grad_fn=<DivBackward0>)\n",
      "Epoch 112\n",
      " ---------------------- loss: tensor([68.2441], grad_fn=<DivBackward0>)\n",
      "Epoch 113\n",
      " ---------------------- loss: tensor([8135.8633], grad_fn=<DivBackward0>)\n",
      "Epoch 114\n",
      " ---------------------- loss: tensor([5603.8262], grad_fn=<DivBackward0>)\n",
      "Epoch 115\n",
      " ---------------------- loss: tensor([792.2497], grad_fn=<DivBackward0>)\n",
      "Epoch 116\n",
      " ---------------------- loss: tensor([551.8405], grad_fn=<DivBackward0>)\n",
      "Epoch 117\n",
      " ---------------------- loss: tensor([468.1061], grad_fn=<DivBackward0>)\n",
      "Epoch 118\n",
      " ---------------------- loss: tensor([416.0274], grad_fn=<DivBackward0>)\n",
      "Epoch 119\n",
      " ---------------------- loss: tensor([373.5781], grad_fn=<DivBackward0>)\n",
      "Epoch 120\n",
      " ---------------------- loss: tensor([327.6754], grad_fn=<DivBackward0>)\n",
      "Epoch 121\n",
      " ---------------------- loss: tensor([282.9974], grad_fn=<DivBackward0>)\n",
      "Epoch 122\n",
      " ---------------------- loss: tensor([249.5289], grad_fn=<DivBackward0>)\n",
      "Epoch 123\n",
      " ---------------------- loss: tensor([229.5964], grad_fn=<DivBackward0>)\n",
      "Epoch 124\n",
      " ---------------------- loss: tensor([217.6912], grad_fn=<DivBackward0>)\n",
      "Epoch 125\n",
      " ---------------------- loss: tensor([209.1227], grad_fn=<DivBackward0>)\n",
      "Epoch 126\n",
      " ---------------------- loss: tensor([201.7767], grad_fn=<DivBackward0>)\n",
      "Epoch 127\n",
      " ---------------------- loss: tensor([193.1884], grad_fn=<DivBackward0>)\n",
      "Epoch 128\n",
      " ---------------------- loss: tensor([180.8105], grad_fn=<DivBackward0>)\n",
      "Epoch 129\n",
      " ---------------------- loss: tensor([173.0184], grad_fn=<DivBackward0>)\n",
      "Epoch 130\n",
      " ---------------------- loss: tensor([168.4702], grad_fn=<DivBackward0>)\n",
      "Epoch 131\n",
      " ---------------------- loss: tensor([165.0371], grad_fn=<DivBackward0>)\n",
      "Epoch 132\n",
      " ---------------------- loss: tensor([161.9173], grad_fn=<DivBackward0>)\n",
      "Epoch 133\n",
      " ---------------------- loss: tensor([159.2225], grad_fn=<DivBackward0>)\n",
      "Epoch 134\n",
      " ---------------------- loss: tensor([152.4820], grad_fn=<DivBackward0>)\n",
      "Epoch 135\n",
      " ---------------------- loss: tensor([147.1614], grad_fn=<DivBackward0>)\n",
      "Epoch 136\n",
      " ---------------------- loss: tensor([144.2393], grad_fn=<DivBackward0>)\n",
      "Epoch 137\n",
      " ---------------------- loss: tensor([142.0055], grad_fn=<DivBackward0>)\n",
      "Epoch 138\n",
      " ---------------------- loss: tensor([139.9798], grad_fn=<DivBackward0>)\n",
      "Epoch 139\n",
      " ---------------------- loss: tensor([138.1906], grad_fn=<DivBackward0>)\n",
      "Epoch 140\n",
      " ---------------------- loss: tensor([429.6007], grad_fn=<DivBackward0>)\n",
      "Epoch 141\n",
      " ---------------------- loss: tensor([401.1140], grad_fn=<DivBackward0>)\n",
      "Epoch 142\n",
      " ---------------------- loss: tensor([347.9183], grad_fn=<DivBackward0>)\n",
      "Epoch 143\n",
      " ---------------------- loss: tensor([311.1259], grad_fn=<DivBackward0>)\n",
      "Epoch 144\n",
      " ---------------------- loss: tensor([287.7578], grad_fn=<DivBackward0>)\n",
      "Epoch 145\n",
      " ---------------------- loss: tensor([271.3128], grad_fn=<DivBackward0>)\n",
      "Epoch 146\n",
      " ---------------------- loss: tensor([259.0706], grad_fn=<DivBackward0>)\n",
      "Epoch 147\n",
      " ---------------------- loss: tensor([249.3980], grad_fn=<DivBackward0>)\n",
      "Epoch 148\n",
      " ---------------------- loss: tensor([241.5158], grad_fn=<DivBackward0>)\n",
      "Epoch 149\n",
      " ---------------------- loss: tensor([234.9570], grad_fn=<DivBackward0>)\n",
      "Epoch 150\n",
      " ---------------------- loss: tensor([229.4047], grad_fn=<DivBackward0>)\n",
      "Epoch 151\n",
      " ---------------------- loss: tensor([224.6952], grad_fn=<DivBackward0>)\n",
      "Epoch 152\n",
      " ---------------------- loss: tensor([220.6566], grad_fn=<DivBackward0>)\n",
      "Epoch 153\n",
      " ---------------------- loss: tensor([217.0920], grad_fn=<DivBackward0>)\n",
      "Epoch 154\n",
      " ---------------------- loss: tensor([213.8759], grad_fn=<DivBackward0>)\n",
      "Epoch 155\n",
      " ---------------------- loss: tensor([210.9106], grad_fn=<DivBackward0>)\n",
      "Epoch 156\n",
      " ---------------------- loss: tensor([208.1561], grad_fn=<DivBackward0>)\n",
      "Epoch 157\n",
      " ---------------------- loss: tensor([205.5674], grad_fn=<DivBackward0>)\n",
      "Epoch 158\n",
      " ---------------------- loss: tensor([203.1164], grad_fn=<DivBackward0>)\n",
      "Epoch 159\n",
      " ---------------------- loss: tensor([200.8080], grad_fn=<DivBackward0>)\n",
      "Epoch 160\n",
      " ---------------------- loss: tensor([198.6305], grad_fn=<DivBackward0>)\n",
      "Epoch 161\n",
      " ---------------------- loss: tensor([196.5888], grad_fn=<DivBackward0>)\n",
      "Epoch 162\n",
      " ---------------------- loss: tensor([194.7154], grad_fn=<DivBackward0>)\n",
      "Epoch 163\n",
      " ---------------------- loss: tensor([193.1233], grad_fn=<DivBackward0>)\n",
      "Epoch 164\n",
      " ---------------------- loss: tensor([191.8582], grad_fn=<DivBackward0>)\n",
      "Epoch 165\n",
      " ---------------------- loss: tensor([190.7691], grad_fn=<DivBackward0>)\n",
      "Epoch 166\n",
      " ---------------------- loss: tensor([189.3514], grad_fn=<DivBackward0>)\n",
      "Epoch 167\n",
      " ---------------------- loss: tensor([167.8295], grad_fn=<DivBackward0>)\n",
      "Epoch 168\n",
      " ---------------------- loss: tensor([152.2519], grad_fn=<DivBackward0>)\n",
      "Epoch 169\n",
      " ---------------------- loss: tensor([147.4051], grad_fn=<DivBackward0>)\n",
      "Epoch 170\n",
      " ---------------------- loss: tensor([144.1384], grad_fn=<DivBackward0>)\n",
      "Epoch 171\n",
      " ---------------------- loss: tensor([141.6265], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172\n",
      " ---------------------- loss: tensor([139.5553], grad_fn=<DivBackward0>)\n",
      "Epoch 173\n",
      " ---------------------- loss: tensor([136.9473], grad_fn=<DivBackward0>)\n",
      "Epoch 174\n",
      " ---------------------- loss: tensor([133.7009], grad_fn=<DivBackward0>)\n",
      "Epoch 175\n",
      " ---------------------- loss: tensor([131.6056], grad_fn=<DivBackward0>)\n",
      "Epoch 176\n",
      " ---------------------- loss: tensor([130.1173], grad_fn=<DivBackward0>)\n",
      "Epoch 177\n",
      " ---------------------- loss: tensor([128.9668], grad_fn=<DivBackward0>)\n",
      "Epoch 178\n",
      " ---------------------- loss: tensor([128.0188], grad_fn=<DivBackward0>)\n",
      "Epoch 179\n",
      " ---------------------- loss: tensor([127.1557], grad_fn=<DivBackward0>)\n",
      "Epoch 180\n",
      " ---------------------- loss: tensor([126.3695], grad_fn=<DivBackward0>)\n",
      "Epoch 181\n",
      " ---------------------- loss: tensor([125.7065], grad_fn=<DivBackward0>)\n",
      "Epoch 182\n",
      " ---------------------- loss: tensor([125.1800], grad_fn=<DivBackward0>)\n",
      "Epoch 183\n",
      " ---------------------- loss: tensor([124.7548], grad_fn=<DivBackward0>)\n",
      "Epoch 184\n",
      " ---------------------- loss: tensor([124.3941], grad_fn=<DivBackward0>)\n",
      "Epoch 185\n",
      " ---------------------- loss: tensor([124.1081], grad_fn=<DivBackward0>)\n",
      "Epoch 186\n",
      " ---------------------- loss: tensor([123.8475], grad_fn=<DivBackward0>)\n",
      "Epoch 187\n",
      " ---------------------- loss: tensor([123.5860], grad_fn=<DivBackward0>)\n",
      "Epoch 188\n",
      " ---------------------- loss: tensor([120.2208], grad_fn=<DivBackward0>)\n",
      "Epoch 189\n",
      " ---------------------- loss: tensor([118.6563], grad_fn=<DivBackward0>)\n",
      "Epoch 190\n",
      " ---------------------- loss: tensor([117.5386], grad_fn=<DivBackward0>)\n",
      "Epoch 191\n",
      " ---------------------- loss: tensor([112.8513], grad_fn=<DivBackward0>)\n",
      "Epoch 192\n",
      " ---------------------- loss: tensor([108.1555], grad_fn=<DivBackward0>)\n",
      "Epoch 193\n",
      " ---------------------- loss: tensor([105.3517], grad_fn=<DivBackward0>)\n",
      "Epoch 194\n",
      " ---------------------- loss: tensor([103.2087], grad_fn=<DivBackward0>)\n",
      "Epoch 195\n",
      " ---------------------- loss: tensor([101.4180], grad_fn=<DivBackward0>)\n",
      "Epoch 196\n",
      " ---------------------- loss: tensor([99.7767], grad_fn=<DivBackward0>)\n",
      "Epoch 197\n",
      " ---------------------- loss: tensor([98.2108], grad_fn=<DivBackward0>)\n",
      "Epoch 198\n",
      " ---------------------- loss: tensor([95.7456], grad_fn=<DivBackward0>)\n",
      "Epoch 199\n",
      " ---------------------- loss: tensor([93.4681], grad_fn=<DivBackward0>)\n",
      "Epoch 200\n",
      " ---------------------- loss: tensor([91.8698], grad_fn=<DivBackward0>)\n",
      "Epoch 201\n",
      " ---------------------- loss: tensor([90.5682], grad_fn=<DivBackward0>)\n",
      "Epoch 202\n",
      " ---------------------- loss: tensor([89.4345], grad_fn=<DivBackward0>)\n",
      "Epoch 203\n",
      " ---------------------- loss: tensor([88.3950], grad_fn=<DivBackward0>)\n",
      "Epoch 204\n",
      " ---------------------- loss: tensor([87.3886], grad_fn=<DivBackward0>)\n",
      "Epoch 205\n",
      " ---------------------- loss: tensor([86.3031], grad_fn=<DivBackward0>)\n",
      "Epoch 206\n",
      " ---------------------- loss: tensor([84.9061], grad_fn=<DivBackward0>)\n",
      "Epoch 207\n",
      " ---------------------- loss: tensor([83.6323], grad_fn=<DivBackward0>)\n",
      "Epoch 208\n",
      " ---------------------- loss: tensor([82.7033], grad_fn=<DivBackward0>)\n",
      "Epoch 209\n",
      " ---------------------- loss: tensor([81.9128], grad_fn=<DivBackward0>)\n",
      "Epoch 210\n",
      " ---------------------- loss: tensor([81.2204], grad_fn=<DivBackward0>)\n",
      "Epoch 211\n",
      " ---------------------- loss: tensor([80.6322], grad_fn=<DivBackward0>)\n",
      "Epoch 212\n",
      " ---------------------- loss: tensor([80.0865], grad_fn=<DivBackward0>)\n",
      "Epoch 213\n",
      " ---------------------- loss: tensor([79.5637], grad_fn=<DivBackward0>)\n",
      "Epoch 214\n",
      " ---------------------- loss: tensor([79.0842], grad_fn=<DivBackward0>)\n",
      "Epoch 215\n",
      " ---------------------- loss: tensor([78.6568], grad_fn=<DivBackward0>)\n",
      "Epoch 216\n",
      " ---------------------- loss: tensor([78.2736], grad_fn=<DivBackward0>)\n",
      "Epoch 217\n",
      " ---------------------- loss: tensor([77.9167], grad_fn=<DivBackward0>)\n",
      "Epoch 218\n",
      " ---------------------- loss: tensor([77.4675], grad_fn=<DivBackward0>)\n",
      "Epoch 219\n",
      " ---------------------- loss: tensor([76.5183], grad_fn=<DivBackward0>)\n",
      "Epoch 220\n",
      " ---------------------- loss: tensor([75.4733], grad_fn=<DivBackward0>)\n",
      "Epoch 221\n",
      " ---------------------- loss: tensor([75.0244], grad_fn=<DivBackward0>)\n",
      "Epoch 222\n",
      " ---------------------- loss: tensor([74.7101], grad_fn=<DivBackward0>)\n",
      "Epoch 223\n",
      " ---------------------- loss: tensor([74.4442], grad_fn=<DivBackward0>)\n",
      "Epoch 224\n",
      " ---------------------- loss: tensor([74.2064], grad_fn=<DivBackward0>)\n",
      "Epoch 225\n",
      " ---------------------- loss: tensor([73.9907], grad_fn=<DivBackward0>)\n",
      "Epoch 226\n",
      " ---------------------- loss: tensor([73.7884], grad_fn=<DivBackward0>)\n",
      "Epoch 227\n",
      " ---------------------- loss: tensor([73.5939], grad_fn=<DivBackward0>)\n",
      "Epoch 228\n",
      " ---------------------- loss: tensor([73.2774], grad_fn=<DivBackward0>)\n",
      "Epoch 229\n",
      " ---------------------- loss: tensor([72.6013], grad_fn=<DivBackward0>)\n",
      "Epoch 230\n",
      " ---------------------- loss: tensor([71.4911], grad_fn=<DivBackward0>)\n",
      "Epoch 231\n",
      " ---------------------- loss: tensor([67.7464], grad_fn=<DivBackward0>)\n",
      "Epoch 232\n",
      " ---------------------- loss: tensor([61.6503], grad_fn=<DivBackward0>)\n",
      "Epoch 233\n",
      " ---------------------- loss: tensor([58.2367], grad_fn=<DivBackward0>)\n",
      "Epoch 234\n",
      " ---------------------- loss: tensor([55.7903], grad_fn=<DivBackward0>)\n",
      "Epoch 235\n",
      " ---------------------- loss: tensor([53.6574], grad_fn=<DivBackward0>)\n",
      "Epoch 236\n",
      " ---------------------- loss: tensor([51.8005], grad_fn=<DivBackward0>)\n",
      "Epoch 237\n",
      " ---------------------- loss: tensor([50.1905], grad_fn=<DivBackward0>)\n",
      "Epoch 238\n",
      " ---------------------- loss: tensor([48.7314], grad_fn=<DivBackward0>)\n",
      "Epoch 239\n",
      " ---------------------- loss: tensor([47.3576], grad_fn=<DivBackward0>)\n",
      "Epoch 240\n",
      " ---------------------- loss: tensor([45.8658], grad_fn=<DivBackward0>)\n",
      "Epoch 241\n",
      " ---------------------- loss: tensor([44.2549], grad_fn=<DivBackward0>)\n",
      "Epoch 242\n",
      " ---------------------- loss: tensor([43.0893], grad_fn=<DivBackward0>)\n",
      "Epoch 243\n",
      " ---------------------- loss: tensor([42.2913], grad_fn=<DivBackward0>)\n",
      "Epoch 244\n",
      " ---------------------- loss: tensor([41.6881], grad_fn=<DivBackward0>)\n",
      "Epoch 245\n",
      " ---------------------- loss: tensor([41.1990], grad_fn=<DivBackward0>)\n",
      "Epoch 246\n",
      " ---------------------- loss: tensor([40.7863], grad_fn=<DivBackward0>)\n",
      "Epoch 247\n",
      " ---------------------- loss: tensor([40.4432], grad_fn=<DivBackward0>)\n",
      "Epoch 248\n",
      " ---------------------- loss: tensor([40.0750], grad_fn=<DivBackward0>)\n",
      "Epoch 249\n",
      " ---------------------- loss: tensor([39.7744], grad_fn=<DivBackward0>)\n",
      "Epoch 250\n",
      " ---------------------- loss: tensor([39.5658], grad_fn=<DivBackward0>)\n",
      "Epoch 251\n",
      " ---------------------- loss: tensor([39.4109], grad_fn=<DivBackward0>)\n",
      "Epoch 252\n",
      " ---------------------- loss: tensor([39.2949], grad_fn=<DivBackward0>)\n",
      "Epoch 253\n",
      " ---------------------- loss: tensor([39.1975], grad_fn=<DivBackward0>)\n",
      "Epoch 254\n",
      " ---------------------- loss: tensor([39.1121], grad_fn=<DivBackward0>)\n",
      "Epoch 255\n",
      " ---------------------- loss: tensor([39.0139], grad_fn=<DivBackward0>)\n",
      "Epoch 256\n",
      " ---------------------- loss: tensor([9979.0596], grad_fn=<DivBackward0>)\n",
      "Epoch 257\n",
      " ---------------------- loss: tensor([4712.8652], grad_fn=<DivBackward0>)\n",
      "Epoch 258\n",
      " ---------------------- loss: tensor([3412.3147], grad_fn=<DivBackward0>)\n",
      "Epoch 259\n",
      " ---------------------- loss: tensor([2930.9453], grad_fn=<DivBackward0>)\n",
      "Epoch 260\n",
      " ---------------------- loss: tensor([2676.8074], grad_fn=<DivBackward0>)\n",
      "Epoch 261\n",
      " ---------------------- loss: tensor([2516.5056], grad_fn=<DivBackward0>)\n",
      "Epoch 262\n",
      " ---------------------- loss: tensor([2399.0349], grad_fn=<DivBackward0>)\n",
      "Epoch 263\n",
      " ---------------------- loss: tensor([2308.8611], grad_fn=<DivBackward0>)\n",
      "Epoch 264\n",
      " ---------------------- loss: tensor([2241.5806], grad_fn=<DivBackward0>)\n",
      "Epoch 265\n",
      " ---------------------- loss: tensor([2187.0659], grad_fn=<DivBackward0>)\n",
      "Epoch 266\n",
      " ---------------------- loss: tensor([2138.1196], grad_fn=<DivBackward0>)\n",
      "Epoch 267\n",
      " ---------------------- loss: tensor([2097.8574], grad_fn=<DivBackward0>)\n",
      "Epoch 268\n",
      " ---------------------- loss: tensor([2070.4321], grad_fn=<DivBackward0>)\n",
      "Epoch 269\n",
      " ---------------------- loss: tensor([2049.9465], grad_fn=<DivBackward0>)\n",
      "Epoch 270\n",
      " ---------------------- loss: tensor([2034.4683], grad_fn=<DivBackward0>)\n",
      "Epoch 271\n",
      " ---------------------- loss: tensor([2022.5804], grad_fn=<DivBackward0>)\n",
      "Epoch 272\n",
      " ---------------------- loss: tensor([2012.5309], grad_fn=<DivBackward0>)\n",
      "Epoch 273\n",
      " ---------------------- loss: tensor([1991.8910], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274\n",
      " ---------------------- loss: tensor([1814.5370], grad_fn=<DivBackward0>)\n",
      "Epoch 275\n",
      " ---------------------- loss: tensor([1726.7644], grad_fn=<DivBackward0>)\n",
      "Epoch 276\n",
      " ---------------------- loss: tensor([1651.7538], grad_fn=<DivBackward0>)\n",
      "Epoch 277\n",
      " ---------------------- loss: tensor([1583.2203], grad_fn=<DivBackward0>)\n",
      "Epoch 278\n",
      " ---------------------- loss: tensor([1519.9819], grad_fn=<DivBackward0>)\n",
      "Epoch 279\n",
      " ---------------------- loss: tensor([1410.7618], grad_fn=<DivBackward0>)\n",
      "Epoch 280\n",
      " ---------------------- loss: tensor([1306.6688], grad_fn=<DivBackward0>)\n",
      "Epoch 281\n",
      " ---------------------- loss: tensor([1271.6042], grad_fn=<DivBackward0>)\n",
      "Epoch 282\n",
      " ---------------------- loss: tensor([1241.9430], grad_fn=<DivBackward0>)\n",
      "Epoch 283\n",
      " ---------------------- loss: tensor([1211.2339], grad_fn=<DivBackward0>)\n",
      "Epoch 284\n",
      " ---------------------- loss: tensor([1181.8718], grad_fn=<DivBackward0>)\n",
      "Epoch 285\n",
      " ---------------------- loss: tensor([1153.9539], grad_fn=<DivBackward0>)\n",
      "Epoch 286\n",
      " ---------------------- loss: tensor([1125.6608], grad_fn=<DivBackward0>)\n",
      "Epoch 287\n",
      " ---------------------- loss: tensor([1096.7944], grad_fn=<DivBackward0>)\n",
      "Epoch 288\n",
      " ---------------------- loss: tensor([1068.4148], grad_fn=<DivBackward0>)\n",
      "Epoch 289\n",
      " ---------------------- loss: tensor([1041.2576], grad_fn=<DivBackward0>)\n",
      "Epoch 290\n",
      " ---------------------- loss: tensor([1014.8193], grad_fn=<DivBackward0>)\n",
      "Epoch 291\n",
      " ---------------------- loss: tensor([989.9724], grad_fn=<DivBackward0>)\n",
      "Epoch 292\n",
      " ---------------------- loss: tensor([969.4277], grad_fn=<DivBackward0>)\n",
      "Epoch 293\n",
      " ---------------------- loss: tensor([953.0621], grad_fn=<DivBackward0>)\n",
      "Epoch 294\n",
      " ---------------------- loss: tensor([938.0588], grad_fn=<DivBackward0>)\n",
      "Epoch 295\n",
      " ---------------------- loss: tensor([918.0631], grad_fn=<DivBackward0>)\n",
      "Epoch 296\n",
      " ---------------------- loss: tensor([898.7827], grad_fn=<DivBackward0>)\n",
      "Epoch 297\n",
      " ---------------------- loss: tensor([879.8499], grad_fn=<DivBackward0>)\n",
      "Epoch 298\n",
      " ---------------------- loss: tensor([859.4845], grad_fn=<DivBackward0>)\n",
      "Epoch 299\n",
      " ---------------------- loss: tensor([837.0455], grad_fn=<DivBackward0>)\n",
      "Epoch 300\n",
      " ---------------------- loss: tensor([811.0811], grad_fn=<DivBackward0>)\n",
      "Done!\n",
      "\n",
      "\n",
      "Epoch 1\n",
      " ---------------------- loss: tensor([24130.5918], grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      " ---------------------- loss: tensor([24030.1953], grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      " ---------------------- loss: tensor([23610.3398], grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      " ---------------------- loss: tensor([22734.5371], grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      " ---------------------- loss: tensor([21988.9648], grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      " ---------------------- loss: tensor([21363.5879], grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      " ---------------------- loss: tensor([20801.2246], grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      " ---------------------- loss: tensor([20221.1309], grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      " ---------------------- loss: tensor([19383.8828], grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      " ---------------------- loss: tensor([18305.5176], grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      " ---------------------- loss: tensor([17294.4961], grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      " ---------------------- loss: tensor([16350.3145], grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      " ---------------------- loss: tensor([15374.4746], grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      " ---------------------- loss: tensor([14209.7969], grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      " ---------------------- loss: tensor([12817.6621], grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      " ---------------------- loss: tensor([11436.9414], grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      " ---------------------- loss: tensor([10195.9688], grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      " ---------------------- loss: tensor([9031.8564], grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      " ---------------------- loss: tensor([7885.2939], grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      " ---------------------- loss: tensor([6698.6792], grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      " ---------------------- loss: tensor([5638.5142], grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      " ---------------------- loss: tensor([4713.1084], grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      " ---------------------- loss: tensor([3841.0920], grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      " ---------------------- loss: tensor([2854.2651], grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      " ---------------------- loss: tensor([2137.9253], grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      " ---------------------- loss: tensor([1779.2882], grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      " ---------------------- loss: tensor([1509.8074], grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      " ---------------------- loss: tensor([1261.2126], grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      " ---------------------- loss: tensor([1057.9895], grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      " ---------------------- loss: tensor([911.6403], grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      " ---------------------- loss: tensor([802.0993], grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      " ---------------------- loss: tensor([717.4352], grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      " ---------------------- loss: tensor([647.4325], grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      " ---------------------- loss: tensor([576.2349], grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      " ---------------------- loss: tensor([501.2877], grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      " ---------------------- loss: tensor([439.0708], grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      " ---------------------- loss: tensor([389.4554], grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      " ---------------------- loss: tensor([348.5019], grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      " ---------------------- loss: tensor([316.3588], grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      " ---------------------- loss: tensor([290.9842], grad_fn=<DivBackward0>)\n",
      "Epoch 41\n",
      " ---------------------- loss: tensor([263.9579], grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      " ---------------------- loss: tensor([232.9970], grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      " ---------------------- loss: tensor([211.3454], grad_fn=<DivBackward0>)\n",
      "Epoch 44\n",
      " ---------------------- loss: tensor([192.8775], grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      " ---------------------- loss: tensor([169.2772], grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      " ---------------------- loss: tensor([155.3872], grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      " ---------------------- loss: tensor([145.9538], grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      " ---------------------- loss: tensor([135.3915], grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      " ---------------------- loss: tensor([125.2188], grad_fn=<DivBackward0>)\n",
      "Epoch 50\n",
      " ---------------------- loss: tensor([117.9644], grad_fn=<DivBackward0>)\n",
      "Epoch 51\n",
      " ---------------------- loss: tensor([111.1721], grad_fn=<DivBackward0>)\n",
      "Epoch 52\n",
      " ---------------------- loss: tensor([104.9107], grad_fn=<DivBackward0>)\n",
      "Epoch 53\n",
      " ---------------------- loss: tensor([100.0429], grad_fn=<DivBackward0>)\n",
      "Epoch 54\n",
      " ---------------------- loss: tensor([95.4478], grad_fn=<DivBackward0>)\n",
      "Epoch 55\n",
      " ---------------------- loss: tensor([91.0189], grad_fn=<DivBackward0>)\n",
      "Epoch 56\n",
      " ---------------------- loss: tensor([87.5725], grad_fn=<DivBackward0>)\n",
      "Epoch 57\n",
      " ---------------------- loss: tensor([84.5254], grad_fn=<DivBackward0>)\n",
      "Epoch 58\n",
      " ---------------------- loss: tensor([81.6063], grad_fn=<DivBackward0>)\n",
      "Epoch 59\n",
      " ---------------------- loss: tensor([79.2427], grad_fn=<DivBackward0>)\n",
      "Epoch 60\n",
      " ---------------------- loss: tensor([77.2706], grad_fn=<DivBackward0>)\n",
      "Epoch 61\n",
      " ---------------------- loss: tensor([75.4900], grad_fn=<DivBackward0>)\n",
      "Epoch 62\n",
      " ---------------------- loss: tensor([73.9531], grad_fn=<DivBackward0>)\n",
      "Epoch 63\n",
      " ---------------------- loss: tensor([72.6854], grad_fn=<DivBackward0>)\n",
      "Epoch 64\n",
      " ---------------------- loss: tensor([71.6174], grad_fn=<DivBackward0>)\n",
      "Epoch 65\n",
      " ---------------------- loss: tensor([70.7405], grad_fn=<DivBackward0>)\n",
      "Epoch 66\n",
      " ---------------------- loss: tensor([69.9673], grad_fn=<DivBackward0>)\n",
      "Epoch 67\n",
      " ---------------------- loss: tensor([69.3029], grad_fn=<DivBackward0>)\n",
      "Epoch 68\n",
      " ---------------------- loss: tensor([68.7670], grad_fn=<DivBackward0>)\n",
      "Epoch 69\n",
      " ---------------------- loss: tensor([68.3553], grad_fn=<DivBackward0>)\n",
      "Epoch 70\n",
      " ---------------------- loss: tensor([67.9765], grad_fn=<DivBackward0>)\n",
      "Epoch 71\n",
      " ---------------------- loss: tensor([67.6494], grad_fn=<DivBackward0>)\n",
      "Epoch 72\n",
      " ---------------------- loss: tensor([67.3873], grad_fn=<DivBackward0>)\n",
      "Epoch 73\n",
      " ---------------------- loss: tensor([67.1905], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74\n",
      " ---------------------- loss: tensor([67.0166], grad_fn=<DivBackward0>)\n",
      "Epoch 75\n",
      " ---------------------- loss: tensor([66.8594], grad_fn=<DivBackward0>)\n",
      "Epoch 76\n",
      " ---------------------- loss: tensor([66.7348], grad_fn=<DivBackward0>)\n",
      "Epoch 77\n",
      " ---------------------- loss: tensor([66.6431], grad_fn=<DivBackward0>)\n",
      "Epoch 78\n",
      " ---------------------- loss: tensor([66.5644], grad_fn=<DivBackward0>)\n",
      "Epoch 79\n",
      " ---------------------- loss: tensor([66.4901], grad_fn=<DivBackward0>)\n",
      "Epoch 80\n",
      " ---------------------- loss: tensor([66.4298], grad_fn=<DivBackward0>)\n",
      "Epoch 81\n",
      " ---------------------- loss: tensor([66.3849], grad_fn=<DivBackward0>)\n",
      "Epoch 82\n",
      " ---------------------- loss: tensor([66.3505], grad_fn=<DivBackward0>)\n",
      "Epoch 83\n",
      " ---------------------- loss: tensor([66.3006], grad_fn=<DivBackward0>)\n",
      "Epoch 84\n",
      " ---------------------- loss: tensor([66.2701], grad_fn=<DivBackward0>)\n",
      "Epoch 85\n",
      " ---------------------- loss: tensor([66.2433], grad_fn=<DivBackward0>)\n",
      "Epoch 86\n",
      " ---------------------- loss: tensor([66.2242], grad_fn=<DivBackward0>)\n",
      "Epoch 87\n",
      " ---------------------- loss: tensor([66.2002], grad_fn=<DivBackward0>)\n",
      "Epoch 88\n",
      " ---------------------- loss: tensor([66.1731], grad_fn=<DivBackward0>)\n",
      "Epoch 89\n",
      " ---------------------- loss: tensor([66.1467], grad_fn=<DivBackward0>)\n",
      "Epoch 90\n",
      " ---------------------- loss: tensor([66.1308], grad_fn=<DivBackward0>)\n",
      "Epoch 91\n",
      " ---------------------- loss: tensor([66.1088], grad_fn=<DivBackward0>)\n",
      "Epoch 92\n",
      " ---------------------- loss: tensor([66.0805], grad_fn=<DivBackward0>)\n",
      "Epoch 93\n",
      " ---------------------- loss: tensor([66.0509], grad_fn=<DivBackward0>)\n",
      "Epoch 94\n",
      " ---------------------- loss: tensor([66.0266], grad_fn=<DivBackward0>)\n",
      "Epoch 95\n",
      " ---------------------- loss: tensor([66.0046], grad_fn=<DivBackward0>)\n",
      "Epoch 96\n",
      " ---------------------- loss: tensor([65.9711], grad_fn=<DivBackward0>)\n",
      "Epoch 97\n",
      " ---------------------- loss: tensor([65.9305], grad_fn=<DivBackward0>)\n",
      "Epoch 98\n",
      " ---------------------- loss: tensor([65.9054], grad_fn=<DivBackward0>)\n",
      "Epoch 99\n",
      " ---------------------- loss: tensor([65.8811], grad_fn=<DivBackward0>)\n",
      "Epoch 100\n",
      " ---------------------- loss: tensor([65.8245], grad_fn=<DivBackward0>)\n",
      "Epoch 101\n",
      " ---------------------- loss: tensor([65.7895], grad_fn=<DivBackward0>)\n",
      "Epoch 102\n",
      " ---------------------- loss: tensor([65.7447], grad_fn=<DivBackward0>)\n",
      "Epoch 103\n",
      " ---------------------- loss: tensor([65.6733], grad_fn=<DivBackward0>)\n",
      "Epoch 104\n",
      " ---------------------- loss: tensor([65.5556], grad_fn=<DivBackward0>)\n",
      "Epoch 105\n",
      " ---------------------- loss: tensor([65.3662], grad_fn=<DivBackward0>)\n",
      "Epoch 106\n",
      " ---------------------- loss: tensor([65.2432], grad_fn=<DivBackward0>)\n",
      "Epoch 107\n",
      " ---------------------- loss: tensor([64.3683], grad_fn=<DivBackward0>)\n",
      "Epoch 108\n",
      " ---------------------- loss: tensor([64.1154], grad_fn=<DivBackward0>)\n",
      "Epoch 109\n",
      " ---------------------- loss: tensor([63.9364], grad_fn=<DivBackward0>)\n",
      "Epoch 110\n",
      " ---------------------- loss: tensor([63.6115], grad_fn=<DivBackward0>)\n",
      "Epoch 111\n",
      " ---------------------- loss: tensor([63.4163], grad_fn=<DivBackward0>)\n",
      "Epoch 112\n",
      " ---------------------- loss: tensor([63.2854], grad_fn=<DivBackward0>)\n",
      "Epoch 113\n",
      " ---------------------- loss: tensor([63.1853], grad_fn=<DivBackward0>)\n",
      "Epoch 114\n",
      " ---------------------- loss: tensor([63.1074], grad_fn=<DivBackward0>)\n",
      "Epoch 115\n",
      " ---------------------- loss: tensor([63.0418], grad_fn=<DivBackward0>)\n",
      "Epoch 116\n",
      " ---------------------- loss: tensor([62.9854], grad_fn=<DivBackward0>)\n",
      "Epoch 117\n",
      " ---------------------- loss: tensor([62.9333], grad_fn=<DivBackward0>)\n",
      "Epoch 118\n",
      " ---------------------- loss: tensor([62.8768], grad_fn=<DivBackward0>)\n",
      "Epoch 119\n",
      " ---------------------- loss: tensor([62.8233], grad_fn=<DivBackward0>)\n",
      "Epoch 120\n",
      " ---------------------- loss: tensor([96.0180], grad_fn=<DivBackward0>)\n",
      "Epoch 121\n",
      " ---------------------- loss: tensor([93.9662], grad_fn=<DivBackward0>)\n",
      "Epoch 122\n",
      " ---------------------- loss: tensor([91.8400], grad_fn=<DivBackward0>)\n",
      "Epoch 123\n",
      " ---------------------- loss: tensor([89.6554], grad_fn=<DivBackward0>)\n",
      "Epoch 124\n",
      " ---------------------- loss: tensor([87.4260], grad_fn=<DivBackward0>)\n",
      "Epoch 125\n",
      " ---------------------- loss: tensor([2060.1719], grad_fn=<DivBackward0>)\n",
      "Epoch 126\n",
      " ---------------------- loss: tensor([1010.4341], grad_fn=<DivBackward0>)\n",
      "Epoch 127\n",
      " ---------------------- loss: tensor([1002.7161], grad_fn=<DivBackward0>)\n",
      "Epoch 128\n",
      " ---------------------- loss: tensor([994.9175], grad_fn=<DivBackward0>)\n",
      "Epoch 129\n",
      " ---------------------- loss: tensor([987.0636], grad_fn=<DivBackward0>)\n",
      "Epoch 130\n",
      " ---------------------- loss: tensor([979.1537], grad_fn=<DivBackward0>)\n",
      "Epoch 131\n",
      " ---------------------- loss: tensor([971.1602], grad_fn=<DivBackward0>)\n",
      "Epoch 132\n",
      " ---------------------- loss: tensor([963.1395], grad_fn=<DivBackward0>)\n",
      "Epoch 133\n",
      " ---------------------- loss: tensor([955.1192], grad_fn=<DivBackward0>)\n",
      "Epoch 134\n",
      " ---------------------- loss: tensor([947.0490], grad_fn=<DivBackward0>)\n",
      "Epoch 135\n",
      " ---------------------- loss: tensor([938.9132], grad_fn=<DivBackward0>)\n",
      "Epoch 136\n",
      " ---------------------- loss: tensor([930.6995], grad_fn=<DivBackward0>)\n",
      "Epoch 137\n",
      " ---------------------- loss: tensor([922.4333], grad_fn=<DivBackward0>)\n",
      "Epoch 138\n",
      " ---------------------- loss: tensor([914.1014], grad_fn=<DivBackward0>)\n",
      "Epoch 139\n",
      " ---------------------- loss: tensor([905.7003], grad_fn=<DivBackward0>)\n",
      "Epoch 140\n",
      " ---------------------- loss: tensor([897.2463], grad_fn=<DivBackward0>)\n",
      "Epoch 141\n",
      " ---------------------- loss: tensor([888.7203], grad_fn=<DivBackward0>)\n",
      "Epoch 142\n",
      " ---------------------- loss: tensor([880.1423], grad_fn=<DivBackward0>)\n",
      "Epoch 143\n",
      " ---------------------- loss: tensor([871.4846], grad_fn=<DivBackward0>)\n",
      "Epoch 144\n",
      " ---------------------- loss: tensor([862.7693], grad_fn=<DivBackward0>)\n",
      "Epoch 145\n",
      " ---------------------- loss: tensor([853.9766], grad_fn=<DivBackward0>)\n",
      "Epoch 146\n",
      " ---------------------- loss: tensor([845.1317], grad_fn=<DivBackward0>)\n",
      "Epoch 147\n",
      " ---------------------- loss: tensor([836.2174], grad_fn=<DivBackward0>)\n",
      "Epoch 148\n",
      " ---------------------- loss: tensor([827.2495], grad_fn=<DivBackward0>)\n",
      "Epoch 149\n",
      " ---------------------- loss: tensor([818.2228], grad_fn=<DivBackward0>)\n",
      "Epoch 150\n",
      " ---------------------- loss: tensor([809.1285], grad_fn=<DivBackward0>)\n",
      "Epoch 151\n",
      " ---------------------- loss: tensor([799.9803], grad_fn=<DivBackward0>)\n",
      "Epoch 152\n",
      " ---------------------- loss: tensor([790.7744], grad_fn=<DivBackward0>)\n",
      "Epoch 153\n",
      " ---------------------- loss: tensor([781.5046], grad_fn=<DivBackward0>)\n",
      "Epoch 154\n",
      " ---------------------- loss: tensor([772.1729], grad_fn=<DivBackward0>)\n",
      "Epoch 155\n",
      " ---------------------- loss: tensor([762.8016], grad_fn=<DivBackward0>)\n",
      "Epoch 156\n",
      " ---------------------- loss: tensor([753.3776], grad_fn=<DivBackward0>)\n",
      "Epoch 157\n",
      " ---------------------- loss: tensor([743.8890], grad_fn=<DivBackward0>)\n",
      "Epoch 158\n",
      " ---------------------- loss: tensor([734.3434], grad_fn=<DivBackward0>)\n",
      "Epoch 159\n",
      " ---------------------- loss: tensor([724.7633], grad_fn=<DivBackward0>)\n",
      "Epoch 160\n",
      " ---------------------- loss: tensor([715.1420], grad_fn=<DivBackward0>)\n",
      "Epoch 161\n",
      " ---------------------- loss: tensor([705.4650], grad_fn=<DivBackward0>)\n",
      "Epoch 162\n",
      " ---------------------- loss: tensor([695.7456], grad_fn=<DivBackward0>)\n",
      "Epoch 163\n",
      " ---------------------- loss: tensor([685.9844], grad_fn=<DivBackward0>)\n",
      "Epoch 164\n",
      " ---------------------- loss: tensor([676.1974], grad_fn=<DivBackward0>)\n",
      "Epoch 165\n",
      " ---------------------- loss: tensor([666.3832], grad_fn=<DivBackward0>)\n",
      "Epoch 166\n",
      " ---------------------- loss: tensor([656.5206], grad_fn=<DivBackward0>)\n",
      "Epoch 167\n",
      " ---------------------- loss: tensor([646.6448], grad_fn=<DivBackward0>)\n",
      "Epoch 168\n",
      " ---------------------- loss: tensor([636.7432], grad_fn=<DivBackward0>)\n",
      "Epoch 169\n",
      " ---------------------- loss: tensor([626.8201], grad_fn=<DivBackward0>)\n",
      "Epoch 170\n",
      " ---------------------- loss: tensor([616.8751], grad_fn=<DivBackward0>)\n",
      "Epoch 171\n",
      " ---------------------- loss: tensor([606.9322], grad_fn=<DivBackward0>)\n",
      "Epoch 172\n",
      " ---------------------- loss: tensor([596.9870], grad_fn=<DivBackward0>)\n",
      "Epoch 173\n",
      " ---------------------- loss: tensor([587.0242], grad_fn=<DivBackward0>)\n",
      "Epoch 174\n",
      " ---------------------- loss: tensor([577.0641], grad_fn=<DivBackward0>)\n",
      "Epoch 175\n",
      " ---------------------- loss: tensor([152.8411], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176\n",
      " ---------------------- loss: tensor([150.5105], grad_fn=<DivBackward0>)\n",
      "Epoch 177\n",
      " ---------------------- loss: tensor([147.7946], grad_fn=<DivBackward0>)\n",
      "Epoch 178\n",
      " ---------------------- loss: tensor([144.6531], grad_fn=<DivBackward0>)\n",
      "Epoch 179\n",
      " ---------------------- loss: tensor([140.9230], grad_fn=<DivBackward0>)\n",
      "Epoch 180\n",
      " ---------------------- loss: tensor([134.1291], grad_fn=<DivBackward0>)\n",
      "Epoch 181\n",
      " ---------------------- loss: tensor([122.8011], grad_fn=<DivBackward0>)\n",
      "Epoch 182\n",
      " ---------------------- loss: tensor([113.7968], grad_fn=<DivBackward0>)\n",
      "Epoch 183\n",
      " ---------------------- loss: tensor([107.5130], grad_fn=<DivBackward0>)\n",
      "Epoch 184\n",
      " ---------------------- loss: tensor([102.8179], grad_fn=<DivBackward0>)\n",
      "Epoch 185\n",
      " ---------------------- loss: tensor([98.9038], grad_fn=<DivBackward0>)\n",
      "Epoch 186\n",
      " ---------------------- loss: tensor([94.7193], grad_fn=<DivBackward0>)\n",
      "Epoch 187\n",
      " ---------------------- loss: tensor([90.2322], grad_fn=<DivBackward0>)\n",
      "Epoch 188\n",
      " ---------------------- loss: tensor([86.5797], grad_fn=<DivBackward0>)\n",
      "Epoch 189\n",
      " ---------------------- loss: tensor([83.6803], grad_fn=<DivBackward0>)\n",
      "Epoch 190\n",
      " ---------------------- loss: tensor([81.5036], grad_fn=<DivBackward0>)\n",
      "Epoch 191\n",
      " ---------------------- loss: tensor([79.8439], grad_fn=<DivBackward0>)\n",
      "Epoch 192\n",
      " ---------------------- loss: tensor([78.0677], grad_fn=<DivBackward0>)\n",
      "Epoch 193\n",
      " ---------------------- loss: tensor([75.9794], grad_fn=<DivBackward0>)\n",
      "Epoch 194\n",
      " ---------------------- loss: tensor([74.4016], grad_fn=<DivBackward0>)\n",
      "Epoch 195\n",
      " ---------------------- loss: tensor([72.7822], grad_fn=<DivBackward0>)\n",
      "Epoch 196\n",
      " ---------------------- loss: tensor([71.3285], grad_fn=<DivBackward0>)\n",
      "Epoch 197\n",
      " ---------------------- loss: tensor([70.3799], grad_fn=<DivBackward0>)\n",
      "Epoch 198\n",
      " ---------------------- loss: tensor([69.6359], grad_fn=<DivBackward0>)\n",
      "Epoch 199\n",
      " ---------------------- loss: tensor([68.5021], grad_fn=<DivBackward0>)\n",
      "Epoch 200\n",
      " ---------------------- loss: tensor([67.6039], grad_fn=<DivBackward0>)\n",
      "Epoch 201\n",
      " ---------------------- loss: tensor([66.8592], grad_fn=<DivBackward0>)\n",
      "Epoch 202\n",
      " ---------------------- loss: tensor([66.1815], grad_fn=<DivBackward0>)\n",
      "Epoch 203\n",
      " ---------------------- loss: tensor([65.6823], grad_fn=<DivBackward0>)\n",
      "Epoch 204\n",
      " ---------------------- loss: tensor([65.2264], grad_fn=<DivBackward0>)\n",
      "Epoch 205\n",
      " ---------------------- loss: tensor([64.7396], grad_fn=<DivBackward0>)\n",
      "Epoch 206\n",
      " ---------------------- loss: tensor([64.3542], grad_fn=<DivBackward0>)\n",
      "Epoch 207\n",
      " ---------------------- loss: tensor([64.0688], grad_fn=<DivBackward0>)\n",
      "Epoch 208\n",
      " ---------------------- loss: tensor([63.7565], grad_fn=<DivBackward0>)\n",
      "Epoch 209\n",
      " ---------------------- loss: tensor([63.5129], grad_fn=<DivBackward0>)\n",
      "Epoch 210\n",
      " ---------------------- loss: tensor([63.3080], grad_fn=<DivBackward0>)\n",
      "Epoch 211\n",
      " ---------------------- loss: tensor([63.1455], grad_fn=<DivBackward0>)\n",
      "Epoch 212\n",
      " ---------------------- loss: tensor([62.9458], grad_fn=<DivBackward0>)\n",
      "Epoch 213\n",
      " ---------------------- loss: tensor([62.8413], grad_fn=<DivBackward0>)\n",
      "Epoch 214\n",
      " ---------------------- loss: tensor([62.7190], grad_fn=<DivBackward0>)\n",
      "Epoch 215\n",
      " ---------------------- loss: tensor([62.5692], grad_fn=<DivBackward0>)\n",
      "Epoch 216\n",
      " ---------------------- loss: tensor([62.5073], grad_fn=<DivBackward0>)\n",
      "Epoch 217\n",
      " ---------------------- loss: tensor([62.4606], grad_fn=<DivBackward0>)\n",
      "Epoch 218\n",
      " ---------------------- loss: tensor([62.3448], grad_fn=<DivBackward0>)\n",
      "Epoch 219\n",
      " ---------------------- loss: tensor([62.3168], grad_fn=<DivBackward0>)\n",
      "Epoch 220\n",
      " ---------------------- loss: tensor([62.1913], grad_fn=<DivBackward0>)\n",
      "Epoch 221\n",
      " ---------------------- loss: tensor([62.1905], grad_fn=<DivBackward0>)\n",
      "Epoch 222\n",
      " ---------------------- loss: tensor([62.1897], grad_fn=<DivBackward0>)\n",
      "Epoch 223\n",
      " ---------------------- loss: tensor([62.1869], grad_fn=<DivBackward0>)\n",
      "Epoch 224\n",
      " ---------------------- loss: tensor([62.1544], grad_fn=<DivBackward0>)\n",
      "Epoch 225\n",
      " ---------------------- loss: tensor([62.1222], grad_fn=<DivBackward0>)\n",
      "Epoch 226\n",
      " ---------------------- loss: tensor([62.1180], grad_fn=<DivBackward0>)\n",
      "Epoch 227\n",
      " ---------------------- loss: tensor([62.1152], grad_fn=<DivBackward0>)\n",
      "Epoch 228\n",
      " ---------------------- loss: tensor([62.1095], grad_fn=<DivBackward0>)\n",
      "Epoch 229\n",
      " ---------------------- loss: tensor([62.0980], grad_fn=<DivBackward0>)\n",
      "Epoch 230\n",
      " ---------------------- loss: tensor([62.0947], grad_fn=<DivBackward0>)\n",
      "Epoch 231\n",
      " ---------------------- loss: tensor([62.0812], grad_fn=<DivBackward0>)\n",
      "Epoch 232\n",
      " ---------------------- loss: tensor([62.0734], grad_fn=<DivBackward0>)\n",
      "Epoch 233\n",
      " ---------------------- loss: tensor([62.0670], grad_fn=<DivBackward0>)\n",
      "Epoch 234\n",
      " ---------------------- loss: tensor([62.0507], grad_fn=<DivBackward0>)\n",
      "Epoch 235\n",
      " ---------------------- loss: tensor([62.0493], grad_fn=<DivBackward0>)\n",
      "Epoch 236\n",
      " ---------------------- loss: tensor([62.0469], grad_fn=<DivBackward0>)\n",
      "Epoch 237\n",
      " ---------------------- loss: tensor([62.0466], grad_fn=<DivBackward0>)\n",
      "Epoch 238\n",
      " ---------------------- loss: tensor([62.0460], grad_fn=<DivBackward0>)\n",
      "Epoch 239\n",
      " ---------------------- loss: tensor([62.0338], grad_fn=<DivBackward0>)\n",
      "Epoch 240\n",
      " ---------------------- loss: tensor([62.0336], grad_fn=<DivBackward0>)\n",
      "Epoch 241\n",
      " ---------------------- loss: tensor([62.0334], grad_fn=<DivBackward0>)\n",
      "Epoch 242\n",
      " ---------------------- loss: tensor([62.0328], grad_fn=<DivBackward0>)\n",
      "Epoch 243\n",
      " ---------------------- loss: tensor([62.0316], grad_fn=<DivBackward0>)\n",
      "Epoch 244\n",
      " ---------------------- loss: tensor([62.0281], grad_fn=<DivBackward0>)\n",
      "Epoch 245\n",
      " ---------------------- loss: tensor([62.0239], grad_fn=<DivBackward0>)\n",
      "Epoch 246\n",
      " ---------------------- loss: tensor([62.0209], grad_fn=<DivBackward0>)\n",
      "Epoch 247\n",
      " ---------------------- loss: tensor([62.0111], grad_fn=<DivBackward0>)\n",
      "Epoch 248\n",
      " ---------------------- loss: tensor([61.9557], grad_fn=<DivBackward0>)\n",
      "Epoch 249\n",
      " ---------------------- loss: tensor([67.1263], grad_fn=<DivBackward0>)\n",
      "Epoch 250\n",
      " ---------------------- loss: tensor([66.4141], grad_fn=<DivBackward0>)\n",
      "Epoch 251\n",
      " ---------------------- loss: tensor([65.9567], grad_fn=<DivBackward0>)\n",
      "Epoch 252\n",
      " ---------------------- loss: tensor([65.6377], grad_fn=<DivBackward0>)\n",
      "Epoch 253\n",
      " ---------------------- loss: tensor([65.3721], grad_fn=<DivBackward0>)\n",
      "Epoch 254\n",
      " ---------------------- loss: tensor([65.1332], grad_fn=<DivBackward0>)\n",
      "Epoch 255\n",
      " ---------------------- loss: tensor([64.2453], grad_fn=<DivBackward0>)\n",
      "Epoch 256\n",
      " ---------------------- loss: tensor([63.7951], grad_fn=<DivBackward0>)\n",
      "Epoch 257\n",
      " ---------------------- loss: tensor([63.5745], grad_fn=<DivBackward0>)\n",
      "Epoch 258\n",
      " ---------------------- loss: tensor([63.4180], grad_fn=<DivBackward0>)\n",
      "Epoch 259\n",
      " ---------------------- loss: tensor([63.3203], grad_fn=<DivBackward0>)\n",
      "Epoch 260\n",
      " ---------------------- loss: tensor([63.2283], grad_fn=<DivBackward0>)\n",
      "Epoch 261\n",
      " ---------------------- loss: tensor([63.1518], grad_fn=<DivBackward0>)\n",
      "Epoch 262\n",
      " ---------------------- loss: tensor([63.0903], grad_fn=<DivBackward0>)\n",
      "Epoch 263\n",
      " ---------------------- loss: tensor([62.9526], grad_fn=<DivBackward0>)\n",
      "Epoch 264\n",
      " ---------------------- loss: tensor([62.9252], grad_fn=<DivBackward0>)\n",
      "Epoch 265\n",
      " ---------------------- loss: tensor([62.9138], grad_fn=<DivBackward0>)\n",
      "Epoch 266\n",
      " ---------------------- loss: tensor([62.9033], grad_fn=<DivBackward0>)\n",
      "Epoch 267\n",
      " ---------------------- loss: tensor([62.8918], grad_fn=<DivBackward0>)\n",
      "Epoch 268\n",
      " ---------------------- loss: tensor([62.8625], grad_fn=<DivBackward0>)\n",
      "Epoch 269\n",
      " ---------------------- loss: tensor([62.8275], grad_fn=<DivBackward0>)\n",
      "Epoch 270\n",
      " ---------------------- loss: tensor([62.8111], grad_fn=<DivBackward0>)\n",
      "Epoch 271\n",
      " ---------------------- loss: tensor([62.7955], grad_fn=<DivBackward0>)\n",
      "Epoch 272\n",
      " ---------------------- loss: tensor([62.7740], grad_fn=<DivBackward0>)\n",
      "Epoch 273\n",
      " ---------------------- loss: tensor([62.7566], grad_fn=<DivBackward0>)\n",
      "Epoch 274\n",
      " ---------------------- loss: tensor([62.7532], grad_fn=<DivBackward0>)\n",
      "Epoch 275\n",
      " ---------------------- loss: tensor([62.7515], grad_fn=<DivBackward0>)\n",
      "Epoch 276\n",
      " ---------------------- loss: tensor([62.7449], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277\n",
      " ---------------------- loss: tensor([62.7339], grad_fn=<DivBackward0>)\n",
      "Epoch 278\n",
      " ---------------------- loss: tensor([62.7200], grad_fn=<DivBackward0>)\n",
      "Epoch 279\n",
      " ---------------------- loss: tensor([62.7085], grad_fn=<DivBackward0>)\n",
      "Epoch 280\n",
      " ---------------------- loss: tensor([62.7003], grad_fn=<DivBackward0>)\n",
      "Epoch 281\n",
      " ---------------------- loss: tensor([62.6848], grad_fn=<DivBackward0>)\n",
      "Epoch 282\n",
      " ---------------------- loss: tensor([62.6775], grad_fn=<DivBackward0>)\n",
      "Epoch 283\n",
      " ---------------------- loss: tensor([62.6764], grad_fn=<DivBackward0>)\n",
      "Epoch 284\n",
      " ---------------------- loss: tensor([62.6692], grad_fn=<DivBackward0>)\n",
      "Epoch 285\n",
      " ---------------------- loss: tensor([62.6325], grad_fn=<DivBackward0>)\n",
      "Epoch 286\n",
      " ---------------------- loss: tensor([62.6302], grad_fn=<DivBackward0>)\n",
      "Epoch 287\n",
      " ---------------------- loss: tensor([62.6155], grad_fn=<DivBackward0>)\n",
      "Epoch 288\n",
      " ---------------------- loss: tensor([62.6097], grad_fn=<DivBackward0>)\n",
      "Epoch 289\n",
      " ---------------------- loss: tensor([62.5966], grad_fn=<DivBackward0>)\n",
      "Epoch 290\n",
      " ---------------------- loss: tensor([62.5843], grad_fn=<DivBackward0>)\n",
      "Epoch 291\n",
      " ---------------------- loss: tensor([62.5681], grad_fn=<DivBackward0>)\n",
      "Epoch 292\n",
      " ---------------------- loss: tensor([62.5497], grad_fn=<DivBackward0>)\n",
      "Epoch 293\n",
      " ---------------------- loss: tensor([62.5054], grad_fn=<DivBackward0>)\n",
      "Epoch 294\n",
      " ---------------------- loss: tensor([62.5000], grad_fn=<DivBackward0>)\n",
      "Epoch 295\n",
      " ---------------------- loss: tensor([62.4824], grad_fn=<DivBackward0>)\n",
      "Epoch 296\n",
      " ---------------------- loss: tensor([62.4587], grad_fn=<DivBackward0>)\n",
      "Epoch 297\n",
      " ---------------------- loss: tensor([62.4527], grad_fn=<DivBackward0>)\n",
      "Epoch 298\n",
      " ---------------------- loss: tensor([62.4442], grad_fn=<DivBackward0>)\n",
      "Epoch 299\n",
      " ---------------------- loss: tensor([62.4300], grad_fn=<DivBackward0>)\n",
      "Epoch 300\n",
      " ---------------------- loss: tensor([62.4130], grad_fn=<DivBackward0>)\n",
      "Done!\n",
      "\n",
      "\n",
      "Epoch 1\n",
      " ---------------------- loss: tensor([11550.5752], grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      " ---------------------- loss: tensor([6370.3198], grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      " ---------------------- loss: tensor([13995.7109], grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      " ---------------------- loss: tensor([13659.6689], grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      " ---------------------- loss: tensor([13240.0430], grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      " ---------------------- loss: tensor([12836.8955], grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      " ---------------------- loss: tensor([12429.8164], grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      " ---------------------- loss: tensor([11935.1064], grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      " ---------------------- loss: tensor([10835.9248], grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      " ---------------------- loss: tensor([5921.1211], grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      " ---------------------- loss: tensor([5620.9673], grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      " ---------------------- loss: tensor([5449.1885], grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      " ---------------------- loss: tensor([5316.9448], grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      " ---------------------- loss: tensor([5204.7451], grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      " ---------------------- loss: tensor([5102.1860], grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      " ---------------------- loss: tensor([4976.1504], grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      " ---------------------- loss: tensor([4834.7246], grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      " ---------------------- loss: tensor([4710.5503], grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      " ---------------------- loss: tensor([4597.6816], grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      " ---------------------- loss: tensor([4480.7998], grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      " ---------------------- loss: tensor([4348.0752], grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      " ---------------------- loss: tensor([4236.9272], grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      " ---------------------- loss: tensor([4151.7539], grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      " ---------------------- loss: tensor([4079.8135], grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      " ---------------------- loss: tensor([4015.1301], grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      " ---------------------- loss: tensor([3945.1958], grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      " ---------------------- loss: tensor([3841.5325], grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      " ---------------------- loss: tensor([3743.4927], grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      " ---------------------- loss: tensor([3656.2568], grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      " ---------------------- loss: tensor([3567.8206], grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      " ---------------------- loss: tensor([3475.8245], grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      " ---------------------- loss: tensor([3398.7144], grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      " ---------------------- loss: tensor([3333.4250], grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      " ---------------------- loss: tensor([3263.7285], grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      " ---------------------- loss: tensor([3175.7534], grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      " ---------------------- loss: tensor([3086.4319], grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      " ---------------------- loss: tensor([3013.1001], grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      " ---------------------- loss: tensor([2959.5745], grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      " ---------------------- loss: tensor([2914.4753], grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      " ---------------------- loss: tensor([2859.6506], grad_fn=<DivBackward0>)\n",
      "Epoch 41\n",
      " ---------------------- loss: tensor([2812.1350], grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      " ---------------------- loss: tensor([2777.4817], grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      " ---------------------- loss: tensor([2745.0640], grad_fn=<DivBackward0>)\n",
      "Epoch 44\n",
      " ---------------------- loss: tensor([2714.3816], grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      " ---------------------- loss: tensor([2684.5732], grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      " ---------------------- loss: tensor([2656.1545], grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      " ---------------------- loss: tensor([2628.0657], grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      " ---------------------- loss: tensor([2599.4023], grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      " ---------------------- loss: tensor([2569.8577], grad_fn=<DivBackward0>)\n",
      "Epoch 50\n",
      " ---------------------- loss: tensor([2540.3394], grad_fn=<DivBackward0>)\n",
      "Epoch 51\n",
      " ---------------------- loss: tensor([2512.1143], grad_fn=<DivBackward0>)\n",
      "Epoch 52\n",
      " ---------------------- loss: tensor([2485.3174], grad_fn=<DivBackward0>)\n",
      "Epoch 53\n",
      " ---------------------- loss: tensor([2460.0400], grad_fn=<DivBackward0>)\n",
      "Epoch 54\n",
      " ---------------------- loss: tensor([2436.5698], grad_fn=<DivBackward0>)\n",
      "Epoch 55\n",
      " ---------------------- loss: tensor([2413.5596], grad_fn=<DivBackward0>)\n",
      "Epoch 56\n",
      " ---------------------- loss: tensor([2383.1450], grad_fn=<DivBackward0>)\n",
      "Epoch 57\n",
      " ---------------------- loss: tensor([2348.4419], grad_fn=<DivBackward0>)\n",
      "Epoch 58\n",
      " ---------------------- loss: tensor([2317.2554], grad_fn=<DivBackward0>)\n",
      "Epoch 59\n",
      " ---------------------- loss: tensor([2288.5566], grad_fn=<DivBackward0>)\n",
      "Epoch 60\n",
      " ---------------------- loss: tensor([2260.7317], grad_fn=<DivBackward0>)\n",
      "Epoch 61\n",
      " ---------------------- loss: tensor([2237.3245], grad_fn=<DivBackward0>)\n",
      "Epoch 62\n",
      " ---------------------- loss: tensor([2216.9419], grad_fn=<DivBackward0>)\n",
      "Epoch 63\n",
      " ---------------------- loss: tensor([2197.9084], grad_fn=<DivBackward0>)\n",
      "Epoch 64\n",
      " ---------------------- loss: tensor([2170.1848], grad_fn=<DivBackward0>)\n",
      "Epoch 65\n",
      " ---------------------- loss: tensor([2142.0562], grad_fn=<DivBackward0>)\n",
      "Epoch 66\n",
      " ---------------------- loss: tensor([2115.4297], grad_fn=<DivBackward0>)\n",
      "Epoch 67\n",
      " ---------------------- loss: tensor([2088.3853], grad_fn=<DivBackward0>)\n",
      "Epoch 68\n",
      " ---------------------- loss: tensor([2060.4373], grad_fn=<DivBackward0>)\n",
      "Epoch 69\n",
      " ---------------------- loss: tensor([2035.5226], grad_fn=<DivBackward0>)\n",
      "Epoch 70\n",
      " ---------------------- loss: tensor([2013.1753], grad_fn=<DivBackward0>)\n",
      "Epoch 71\n",
      " ---------------------- loss: tensor([1992.2837], grad_fn=<DivBackward0>)\n",
      "Epoch 72\n",
      " ---------------------- loss: tensor([1971.3567], grad_fn=<DivBackward0>)\n",
      "Epoch 73\n",
      " ---------------------- loss: tensor([1946.0676], grad_fn=<DivBackward0>)\n",
      "Epoch 74\n",
      " ---------------------- loss: tensor([1920.9917], grad_fn=<DivBackward0>)\n",
      "Epoch 75\n",
      " ---------------------- loss: tensor([1892.7410], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76\n",
      " ---------------------- loss: tensor([1865.9180], grad_fn=<DivBackward0>)\n",
      "Epoch 77\n",
      " ---------------------- loss: tensor([1836.9052], grad_fn=<DivBackward0>)\n",
      "Epoch 78\n",
      " ---------------------- loss: tensor([1810.5243], grad_fn=<DivBackward0>)\n",
      "Epoch 79\n",
      " ---------------------- loss: tensor([1788.0798], grad_fn=<DivBackward0>)\n",
      "Epoch 80\n",
      " ---------------------- loss: tensor([1763.9028], grad_fn=<DivBackward0>)\n",
      "Epoch 81\n",
      " ---------------------- loss: tensor([1734.4391], grad_fn=<DivBackward0>)\n",
      "Epoch 82\n",
      " ---------------------- loss: tensor([1708.9874], grad_fn=<DivBackward0>)\n",
      "Epoch 83\n",
      " ---------------------- loss: tensor([1673.3651], grad_fn=<DivBackward0>)\n",
      "Epoch 84\n",
      " ---------------------- loss: tensor([1492.5186], grad_fn=<DivBackward0>)\n",
      "Epoch 85\n",
      " ---------------------- loss: tensor([1298.7585], grad_fn=<DivBackward0>)\n",
      "Epoch 86\n",
      " ---------------------- loss: tensor([1263.8687], grad_fn=<DivBackward0>)\n",
      "Epoch 87\n",
      " ---------------------- loss: tensor([1237.6327], grad_fn=<DivBackward0>)\n",
      "Epoch 88\n",
      " ---------------------- loss: tensor([1216.5717], grad_fn=<DivBackward0>)\n",
      "Epoch 89\n",
      " ---------------------- loss: tensor([1190.7050], grad_fn=<DivBackward0>)\n",
      "Epoch 90\n",
      " ---------------------- loss: tensor([1163.9460], grad_fn=<DivBackward0>)\n",
      "Epoch 91\n",
      " ---------------------- loss: tensor([1135.2491], grad_fn=<DivBackward0>)\n",
      "Epoch 92\n",
      " ---------------------- loss: tensor([1087.1428], grad_fn=<DivBackward0>)\n",
      "Epoch 93\n",
      " ---------------------- loss: tensor([1034.8872], grad_fn=<DivBackward0>)\n",
      "Epoch 94\n",
      " ---------------------- loss: tensor([1006.3854], grad_fn=<DivBackward0>)\n",
      "Epoch 95\n",
      " ---------------------- loss: tensor([987.4676], grad_fn=<DivBackward0>)\n",
      "Epoch 96\n",
      " ---------------------- loss: tensor([971.3282], grad_fn=<DivBackward0>)\n",
      "Epoch 97\n",
      " ---------------------- loss: tensor([955.5000], grad_fn=<DivBackward0>)\n",
      "Epoch 98\n",
      " ---------------------- loss: tensor([942.7098], grad_fn=<DivBackward0>)\n",
      "Epoch 99\n",
      " ---------------------- loss: tensor([931.7493], grad_fn=<DivBackward0>)\n",
      "Epoch 100\n",
      " ---------------------- loss: tensor([919.1644], grad_fn=<DivBackward0>)\n",
      "Epoch 101\n",
      " ---------------------- loss: tensor([906.2674], grad_fn=<DivBackward0>)\n",
      "Epoch 102\n",
      " ---------------------- loss: tensor([896.2239], grad_fn=<DivBackward0>)\n",
      "Epoch 103\n",
      " ---------------------- loss: tensor([887.9966], grad_fn=<DivBackward0>)\n",
      "Epoch 104\n",
      " ---------------------- loss: tensor([881.1096], grad_fn=<DivBackward0>)\n",
      "Epoch 105\n",
      " ---------------------- loss: tensor([875.4925], grad_fn=<DivBackward0>)\n",
      "Epoch 106\n",
      " ---------------------- loss: tensor([870.6849], grad_fn=<DivBackward0>)\n",
      "Epoch 107\n",
      " ---------------------- loss: tensor([864.3981], grad_fn=<DivBackward0>)\n",
      "Epoch 108\n",
      " ---------------------- loss: tensor([855.8525], grad_fn=<DivBackward0>)\n",
      "Epoch 109\n",
      " ---------------------- loss: tensor([849.2631], grad_fn=<DivBackward0>)\n",
      "Epoch 110\n",
      " ---------------------- loss: tensor([832.7827], grad_fn=<DivBackward0>)\n",
      "Epoch 111\n",
      " ---------------------- loss: tensor([706.5114], grad_fn=<DivBackward0>)\n",
      "Epoch 112\n",
      " ---------------------- loss: tensor([690.3992], grad_fn=<DivBackward0>)\n",
      "Epoch 113\n",
      " ---------------------- loss: tensor([671.9041], grad_fn=<DivBackward0>)\n",
      "Epoch 114\n",
      " ---------------------- loss: tensor([650.1356], grad_fn=<DivBackward0>)\n",
      "Epoch 115\n",
      " ---------------------- loss: tensor([623.0737], grad_fn=<DivBackward0>)\n",
      "Epoch 116\n",
      " ---------------------- loss: tensor([579.4053], grad_fn=<DivBackward0>)\n",
      "Epoch 117\n",
      " ---------------------- loss: tensor([248.6552], grad_fn=<DivBackward0>)\n",
      "Epoch 118\n",
      " ---------------------- loss: tensor([241.1533], grad_fn=<DivBackward0>)\n",
      "Epoch 119\n",
      " ---------------------- loss: tensor([236.3155], grad_fn=<DivBackward0>)\n",
      "Epoch 120\n",
      " ---------------------- loss: tensor([231.3589], grad_fn=<DivBackward0>)\n",
      "Epoch 121\n",
      " ---------------------- loss: tensor([224.4221], grad_fn=<DivBackward0>)\n",
      "Epoch 122\n",
      " ---------------------- loss: tensor([215.7538], grad_fn=<DivBackward0>)\n",
      "Epoch 123\n",
      " ---------------------- loss: tensor([207.1913], grad_fn=<DivBackward0>)\n",
      "Epoch 124\n",
      " ---------------------- loss: tensor([199.0062], grad_fn=<DivBackward0>)\n",
      "Epoch 125\n",
      " ---------------------- loss: tensor([192.1753], grad_fn=<DivBackward0>)\n",
      "Epoch 126\n",
      " ---------------------- loss: tensor([186.3789], grad_fn=<DivBackward0>)\n",
      "Epoch 127\n",
      " ---------------------- loss: tensor([181.2139], grad_fn=<DivBackward0>)\n",
      "Epoch 128\n",
      " ---------------------- loss: tensor([176.5273], grad_fn=<DivBackward0>)\n",
      "Epoch 129\n",
      " ---------------------- loss: tensor([172.3299], grad_fn=<DivBackward0>)\n",
      "Epoch 130\n",
      " ---------------------- loss: tensor([168.5042], grad_fn=<DivBackward0>)\n",
      "Epoch 131\n",
      " ---------------------- loss: tensor([164.2618], grad_fn=<DivBackward0>)\n",
      "Epoch 132\n",
      " ---------------------- loss: tensor([158.4970], grad_fn=<DivBackward0>)\n",
      "Epoch 133\n",
      " ---------------------- loss: tensor([153.2335], grad_fn=<DivBackward0>)\n",
      "Epoch 134\n",
      " ---------------------- loss: tensor([147.7657], grad_fn=<DivBackward0>)\n",
      "Epoch 135\n",
      " ---------------------- loss: tensor([140.5307], grad_fn=<DivBackward0>)\n",
      "Epoch 136\n",
      " ---------------------- loss: tensor([136.3015], grad_fn=<DivBackward0>)\n",
      "Epoch 137\n",
      " ---------------------- loss: tensor([133.0976], grad_fn=<DivBackward0>)\n",
      "Epoch 138\n",
      " ---------------------- loss: tensor([130.2683], grad_fn=<DivBackward0>)\n",
      "Epoch 139\n",
      " ---------------------- loss: tensor([127.7654], grad_fn=<DivBackward0>)\n",
      "Epoch 140\n",
      " ---------------------- loss: tensor([125.6977], grad_fn=<DivBackward0>)\n",
      "Epoch 141\n",
      " ---------------------- loss: tensor([124.0404], grad_fn=<DivBackward0>)\n",
      "Epoch 142\n",
      " ---------------------- loss: tensor([122.6231], grad_fn=<DivBackward0>)\n",
      "Epoch 143\n",
      " ---------------------- loss: tensor([119.5697], grad_fn=<DivBackward0>)\n",
      "Epoch 144\n",
      " ---------------------- loss: tensor([116.6910], grad_fn=<DivBackward0>)\n",
      "Epoch 145\n",
      " ---------------------- loss: tensor([114.7173], grad_fn=<DivBackward0>)\n",
      "Epoch 146\n",
      " ---------------------- loss: tensor([112.9395], grad_fn=<DivBackward0>)\n",
      "Epoch 147\n",
      " ---------------------- loss: tensor([111.4503], grad_fn=<DivBackward0>)\n",
      "Epoch 148\n",
      " ---------------------- loss: tensor([110.2438], grad_fn=<DivBackward0>)\n",
      "Epoch 149\n",
      " ---------------------- loss: tensor([109.1636], grad_fn=<DivBackward0>)\n",
      "Epoch 150\n",
      " ---------------------- loss: tensor([108.1582], grad_fn=<DivBackward0>)\n",
      "Epoch 151\n",
      " ---------------------- loss: tensor([107.2776], grad_fn=<DivBackward0>)\n",
      "Epoch 152\n",
      " ---------------------- loss: tensor([106.5086], grad_fn=<DivBackward0>)\n",
      "Epoch 153\n",
      " ---------------------- loss: tensor([105.8006], grad_fn=<DivBackward0>)\n",
      "Epoch 154\n",
      " ---------------------- loss: tensor([105.1172], grad_fn=<DivBackward0>)\n",
      "Epoch 155\n",
      " ---------------------- loss: tensor([104.4058], grad_fn=<DivBackward0>)\n",
      "Epoch 156\n",
      " ---------------------- loss: tensor([103.6807], grad_fn=<DivBackward0>)\n",
      "Epoch 157\n",
      " ---------------------- loss: tensor([102.6842], grad_fn=<DivBackward0>)\n",
      "Epoch 158\n",
      " ---------------------- loss: tensor([101.1943], grad_fn=<DivBackward0>)\n",
      "Epoch 159\n",
      " ---------------------- loss: tensor([100.1368], grad_fn=<DivBackward0>)\n",
      "Epoch 160\n",
      " ---------------------- loss: tensor([99.3826], grad_fn=<DivBackward0>)\n",
      "Epoch 161\n",
      " ---------------------- loss: tensor([98.8403], grad_fn=<DivBackward0>)\n",
      "Epoch 162\n",
      " ---------------------- loss: tensor([98.3761], grad_fn=<DivBackward0>)\n",
      "Epoch 163\n",
      " ---------------------- loss: tensor([97.9673], grad_fn=<DivBackward0>)\n",
      "Epoch 164\n",
      " ---------------------- loss: tensor([97.6432], grad_fn=<DivBackward0>)\n",
      "Epoch 165\n",
      " ---------------------- loss: tensor([97.3692], grad_fn=<DivBackward0>)\n",
      "Epoch 166\n",
      " ---------------------- loss: tensor([97.1579], grad_fn=<DivBackward0>)\n",
      "Epoch 167\n",
      " ---------------------- loss: tensor([96.3414], grad_fn=<DivBackward0>)\n",
      "Epoch 168\n",
      " ---------------------- loss: tensor([96.2326], grad_fn=<DivBackward0>)\n",
      "Epoch 169\n",
      " ---------------------- loss: tensor([96.0798], grad_fn=<DivBackward0>)\n",
      "Epoch 170\n",
      " ---------------------- loss: tensor([95.6173], grad_fn=<DivBackward0>)\n",
      "Epoch 171\n",
      " ---------------------- loss: tensor([95.1330], grad_fn=<DivBackward0>)\n",
      "Epoch 172\n",
      " ---------------------- loss: tensor([94.7459], grad_fn=<DivBackward0>)\n",
      "Epoch 173\n",
      " ---------------------- loss: tensor([94.3859], grad_fn=<DivBackward0>)\n",
      "Epoch 174\n",
      " ---------------------- loss: tensor([94.0387], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175\n",
      " ---------------------- loss: tensor([93.6681], grad_fn=<DivBackward0>)\n",
      "Epoch 176\n",
      " ---------------------- loss: tensor([93.2086], grad_fn=<DivBackward0>)\n",
      "Epoch 177\n",
      " ---------------------- loss: tensor([92.6982], grad_fn=<DivBackward0>)\n",
      "Epoch 178\n",
      " ---------------------- loss: tensor([92.1958], grad_fn=<DivBackward0>)\n",
      "Epoch 179\n",
      " ---------------------- loss: tensor([91.7682], grad_fn=<DivBackward0>)\n",
      "Epoch 180\n",
      " ---------------------- loss: tensor([91.3888], grad_fn=<DivBackward0>)\n",
      "Epoch 181\n",
      " ---------------------- loss: tensor([90.9853], grad_fn=<DivBackward0>)\n",
      "Epoch 182\n",
      " ---------------------- loss: tensor([90.6276], grad_fn=<DivBackward0>)\n",
      "Epoch 183\n",
      " ---------------------- loss: tensor([90.2826], grad_fn=<DivBackward0>)\n",
      "Epoch 184\n",
      " ---------------------- loss: tensor([89.8094], grad_fn=<DivBackward0>)\n",
      "Epoch 185\n",
      " ---------------------- loss: tensor([89.3370], grad_fn=<DivBackward0>)\n",
      "Epoch 186\n",
      " ---------------------- loss: tensor([88.9455], grad_fn=<DivBackward0>)\n",
      "Epoch 187\n",
      " ---------------------- loss: tensor([88.5543], grad_fn=<DivBackward0>)\n",
      "Epoch 188\n",
      " ---------------------- loss: tensor([88.1053], grad_fn=<DivBackward0>)\n",
      "Epoch 189\n",
      " ---------------------- loss: tensor([87.6993], grad_fn=<DivBackward0>)\n",
      "Epoch 190\n",
      " ---------------------- loss: tensor([87.3090], grad_fn=<DivBackward0>)\n",
      "Epoch 191\n",
      " ---------------------- loss: tensor([86.7987], grad_fn=<DivBackward0>)\n",
      "Epoch 192\n",
      " ---------------------- loss: tensor([86.2976], grad_fn=<DivBackward0>)\n",
      "Epoch 193\n",
      " ---------------------- loss: tensor([85.8076], grad_fn=<DivBackward0>)\n",
      "Epoch 194\n",
      " ---------------------- loss: tensor([85.1458], grad_fn=<DivBackward0>)\n",
      "Epoch 195\n",
      " ---------------------- loss: tensor([84.4782], grad_fn=<DivBackward0>)\n",
      "Epoch 196\n",
      " ---------------------- loss: tensor([83.9153], grad_fn=<DivBackward0>)\n",
      "Epoch 197\n",
      " ---------------------- loss: tensor([83.3901], grad_fn=<DivBackward0>)\n",
      "Epoch 198\n",
      " ---------------------- loss: tensor([82.8885], grad_fn=<DivBackward0>)\n",
      "Epoch 199\n",
      " ---------------------- loss: tensor([82.3900], grad_fn=<DivBackward0>)\n",
      "Epoch 200\n",
      " ---------------------- loss: tensor([81.7514], grad_fn=<DivBackward0>)\n",
      "Epoch 201\n",
      " ---------------------- loss: tensor([81.0612], grad_fn=<DivBackward0>)\n",
      "Epoch 202\n",
      " ---------------------- loss: tensor([80.3597], grad_fn=<DivBackward0>)\n",
      "Epoch 203\n",
      " ---------------------- loss: tensor([79.6929], grad_fn=<DivBackward0>)\n",
      "Epoch 204\n",
      " ---------------------- loss: tensor([79.0873], grad_fn=<DivBackward0>)\n",
      "Epoch 205\n",
      " ---------------------- loss: tensor([78.5520], grad_fn=<DivBackward0>)\n",
      "Epoch 206\n",
      " ---------------------- loss: tensor([78.0672], grad_fn=<DivBackward0>)\n",
      "Epoch 207\n",
      " ---------------------- loss: tensor([77.6040], grad_fn=<DivBackward0>)\n",
      "Epoch 208\n",
      " ---------------------- loss: tensor([77.0548], grad_fn=<DivBackward0>)\n",
      "Epoch 209\n",
      " ---------------------- loss: tensor([76.5371], grad_fn=<DivBackward0>)\n",
      "Epoch 210\n",
      " ---------------------- loss: tensor([76.0455], grad_fn=<DivBackward0>)\n",
      "Epoch 211\n",
      " ---------------------- loss: tensor([75.5192], grad_fn=<DivBackward0>)\n",
      "Epoch 212\n",
      " ---------------------- loss: tensor([74.9711], grad_fn=<DivBackward0>)\n",
      "Epoch 213\n",
      " ---------------------- loss: tensor([74.4353], grad_fn=<DivBackward0>)\n",
      "Epoch 214\n",
      " ---------------------- loss: tensor([73.9133], grad_fn=<DivBackward0>)\n",
      "Epoch 215\n",
      " ---------------------- loss: tensor([73.3824], grad_fn=<DivBackward0>)\n",
      "Epoch 216\n",
      " ---------------------- loss: tensor([72.8278], grad_fn=<DivBackward0>)\n",
      "Epoch 217\n",
      " ---------------------- loss: tensor([72.2615], grad_fn=<DivBackward0>)\n",
      "Epoch 218\n",
      " ---------------------- loss: tensor([71.6752], grad_fn=<DivBackward0>)\n",
      "Epoch 219\n",
      " ---------------------- loss: tensor([71.1000], grad_fn=<DivBackward0>)\n",
      "Epoch 220\n",
      " ---------------------- loss: tensor([70.4879], grad_fn=<DivBackward0>)\n",
      "Epoch 221\n",
      " ---------------------- loss: tensor([69.8356], grad_fn=<DivBackward0>)\n",
      "Epoch 222\n",
      " ---------------------- loss: tensor([69.1962], grad_fn=<DivBackward0>)\n",
      "Epoch 223\n",
      " ---------------------- loss: tensor([68.6373], grad_fn=<DivBackward0>)\n",
      "Epoch 224\n",
      " ---------------------- loss: tensor([68.1003], grad_fn=<DivBackward0>)\n",
      "Epoch 225\n",
      " ---------------------- loss: tensor([67.5202], grad_fn=<DivBackward0>)\n",
      "Epoch 226\n",
      " ---------------------- loss: tensor([66.9101], grad_fn=<DivBackward0>)\n",
      "Epoch 227\n",
      " ---------------------- loss: tensor([66.1794], grad_fn=<DivBackward0>)\n",
      "Epoch 228\n",
      " ---------------------- loss: tensor([65.4876], grad_fn=<DivBackward0>)\n",
      "Epoch 229\n",
      " ---------------------- loss: tensor([64.9121], grad_fn=<DivBackward0>)\n",
      "Epoch 230\n",
      " ---------------------- loss: tensor([64.4030], grad_fn=<DivBackward0>)\n",
      "Epoch 231\n",
      " ---------------------- loss: tensor([63.8681], grad_fn=<DivBackward0>)\n",
      "Epoch 232\n",
      " ---------------------- loss: tensor([63.2819], grad_fn=<DivBackward0>)\n",
      "Epoch 233\n",
      " ---------------------- loss: tensor([62.5411], grad_fn=<DivBackward0>)\n",
      "Epoch 234\n",
      " ---------------------- loss: tensor([61.8932], grad_fn=<DivBackward0>)\n",
      "Epoch 235\n",
      " ---------------------- loss: tensor([61.3323], grad_fn=<DivBackward0>)\n",
      "Epoch 236\n",
      " ---------------------- loss: tensor([60.7520], grad_fn=<DivBackward0>)\n",
      "Epoch 237\n",
      " ---------------------- loss: tensor([60.1141], grad_fn=<DivBackward0>)\n",
      "Epoch 238\n",
      " ---------------------- loss: tensor([59.4910], grad_fn=<DivBackward0>)\n",
      "Epoch 239\n",
      " ---------------------- loss: tensor([58.9135], grad_fn=<DivBackward0>)\n",
      "Epoch 240\n",
      " ---------------------- loss: tensor([58.3325], grad_fn=<DivBackward0>)\n",
      "Epoch 241\n",
      " ---------------------- loss: tensor([56.8743], grad_fn=<DivBackward0>)\n",
      "Epoch 242\n",
      " ---------------------- loss: tensor([55.4680], grad_fn=<DivBackward0>)\n",
      "Epoch 243\n",
      " ---------------------- loss: tensor([54.3057], grad_fn=<DivBackward0>)\n",
      "Epoch 244\n",
      " ---------------------- loss: tensor([53.3888], grad_fn=<DivBackward0>)\n",
      "Epoch 245\n",
      " ---------------------- loss: tensor([52.7473], grad_fn=<DivBackward0>)\n",
      "Epoch 246\n",
      " ---------------------- loss: tensor([52.2578], grad_fn=<DivBackward0>)\n",
      "Epoch 247\n",
      " ---------------------- loss: tensor([51.8566], grad_fn=<DivBackward0>)\n",
      "Epoch 248\n",
      " ---------------------- loss: tensor([51.5172], grad_fn=<DivBackward0>)\n",
      "Epoch 249\n",
      " ---------------------- loss: tensor([51.2180], grad_fn=<DivBackward0>)\n",
      "Epoch 250\n",
      " ---------------------- loss: tensor([50.9112], grad_fn=<DivBackward0>)\n",
      "Epoch 251\n",
      " ---------------------- loss: tensor([50.5979], grad_fn=<DivBackward0>)\n",
      "Epoch 252\n",
      " ---------------------- loss: tensor([50.3248], grad_fn=<DivBackward0>)\n",
      "Epoch 253\n",
      " ---------------------- loss: tensor([50.0778], grad_fn=<DivBackward0>)\n",
      "Epoch 254\n",
      " ---------------------- loss: tensor([49.7899], grad_fn=<DivBackward0>)\n",
      "Epoch 255\n",
      " ---------------------- loss: tensor([49.4772], grad_fn=<DivBackward0>)\n",
      "Epoch 256\n",
      " ---------------------- loss: tensor([49.1959], grad_fn=<DivBackward0>)\n",
      "Epoch 257\n",
      " ---------------------- loss: tensor([48.9479], grad_fn=<DivBackward0>)\n",
      "Epoch 258\n",
      " ---------------------- loss: tensor([48.7152], grad_fn=<DivBackward0>)\n",
      "Epoch 259\n",
      " ---------------------- loss: tensor([48.4765], grad_fn=<DivBackward0>)\n",
      "Epoch 260\n",
      " ---------------------- loss: tensor([48.2370], grad_fn=<DivBackward0>)\n",
      "Epoch 261\n",
      " ---------------------- loss: tensor([47.9984], grad_fn=<DivBackward0>)\n",
      "Epoch 262\n",
      " ---------------------- loss: tensor([47.7799], grad_fn=<DivBackward0>)\n",
      "Epoch 263\n",
      " ---------------------- loss: tensor([47.5780], grad_fn=<DivBackward0>)\n",
      "Epoch 264\n",
      " ---------------------- loss: tensor([47.4148], grad_fn=<DivBackward0>)\n",
      "Epoch 265\n",
      " ---------------------- loss: tensor([47.2613], grad_fn=<DivBackward0>)\n",
      "Epoch 266\n",
      " ---------------------- loss: tensor([47.0822], grad_fn=<DivBackward0>)\n",
      "Epoch 267\n",
      " ---------------------- loss: tensor([46.8172], grad_fn=<DivBackward0>)\n",
      "Epoch 268\n",
      " ---------------------- loss: tensor([46.5925], grad_fn=<DivBackward0>)\n",
      "Epoch 269\n",
      " ---------------------- loss: tensor([46.3952], grad_fn=<DivBackward0>)\n",
      "Epoch 270\n",
      " ---------------------- loss: tensor([46.2088], grad_fn=<DivBackward0>)\n",
      "Epoch 271\n",
      " ---------------------- loss: tensor([45.9891], grad_fn=<DivBackward0>)\n",
      "Epoch 272\n",
      " ---------------------- loss: tensor([45.6258], grad_fn=<DivBackward0>)\n",
      "Epoch 273\n",
      " ---------------------- loss: tensor([45.3616], grad_fn=<DivBackward0>)\n",
      "Epoch 274\n",
      " ---------------------- loss: tensor([45.0487], grad_fn=<DivBackward0>)\n",
      "Epoch 275\n",
      " ---------------------- loss: tensor([44.6213], grad_fn=<DivBackward0>)\n",
      "Epoch 276\n",
      " ---------------------- loss: tensor([44.2513], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277\n",
      " ---------------------- loss: tensor([43.8533], grad_fn=<DivBackward0>)\n",
      "Epoch 278\n",
      " ---------------------- loss: tensor([43.3902], grad_fn=<DivBackward0>)\n",
      "Epoch 279\n",
      " ---------------------- loss: tensor([43.0256], grad_fn=<DivBackward0>)\n",
      "Epoch 280\n",
      " ---------------------- loss: tensor([42.6976], grad_fn=<DivBackward0>)\n",
      "Epoch 281\n",
      " ---------------------- loss: tensor([42.3306], grad_fn=<DivBackward0>)\n",
      "Epoch 282\n",
      " ---------------------- loss: tensor([41.9517], grad_fn=<DivBackward0>)\n",
      "Epoch 283\n",
      " ---------------------- loss: tensor([41.5079], grad_fn=<DivBackward0>)\n",
      "Epoch 284\n",
      " ---------------------- loss: tensor([41.0726], grad_fn=<DivBackward0>)\n",
      "Epoch 285\n",
      " ---------------------- loss: tensor([40.6518], grad_fn=<DivBackward0>)\n",
      "Epoch 286\n",
      " ---------------------- loss: tensor([40.2260], grad_fn=<DivBackward0>)\n",
      "Epoch 287\n",
      " ---------------------- loss: tensor([39.8163], grad_fn=<DivBackward0>)\n",
      "Epoch 288\n",
      " ---------------------- loss: tensor([39.4241], grad_fn=<DivBackward0>)\n",
      "Epoch 289\n",
      " ---------------------- loss: tensor([39.0084], grad_fn=<DivBackward0>)\n",
      "Epoch 290\n",
      " ---------------------- loss: tensor([38.5767], grad_fn=<DivBackward0>)\n",
      "Epoch 291\n",
      " ---------------------- loss: tensor([38.1201], grad_fn=<DivBackward0>)\n",
      "Epoch 292\n",
      " ---------------------- loss: tensor([37.6061], grad_fn=<DivBackward0>)\n",
      "Epoch 293\n",
      " ---------------------- loss: tensor([37.1257], grad_fn=<DivBackward0>)\n",
      "Epoch 294\n",
      " ---------------------- loss: tensor([36.6834], grad_fn=<DivBackward0>)\n",
      "Epoch 295\n",
      " ---------------------- loss: tensor([36.2629], grad_fn=<DivBackward0>)\n",
      "Epoch 296\n",
      " ---------------------- loss: tensor([35.8313], grad_fn=<DivBackward0>)\n",
      "Epoch 297\n",
      " ---------------------- loss: tensor([35.4265], grad_fn=<DivBackward0>)\n",
      "Epoch 298\n",
      " ---------------------- loss: tensor([34.9064], grad_fn=<DivBackward0>)\n",
      "Epoch 299\n",
      " ---------------------- loss: tensor([34.4007], grad_fn=<DivBackward0>)\n",
      "Epoch 300\n",
      " ---------------------- loss: tensor([33.9306], grad_fn=<DivBackward0>)\n",
      "Done!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upper_r = 6\n",
    "lower_r = 1e-2\n",
    "steps = 100\n",
    "R_train = torch.Tensor(np.linspace(lower_r, upper_r, steps)[:,None])\n",
    "epochs = 300\n",
    "lrs = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "Phis_t = []\n",
    "Es = []\n",
    "\n",
    "for lr in lrs:\n",
    "    model = NeuralNetwork().to(device)\n",
    "    initialize_weights(model)\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n ---------------------- loss: {loss_fn(R_train.to(device))}\")\n",
    "        training(R_train, loss_fn, optimizer)\n",
    "    print(\"Done!\\n\\n\")\n",
    "    Phis_t.append(Phi_t(R_train).detach().numpy())\n",
    "    Es.append(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d87e2420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rr = np.linspace(lower_r, upper_r, steps)[:,None]\n",
    "fig, axs = plt.subplots(3,3,figsize=(15,10))\n",
    "\n",
    "fil = 0\n",
    "col = 0\n",
    "for i in range(len(Es)):\n",
    "    yy = Phis_t[i]\n",
    "    axs[fil,col].plot(rr, yy.squeeze(), \".\", label=f\"learning rate {lrs[i]}\")\n",
    "    axs[fil,col].set_xlabel(\"$r$\")\n",
    "    axs[fil,col].set_ylabel(\"$\\phi(x)$\")\n",
    "    axs[fil,col].legend(loc=\"best\")\n",
    "    axs[fil,col].ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    if col == 2:\n",
    "       col = 0\n",
    "       fil = fil+1\n",
    "    else:\n",
    "       col = col+1\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "803f019e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-22.1364], grad_fn=<DivBackward0>),\n",
       " tensor([-19.8373], grad_fn=<DivBackward0>),\n",
       " tensor([-2.4084], grad_fn=<DivBackward0>),\n",
       " tensor([-18.4017], grad_fn=<DivBackward0>),\n",
       " tensor([3.9739], grad_fn=<DivBackward0>),\n",
       " tensor([0.8456], grad_fn=<DivBackward0>),\n",
       " tensor([-0.0149], grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa85a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
